(13696, 300, 8)
(13696,)
epochs: 50
Shuffle: True
[1. 1. 1. ... 1. 1. 1.]
(10956, 300, 8)
weight0: 1.486970684039088
weight1: 0.8022648514851484
*****************************************
model_bigger_biggest_lstm_v16
Model: "sequential_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_16 (LSTM)              (None, 256)               271360    
                                                                 
 dense_56 (Dense)            (None, 16)                4112      
                                                                 
 dense_57 (Dense)            (None, 16)                272       
                                                                 
 dense_58 (Dense)            (None, 16)                272       
                                                                 
 dense_59 (Dense)            (None, 16)                272       
                                                                 
 dense_60 (Dense)            (None, 16)                272       
                                                                 
 dense_61 (Dense)            (None, 16)                272       
                                                                 
 dense_62 (Dense)            (None, 16)                272       
                                                                 
 dense_63 (Dense)            (None, 16)                272       
                                                                 
 dense_64 (Dense)            (None, 16)                272       
                                                                 
 dense_65 (Dense)            (None, 16)                272       
                                                                 
 dense_66 (Dense)            (None, 16)                272       
                                                                 
 dense_67 (Dense)            (None, 16)                272       
                                                                 
 dense_68 (Dense)            (None, 16)                272       
                                                                 
 dense_69 (Dense)            (None, 16)                272       
                                                                 
 dense_70 (Dense)            (None, 1)                 17        
                                                                 
=================================================================
Total params: 279,025
Trainable params: 279,025
Non-trainable params: 0
_________________________________________________________________
*****************************************
-------------------------------
unique model id: model_bigger_biggest_lstm_v16-Adagrad0,001
optimizer: Adagrad<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>
-------------------------------
loss : [0.7155981063842773, 0.7154277563095093, 0.7153076529502869, 0.715225100517273, 0.7151641249656677, 0.7151168584823608, 0.7150800824165344, 0.7150464653968811, 0.7150102853775024, 0.7149906158447266, 0.7149478793144226, 0.7149016261100769, 0.7148385643959045, 0.7147629857063293, 0.7146992683410645, 0.7146303057670593, 0.7145525217056274, 0.7144783139228821, 0.7143679857254028, 0.7142369747161865, 0.7140859365463257, 0.7139280438423157, 0.7137320041656494, 0.7135457992553711, 0.7133316397666931, 0.7131752371788025, 0.7129598259925842, 0.712792694568634, 0.7125542163848877, 0.7122542262077332, 0.7119643688201904, 0.7118479013442993, 0.7116054892539978, 0.7113056182861328, 0.7110365033149719, 0.7106502652168274, 0.7104215621948242, 0.7101357579231262, 0.7098379135131836, 0.7094689011573792, 0.7091156244277954, 0.7087274193763733, 0.7084499001502991, 0.7079129815101624, 0.7075316309928894, 0.7070428133010864, 0.7065213322639465, 0.7061223387718201, 0.7056016325950623, 0.705150842666626]
tp : [7177.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7272.0, 7260.0, 7187.0, 6839.0, 6610.0, 6338.0, 6009.0, 5975.0, 5741.0, 5730.0, 5528.0, 5406.0, 5386.0, 5100.0]
tn : [31.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 46.0, 217.0, 578.0, 853.0, 1151.0, 1382.0, 1403.0, 1551.0, 1555.0, 1664.0, 1729.0, 1711.0, 1866.0]
accuracy : [0.657904326915741, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.663837194442749, 0.6668491959571838, 0.6757940649986267, 0.6769806742668152, 0.6811792850494385, 0.6835523843765259, 0.674607515335083, 0.6734209656715393, 0.665571391582489, 0.6649324297904968, 0.6564439535140991, 0.6512413024902344, 0.6477729082107544, 0.6358159780502319]
precision : [0.6626962423324585, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6637458801269531, 0.6638064980506897, 0.6661772727966309, 0.674582302570343, 0.6876822710037231, 0.7001376748085022, 0.7144628763198853, 0.723017692565918, 0.7237160801887512, 0.729108452796936, 0.7291004061698914, 0.7323794364929199, 0.7344110608100891, 0.7318929433822632, 0.737207293510437]
val_loss : [0.6912834644317627, 0.6898120045661926, 0.6887773871421814, 0.6883401870727539, 0.6876932382583618, 0.6871883869171143, 0.687038242816925, 0.6867579221725464, 0.686435341835022, 0.6862612366676331, 0.6861106753349304, 0.686088502407074, 0.6859472393989563, 0.6858544945716858, 0.6857230067253113, 0.6856675148010254, 0.6854099035263062, 0.6855606436729431, 0.6852744221687317, 0.6851469278335571, 0.685116708278656, 0.684748113155365, 0.6847826838493347, 0.6850035190582275, 0.6844045519828796, 0.6846965551376343, 0.6842309832572937, 0.6840954422950745, 0.6839194893836975, 0.6832497715950012, 0.682407796382904, 0.6825366020202637, 0.6822893619537354, 0.6817137598991394, 0.6819808483123779, 0.6817483901977539, 0.6813381910324097, 0.6816489696502686, 0.681388795375824, 0.6800203323364258, 0.6795840263366699, 0.6794695854187012, 0.6783986687660217, 0.6781023144721985, 0.677718997001648, 0.6787965297698975, 0.6775070428848267, 0.6772177815437317, 0.6766853928565979, 0.6751357316970825]
val_tp : [1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1831.0, 1814.0, 1761.0, 1724.0, 1665.0, 1614.0, 1664.0, 1570.0, 1551.0, 1331.0, 1407.0, 1326.0, 1353.0, 1416.0]
val_tn : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 38.0, 94.0, 139.0, 189.0, 251.0, 212.0, 288.0, 297.0, 418.0, 393.0, 424.0, 418.0, 392.0]
val_accuracy : [0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.6711679100990295, 0.6759123802185059, 0.6770073175430298, 0.6799269914627075, 0.6766423583030701, 0.680656909942627, 0.6846715211868286, 0.6781021952629089, 0.674452543258667, 0.6383211612701416, 0.6569343209266663, 0.6386861205101013, 0.6463503837585449, 0.659853994846344]
val_precision : [0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.668248176574707, 0.6702049970626831, 0.6756052374839783, 0.6836180090904236, 0.6912590265274048, 0.698113203048706, 0.7103873491287231, 0.7047861218452454, 0.7165677547454834, 0.7170596122741699, 0.7305158972740173, 0.7316692471504211, 0.7321921586990356, 0.7337310314178467, 0.7325400710105896]

[[2951  832]
 [ 472  204]]
model_bigger_biggest_lstm_v16-Adagrad0,001
[[2951  832]
 [ 472  204]]
tn: %: 0.30177514792899407 tp %: 0.7800687285223368
