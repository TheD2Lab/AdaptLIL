epochs: 100
Shuffle on compile: False
trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
trainData_500.0mssec_P55 p55.anatomy.list.csv.arff
trainData_500.0mssec_P59 p59.conf.list.csv.arff
trainData_500.0mssec_P17 p17.conf.list.csv.arff
trainData_500.0mssec_P8 p8.anatomy.matrix.csv.arff
trainData_500.0mssec_P58 p58.conf.matrix.csv.arff
trainData_500.0mssec_P55 p55.conf.list.csv.arff
trainData_500.0mssec_P7 p7.conf.list.csv.arff
trainData_500.0mssec_P34 p34.conf.matrix.csv.arff
trainData_500.0mssec_P20 p20.anatomy.matrix.csv.arff
trainData_500.0mssec_P30 p30.conf.matrix.csv.arff
trainData_500.0mssec_P13 p13.anatomy.list.csv.arff
trainData_500.0mssec_P64 p64.anatomy.matrix.csv.arff
trainData_500.0mssec_P59 p59.anatomy.list.csv.arff
trainData_500.0mssec_P3 p3.conf.list.csv.arff
trainData_500.0mssec_P38 p38.conf.matrix.csv.arff
trainData_500.0mssec_P3 p3.anatomy.list.csv.arff
trainData_500.0mssec_P13 p13.conf.list.csv.arff
trainData_500.0mssec_P1 p1.conf.list.csv.arff
trainData_500.0mssec_P60 p60.conf.matrix.csv.arff
trainData_500.0mssec_P39 p39.anatomy.list.csv.arff
trainData_500.0mssec_P19 p19.conf.list.csv.arff
trainData_500.0mssec_P63 p63.conf.list.csv.arff
trainData_500.0mssec_P68 p68.anatomy.matrix.csv.arff
trainData_500.0mssec_P24 p24.conf.matrix.csv.arff
trainData_500.0mssec_P61 p61.anatomy.list.csv.arff
trainData_500.0mssec_P71 p71.anatomy.list.csv.arff
trainData_500.0mssec_P56 p56.anatomy.matrix.csv.arff
trainData_500.0mssec_P26 p26.anatomy.matrix.csv.arff
trainData_500.0mssec_P5 p5.anatomy.list.csv.arff
trainData_500.0mssec_P11 p11.conf.list.csv.arff
trainData_500.0mssec_P80 p80.anatomy.list.csv.arff
trainData_500.0mssec_P53 p53.anatomy.list.csv.arff
trainData_500.0mssec_P78 p78.conf.matrix.csv.arff
trainData_500.0mssec_P34 p34.anatomy.matrix.csv.arff
trainData_500.0mssec_P27 p27.conf.list.csv.arff
trainData_500.0mssec_P4 p4.conf.matrix.csv.arff
trainData_500.0mssec_P69 p69.conf.list.csv.arff
trainData_500.0mssec_P74 p74.conf.matrix.csv.arff
trainData_500.0mssec_P1 p1.anatomy.list.csv.arff
testData_500.0msec_window_1.arff
trainData_500.0mssec_P62 p62.conf.matrix.csv.arff
trainData_500.0mssec_P30 p30.anatomy.matrix.csv.arff
test_participants.txt
trainData_500.0mssec_P49 p49.anatomy.list.csv.arff
trainData_500.0mssec_P65 p65.anatomy.list.csv.arff
trainData_500.0mssec_P60 p60.anatomy.matrix.csv.arff
trainData_500.0mssec_P2 p2.conf.matrix.csv.arff
trainData_500.0mssec_P70 p70.anatomy.matrix.csv.arff
trainData_500.0mssec_P24 p24.anatomy.matrix.csv.arff
trainData_500.0mssec_P20 p20.conf.matrix.csv.arff
trainData_500.0mssec_P37 p37.anatomy.list.csv.arff
trainData_500.0mssec_P45 p45.anatomy.list.csv.arff
trainData_500.0mssec_P10 p10.anatomy.matrix.csv.arff
trainData_500.0mssec_P69 p69.anatomy.list.csv.arff
trainData_500.0mssec_P66 p66.conf.matrix.csv.arff
trainData_500.0mssec_P67 p67.anatomy.list.csv.arff
trainData_500.0mssec_P47 p47.conf.list.csv.arff
trainData_500.0mssec_P26 p26.conf.matrix.csv.arff
trainData_500.0mssec_P56 p56.conf.matrix.csv.arff
trainData_500.0mssec_P6 p6.conf.matrix.csv.arff
trainData_500.0mssec_P11 p11.anatomy.list.csv.arff
trainData_500.0mssec_P25 p25.conf.list.csv.arff
trainData_500.0mssec_P65 p65.conf.list.csv.arff
trainData_500.0mssec_P74 p74.anatomy.matrix.csv.arff
trainData_500.0mssec_P23 p23.anatomy.list.csv.arff
trainData_500.0mssec_P50 p50.conf.matrix.csv.arff
trainData_500.0mssec_P22 p22.conf.matrix.csv.arff
trainData_500.0mssec_P52 p52.conf.matrix.csv.arff
trainData_500.0mssec_P79 p79.anatomy.list.csv.arff
trainData_500.0mssec_P6 p6.anatomy.matrix.csv.arff
trainData_500.0mssec_P78 p78.anatomy.matrix.csv.arff
trainData_500.0mssec_P37 p37.conf.list.csv.arff
trainData_500.0mssec_P14 p14.conf.matrix.csv.arff
trainData_500.0mssec_P51 p51.conf.list.csv.arff
trainData_500.0mssec_P76 p76.anatomy.matrix.csv.arff
trainData_500.0mssec_P36 p36.conf.matrix.csv.arff
trainData_500.0mssec_P39 p39.conf.list.csv.arff
trainData_500.0mssec_P81 p81.anatomy.matrix.csv.arff
trainData_500.0mssec_P35 p35.anatomy.list.csv.arff
trainData_500.0mssec_P17 p17.anatomy.list.csv.arff
trainData_500.0mssec_P7 p7.anatomy.list.csv.arff
trainData_500.0mssec_P54 p54.conf.matrix.csv.arff
trainData_500.0mssec_P22 p22.anatomy.matrix.csv.arff
trainData_500.0mssec_P49 p49.conf.list.csv.arff
trainData_500.0mssec_P14 p14.anatomy.matrix.csv.arff
trainData_500.0mssec_P4 p4.anatomy.matrix.csv.arff
trainData_500.0mssec_P76 p76.conf.matrix.csv.arff
trainData_500.0mssec_P79 p79.conf.list.csv.arff
trainData_500.0mssec_P58 p58.anatomy.matrix.csv.arff
trainData_500.0mssec_P10 p10.conf.matrix.csv.arff
trainData_500.0mssec_P35 p35.conf.list.csv.arff
trainData_500.0mssec_P66 p66.anatomy.matrix.csv.arff
trainData_500.0mssec_P47 p47.anatomy.list.csv.arff
trainData_500.0mssec_P19 p19.anatomy.list.csv.arff
trainData_500.0mssec_P80 p80.conf.list.csv.arff
trainData_500.0mssec_P36 p36.anatomy.matrix.csv.arff
trainData_500.0mssec_P25 p25.anatomy.list.csv.arff
trainData_500.0mssec_P45 p45.conf.list.csv.arff
trainData_500.0mssec_P67 p67.conf.list.csv.arff
trainData_500.0mssec_P53 p53.conf.list.csv.arff
trainData_500.0mssec_P61 p61.conf.list.csv.arff
trainData_500.0mssec_P51 p51.anatomy.list.csv.arff
trainData_500.0mssec_P27 p27.anatomy.list.csv.arff
trainData_500.0mssec_P23 p23.conf.list.csv.arff
trainData_500.0mssec_P62 p62.anatomy.matrix.csv.arff
trainData_500.0mssec_P68 p68.conf.matrix.csv.arff
trainData_500.0mssec_P63 p63.anatomy.list.csv.arff
trainData_500.0mssec_P50 p50.anatomy.matrix.csv.arff
trainData_500.0mssec_P12 p12.conf.matrix.csv.arff
trainData_500.0mssec_P8 p8.conf.matrix.csv.arff
trainData_500.0mssec_P81 p81.conf.matrix.csv.arff
trainData_500.0mssec_P12 p12.anatomy.matrix.csv.arff
trainData_500.0mssec_P5 p5.conf.list.csv.arff
trainData_500.0mssec_P38 p38.anatomy.matrix.csv.arff
trainData_500.0mssec_P64 p64.conf.matrix.csv.arff
trainData_500.0mssec_P70 p70.conf.matrix.csv.arff
trainData_500.0mssec_P2 p2.anatomy.matrix.csv.arff
trainData_500.0mssec_P71 p71.conf.list.csv.arff
trainData_500.0mssec_P54 p54.anatomy.matrix.csv.arff
full input shape: (495, 2, 600)
full input shape: (367, 2, 600)
full input shape: (1534, 2, 600)
full input shape: (792, 2, 600)
full input shape: (527, 2, 600)
full input shape: (1309, 2, 600)
full input shape: (969, 2, 600)
full input shape: (1218, 2, 600)
full input shape: (903, 2, 600)
full input shape: (1036, 2, 600)
full input shape: (1032, 2, 600)
full input shape: (345, 2, 600)
full input shape: (660, 2, 600)
full input shape: (772, 2, 600)
full input shape: (875, 2, 600)
full input shape: (1084, 2, 600)
full input shape: (480, 2, 600)
full input shape: (762, 2, 600)
full input shape: (1155, 2, 600)
full input shape: (1605, 2, 600)
full input shape: (589, 2, 600)
full input shape: (1276, 2, 600)
full input shape: (664, 2, 600)
full input shape: (626, 2, 600)
full input shape: (1742, 2, 600)
full input shape: (577, 2, 600)
full input shape: (564, 2, 600)
full input shape: (472, 2, 600)
full input shape: (461, 2, 600)
full input shape: (468, 2, 600)
full input shape: (1623, 2, 600)
full input shape: (767, 2, 600)
full input shape: (366, 2, 600)
full input shape: (1155, 2, 600)
full input shape: (362, 2, 600)
full input shape: (2317, 2, 600)
full input shape: (1145, 2, 600)
full input shape: (1123, 2, 600)
full input shape: (970, 2, 600)
full input shape: (468, 2, 600)
full input shape: (1952, 2, 600)
full input shape: (470, 2, 600)
full input shape: (386, 2, 600)
full input shape: (646, 2, 600)
full input shape: (823, 2, 600)
full input shape: (1119, 2, 600)
full input shape: (1366, 2, 600)
full input shape: (1040, 2, 600)
full input shape: (1645, 2, 600)
full input shape: (485, 2, 600)
full input shape: (497, 2, 600)
full input shape: (586, 2, 600)
full input shape: (426, 2, 600)
full input shape: (1460, 2, 600)
full input shape: (280, 2, 600)
full input shape: (793, 2, 600)
full input shape: (1252, 2, 600)
full input shape: (1032, 2, 600)
full input shape: (937, 2, 600)
full input shape: (881, 2, 600)
full input shape: (1132, 2, 600)
full input shape: (1416, 2, 600)
full input shape: (428, 2, 600)
full input shape: (554, 2, 600)
full input shape: (1468, 2, 600)
full input shape: (1286, 2, 600)
full input shape: (1138, 2, 600)
full input shape: (330, 2, 600)
full input shape: (527, 2, 600)
full input shape: (574, 2, 600)
full input shape: (1084, 2, 600)
full input shape: (1422, 2, 600)
full input shape: (565, 2, 600)
full input shape: (710, 2, 600)
full input shape: (915, 2, 600)
full input shape: (1362, 2, 600)
full input shape: (617, 2, 600)
full input shape: (593, 2, 600)
full input shape: (385, 2, 600)
full input shape: (698, 2, 600)
full input shape: (1067, 2, 600)
full input shape: (605, 2, 600)
full input shape: (727, 2, 600)
full input shape: (632, 2, 600)
full input shape: (415, 2, 600)
full input shape: (1196, 2, 600)
full input shape: (1040, 2, 600)
full input shape: (577, 2, 600)
full input shape: (1631, 2, 600)
full input shape: (1220, 2, 600)
full input shape: (471, 2, 600)
full input shape: (474, 2, 600)
full input shape: (742, 2, 600)
full input shape: (1666, 2, 600)
full input shape: (502, 2, 600)
full input shape: (475, 2, 600)
full input shape: (1325, 2, 600)
full input shape: (721, 2, 600)
full input shape: (908, 2, 600)
full input shape: (1239, 2, 600)
full input shape: (303, 2, 600)
full input shape: (1466, 2, 600)
full input shape: (1071, 2, 600)
full input shape: (891, 2, 600)
full input shape: (1187, 2, 600)
full input shape: (371, 2, 600)
full input shape: (527, 2, 600)
full input shape: (932, 2, 600)
full input shape: (1103, 2, 600)
full input shape: (1385, 2, 600)
full input shape: (594, 2, 600)
full input shape: (1366, 2, 600)
full input shape: (449, 2, 600)
full input shape: (1290, 2, 600)
full input shape: (2191, 2, 600)
full input shape: (518, 2, 600)
full input shape: (916, 2, 600)
full input shape: (456, 2, 600)
normalization data attributes (keep handy)
"{0: {'min': -32.2858, 'max': 84.03265}, 1: {'min': -138.21996, 'max': 14.62452}, 2: {'min': 0.0, 'max': 1483.6432}, 3: {'min': 0.01086, 'max': 4.46191}, 4: {'min': -37.76155, 'max': 250.86676}, 5: {'min': -208.19109, 'max': 14.48806}, 6: {'min': 0.0, 'max': 120.70314}, 7: {'min': 4.18604, 'max': 168.97337}, 8: {'min': -29.95229, 'max': 219.71356}, 9: {'min': -169.06012, 'max': 14.59566}, 10: {'min': 0.0, 'max': 1483.1273}, 11: {'min': 0.0, 'max': 4.46777}, 12: {'min': -37.76155, 'max': 355.39447}, 13: {'min': -273.52362, 'max': 32.39463}, 14: {'min': 0.0, 'max': 120.70314}, 15: {'min': 4.18604, 'max': 164.21548}, 16: {'min': -32.27999, 'max': 27.22108}, 17: {'min': -68.32915, 'max': 12.96706}, 18: {'min': 0.0, 'max': 1483.1273}, 19: {'min': 0.0, 'max': 4.4751}, 20: {'min': -32.27999, 'max': 28.33345}, 21: {'min': -51.43461, 'max': 29.42237}, 22: {'min': 0.0, 'max': 120.70314}, 23: {'min': 4.43518, 'max': 159.4576}, 24: {'min': -26.86854, 'max': 26.58715}, 25: {'min': -49.63952, 'max': 29.95598}, 26: {'min': 0.0, 'max': 1483.1273}, 27: {'min': 0.0, 'max': 4.48145}, 28: {'min': -29.81033, 'max': 40.97514}, 29: {'min': -50.67493, 'max': 86.231}, 30: {'min': 0.0, 'max': 120.70314}, 31: {'min': 4.43518, 'max': 154.6997}, 32: {'min': -34.91608, 'max': 26.33055}, 33: {'min': -49.14885, 'max': 30.2937}, 34: {'min': 0.0, 'max': 1483.1273}, 35: {'min': 0.0, 'max': 4.48828}, 36: {'min': -70.34879, 'max': 34.10358}, 37: {'min': -48.65817, 'max': 59.08841}, 38: {'min': 0.0, 'max': 120.70314}, 39: {'min': 4.3541, 'max': 149.94182}, 40: {'min': -46.72699, 'max': 26.36337}, 41: {'min': -49.20552, 'max': 39.89194}, 42: {'min': 0.0, 'max': 1483.2484}, 43: {'min': 0.0, 'max': 4.49512}, 44: {'min': -70.34879, 'max': 34.10358}, 45: {'min': -49.31887, 'max': 59.08841}, 46: {'min': 0.0, 'max': 120.70314}, 47: {'min': 4.43518, 'max': 145.18393}, 48: {'min': -105.01499, 'max': 26.31066}, 49: {'min': -266.7571, 'max': 39.98653}, 50: {'min': 0.0, 'max': 1483.2484}, 51: {'min': 0.0, 'max': 4.50195}, 52: {'min': -215.97034, 'max': 26.42901}, 53: {'min': -542.4342, 'max': 16.17193}, 54: {'min': 0.0, 'max': 120.70314}, 55: {'min': 4.43518, 'max': 140.42604}, 56: {'min': -142.0001, 'max': 24.82811}, 57: {'min': -358.6495, 'max': 20.77295}, 58: {'min': 0.0, 'max': 1483.2484}, 59: {'min': 0.0, 'max': 4.5083}, 60: {'min': -215.97034, 'max': 23.34557}, 61: {'min': -542.4342, 'max': 22.23582}, 62: {'min': 0.0, 'max': 120.70314}, 63: {'min': 4.1681, 'max': 135.66815}, 64: {'min': -144.09656, 'max': 24.28766}, 65: {'min': -361.3071, 'max': 23.13564}, 66: {'min': 0.0, 'max': 1483.2484}, 67: {'min': 0.0, 'max': 4.51514}, 68: {'min': -29.81033, 'max': 40.77556}, 69: {'min': -43.2463, 'max': 46.27127}, 70: {'min': 0.0, 'max': 120.70314}, 71: {'min': 4.41509, 'max': 130.91028}, 72: {'min': -71.90334, 'max': 27.18371}, 73: {'min': -180.04134, 'max': 30.84752}, 74: {'min': 0.0, 'max': 1483.2484}, 75: {'min': 0.0, 'max': 4.52148}, 76: {'min': -29.81033, 'max': 40.77556}, 77: {'min': -37.16865, 'max': 46.27127}, 78: {'min': 0.0, 'max': 120.70314}, 79: {'min': 4.43518, 'max': 126.15238}, 80: {'min': -47.83894, 'max': 19.36316}, 81: {'min': -119.61942, 'max': 20.0126}, 82: {'min': 0.0, 'max': 1483.2484}, 83: {'min': 0.0, 'max': 4.52832}, 84: {'min': -30.96984, 'max': 20.7282}, 85: {'min': -32.50474, 'max': 22.30304}, 86: {'min': 0.0, 'max': 120.70314}, 87: {'min': 4.4391, 'max': 121.3945}, 88: {'min': -18.42727, 'max': 27.20675}, 89: {'min': -32.68973, 'max': 31.46919}, 90: {'min': 0.0, 'max': 1483.2484}, 91: {'min': 0.0, 'max': 4.53467}, 92: {'min': -29.81033, 'max': 37.55308}, 93: {'min': -30.50183, 'max': 42.92578}, 94: {'min': 0.0, 'max': 120.70314}, 95: {'min': 4.36079, 'max': 116.63661}, 96: {'min': -18.53542, 'max': 30.0162}, 97: {'min': -25.97153, 'max': 34.61985}, 98: {'min': 0.0, 'max': 1483.2484}, 99: {'min': 0.00671, 'max': 4.54199}, 100: {'min': -29.81033, 'max': 35.6351}, 101: {'min': -30.50183, 'max': 40.92116}, 102: {'min': 0.0, 'max': 120.70314}, 103: {'min': 4.37157, 'max': 111.87872}, 104: {'min': -23.08242, 'max': 36.56454}, 105: {'min': -24.63319, 'max': 41.88069}, 106: {'min': 0.0, 'max': 1483.2484}, 107: {'min': 0.00671, 'max': 4.54834}, 108: {'min': -33.63338, 'max': 36.50544}, 109: {'min': -35.4249, 'max': 41.79513}, 110: {'min': 0.0, 'max': 105.79845}, 111: {'min': 4.24784, 'max': 107.120834}, 112: {'min': -29.40847, 'max': 53.786}, 113: {'min': -31.00678, 'max': 46.50679}, 114: {'min': 0.0, 'max': 1483.2484}, 115: {'min': 0.00671, 'max': 4.55518}, 116: {'min': -35.73453, 'max': 111.86588}, 117: {'min': -37.38036, 'max': 100.66254}, 118: {'min': 0.0, 'max': 96.17146}, 119: {'min': 4.46283, 'max': 102.362946}, 120: {'min': -45.50705, 'max': 111.86588}, 121: {'min': -33.13131, 'max': 100.66254}, 122: {'min': 0.0, 'max': 1483.2484}, 123: {'min': 0.00671, 'max': 4.56152}, 124: {'min': -137.53714, 'max': 111.86588}, 125: {'min': -37.38036, 'max': 100.66254}, 126: {'min': 0.0, 'max': 96.17146}, 127: {'min': 4.50828, 'max': 105.78858}, 128: {'min': -34.94356, 'max': 111.86588}, 129: {'min': -36.63922, 'max': 100.66254}, 130: {'min': 0.0, 'max': 1483.2484}, 131: {'min': 0.00671, 'max': 4.56836}, 132: {'min': -33.3616, 'max': 14.80743}, 133: {'min': -35.15693, 'max': 28.639652}, 134: {'min': 0.0, 'max': 96.17146}, 135: {'min': 3.92201, 'max': 112.40097}, 136: {'min': -35.28956, 'max': 111.86588}, 137: {'min': -36.98656, 'max': 100.66254}, 138: {'min': 0.0, 'max': 1483.2484}, 139: {'min': 0.00671, 'max': 4.5752}, 140: {'min': -35.63556, 'max': 44.83504}, 141: {'min': -37.33392, 'max': 30.4442}, 142: {'min': 0.0, 'max': 49.14301}, 143: {'min': 4.34964, 'max': 119.01335}, 144: {'min': -35.40489, 'max': 16.50756}, 145: {'min': -56.81126, 'max': 15.76818}, 146: {'min': 0.0, 'max': 1483.2484}, 147: {'min': 0.00671, 'max': 4.58203}, 148: {'min': -85.88333, 'max': 22.63442}, 149: {'min': -172.39307, 'max': 32.248753}, 150: {'min': 0.0, 'max': 49.14301}, 151: {'min': 4.2405, 'max': 125.62573}, 152: {'min': -1190.9728, 'max': 13.09845}, 153: {'min': -3705.5376, 'max': 16.27255}, 154: {'min': 0.0, 'max': 1483.2484}, 155: {'min': 0.00671, 'max': 4.58203}, 156: {'min': -2382.5874, 'max': 16.41342}, 157: {'min': -7412.662, 'max': 21.60993}, 158: {'min': 0.0, 'max': 49.14301}, 159: {'min': 4.44629, 'max': 132.23811}, 160: {'min': -1588.1777, 'max': 15.01111}, 161: {'min': -4941.246, 'max': 18.6054}, 162: {'min': 0.0, 'max': 1483.1273}, 163: {'min': 0.00671, 'max': 4.58203}, 164: {'min': -2382.5874, 'max': 42.67863}, 165: {'min': -7412.662, 'max': 48.22748}, 166: {'min': 0.0, 'max': 55.10622}, 167: {'min': 4.26205, 'max': 138.8505}, 168: {'min': -1588.3298, 'max': 17.21437}, 169: {'min': -4941.3135, 'max': 21.10494}, 170: {'min': 0.0, 'max': 1483.1273}, 171: {'min': 0.00671, 'max': 4.58203}, 172: {'min': -21.23478, 'max': 42.67863}, 173: {'min': -26.51441, 'max': 48.22748}, 174: {'min': 0.0, 'max': 49.21943}, 175: {'min': 4.42671, 'max': 145.46288}, 176: {'min': -793.2078, 'max': 18.64524}, 177: {'min': -2469.9924, 'max': 19.63135}, 178: {'min': 0.0, 'max': 1483.1273}, 179: {'min': 0.00671, 'max': 4.28906}, 180: {'min': -35.89571, 'max': 19.4005}, 181: {'min': -47.14013, 'max': 14.29571}, 182: {'min': 0.0, 'max': 69.31743}, 183: {'min': 4.63778, 'max': 134.42265}, 184: {'min': -528.1672, 'max': 19.64892}, 185: {'min': -1646.2188, 'max': 13.10133}, 186: {'min': 0.0, 'max': 1483.1273}, 187: {'min': 0.00671, 'max': 4.28906}, 188: {'min': -34.06763, 'max': 32.65471}, 189: {'min': -77.41959, 'max': 21.72524}, 190: {'min': 0.0, 'max': 53.37851}, 191: {'min': 4.15256, 'max': 123.38242}, 192: {'min': -19.68957, 'max': 27.3227}, 193: {'min': -50.45885, 'max': 14.99423}, 194: {'min': 0.0, 'max': 1483.1273}, 195: {'min': 0.00671, 'max': 4.28906}, 196: {'min': -34.0406, 'max': 40.40388}, 197: {'min': -74.4092, 'max': 21.72524}, 198: {'min': 0.0, 'max': 49.14301}, 199: {'min': 4.15256, 'max': 128.10008}, 200: {'min': -58.05934, 'max': 27.83883}, 201: {'min': -262.87042, 'max': 16.33584}, 202: {'min': 0.0, 'max': 1483.1273}, 203: {'min': 0.00671, 'max': 4.28906}, 204: {'min': -174.18808, 'max': 29.92131}, 205: {'min': -790.6038, 'max': 21.421453}, 206: {'min': 0.0, 'max': 71.18565}, 207: {'min': 4.15256, 'max': 128.10008}, 208: {'min': -57.83787, 'max': 31.26296}, 209: {'min': -262.82428, 'max': 18.96572}, 210: {'min': 0.0, 'max': 1483.2484}, 211: {'min': 0.00671, 'max': 4.28906}, 212: {'min': -36.49799, 'max': 29.92131}, 213: {'min': -54.58651, 'max': 23.226004}, 214: {'min': 0.0, 'max': 71.18565}, 215: {'min': 4.43681, 'max': 128.10008}, 216: {'min': -45.96652, 'max': 31.12298}, 217: {'min': -130.88634, 'max': 14.06113}, 218: {'min': 0.0, 'max': 1483.2484}, 219: {'min': 0.00671, 'max': 4.28906}, 220: {'min': -45.96652, 'max': 46.86621}, 221: {'min': -56.57317, 'max': 25.030554}, 222: {'min': 0.0, 'max': 71.18565}, 223: {'min': 4.26107, 'max': 128.10008}, 224: {'min': -46.29395, 'max': 30.87523}, 225: {'min': -87.00229, 'max': 34.55603}, 226: {'min': 0.0, 'max': 1483.2484}, 227: {'min': 0.00671, 'max': 4.28906}, 228: {'min': -46.62137, 'max': 52.14228}, 229: {'min': -141.00714, 'max': 100.8566}, 230: {'min': 0.0, 'max': 71.18565}, 231: {'min': 4.62795, 'max': 128.10008}, 232: {'min': -3265.7722, 'max': 30.58081}, 233: {'min': -78.53788, 'max': 692.04846}, 234: {'min': 0.0, 'max': 1483.2484}, 235: {'min': 0.00671, 'max': 4.28906}, 236: {'min': -6531.9536, 'max': 79.11368}, 237: {'min': -141.00714, 'max': 1382.4773}, 238: {'min': 0.0, 'max': 71.18565}, 239: {'min': 4.33103, 'max': 128.10008}, 240: {'min': -4354.4995, 'max': 52.82858}, 241: {'min': -54.51345, 'max': 922.1914}, 242: {'min': 0.0, 'max': 1483.2484}, 243: {'min': 0.01123, 'max': 4.28906}, 244: {'min': -6531.9536, 'max': 79.11368}, 245: {'min': -53.22598, 'max': 1382.4773}, 246: {'min': 0.0, 'max': 71.18565}, 247: {'min': 4.23813, 'max': 128.10008}, 248: {'min': -4354.5034, 'max': 29.96094}, 249: {'min': -54.62948, 'max': 922.2036}, 250: {'min': 0.0, 'max': 1483.2484}, 251: {'min': 0.01172, 'max': 4.28906}, 252: {'min': -49.50463, 'max': 40.18666}, 253: {'min': -54.86152, 'max': 48.4235}, 254: {'min': 0.0, 'max': 71.18565}, 255: {'min': 4.52565, 'max': 128.10008}, 256: {'min': -2176.9473, 'max': 29.80494}, 257: {'min': -54.31634, 'max': 461.35608}, 258: {'min': 0.0, 'max': 1483.2484}, 259: {'min': 0.01318, 'max': 4.28906}, 260: {'min': -39.36811, 'max': 30.11281}, 261: {'min': -54.86152, 'max': 33.67267}, 262: {'min': 0.0, 'max': 71.18565}, 263: {'min': 4.11902, 'max': 173.73126}, 264: {'min': -1451.0952, 'max': 1377.625}, 265: {'min': -1172.9316, 'max': 307.74023}, 266: {'min': 0.0, 'max': 1483.2484}, 267: {'min': 0.00879, 'max': 4.28906}, 268: {'min': -39.36811, 'max': 4131.6353}, 269: {'min': -3521.4282, 'max': 35.85785}, 270: {'min': 0.0, 'max': 71.18565}, 271: {'min': 4.58329, 'max': 128.10008}, 272: {'min': -35.50549, 'max': 1377.8358}, 273: {'min': -1172.8933, 'max': 32.23598}, 274: {'min': 0.0, 'max': 1483.2484}, 275: {'min': 0.01147, 'max': 4.18701}, 276: {'min': -56.36264, 'max': 155.98381}, 277: {'min': -88.34991, 'max': 48.02289}, 278: {'min': 0.0, 'max': 85.07389}, 279: {'min': 4.58329, 'max': 128.10008}, 280: {'min': -43.51957, 'max': 689.6065}, 281: {'min': -585.5629, 'max': 40.90836}, 282: {'min': 0.0, 'max': 1483.2484}, 283: {'min': 0.01245, 'max': 4.19385}, 284: {'min': -132.00795, 'max': 29.55464}, 285: {'min': -117.85101, 'max': 48.02289}, 286: {'min': 0.0, 'max': 93.03178}, 287: {'min': 4.1645, 'max': 128.10008}, 288: {'min': -34.1641, 'max': 459.75397}, 289: {'min': -389.9072, 'max': 36.83646}, 290: {'min': 0.0, 'max': 1483.2484}, 291: {'min': 0.0127, 'max': 4.2002}, 292: {'min': -33.50392, 'max': 32.51362}, 293: {'min': -51.84606, 'max': 58.22239}, 294: {'min': 0.0, 'max': 71.18565}, 295: {'min': 4.1645, 'max': 77.88287}, 296: {'min': -49.59118, 'max': 29.2652}, 297: {'min': -53.31129, 'max': 39.51175}, 298: {'min': 0.0, 'max': 1483.2484}, 299: {'min': 0.01318, 'max': 4.20654}, 300: {'min': -145.34612, 'max': 32.51362}, 301: {'min': -51.70164, 'max': 114.97408}, 302: {'min': 0.0, 'max': 71.18565}, 303: {'min': 4.33612, 'max': 77.88287}, 304: {'min': -49.56414, 'max': 28.40213}, 305: {'min': -51.74979, 'max': 39.54118}, 306: {'min': 0.0, 'max': 1483.2484}, 307: {'min': 0.01025, 'max': 4.21338}, 308: {'min': -40.2796, 'max': 28.37703}, 309: {'min': -51.70164, 'max': 33.01593}, 310: {'min': 0.0, 'max': 71.18565}, 311: {'min': 4.17035, 'max': 84.36928}, 312: {'min': -38.46487, 'max': 28.40213}, 313: {'min': -51.74979, 'max': 32.72471}, 314: {'min': 0.0, 'max': 1483.2484}, 315: {'min': 0.01099, 'max': 4.22021}, 316: {'min': -40.2796, 'max': 28.37703}, 317: {'min': -51.70164, 'max': 32.51728}, 318: {'min': 0.0, 'max': 71.18565}, 319: {'min': 4.56996, 'max': 88.77907}, 320: {'min': -37.79916, 'max': 27.60367}, 321: {'min': -50.28956, 'max': 34.71622}, 322: {'min': 0.0, 'max': 1483.2484}, 323: {'min': 0.01221, 'max': 4.22705}, 324: {'min': -34.93856, 'max': 38.67519}, 325: {'min': -48.82934, 'max': 38.69923}, 326: {'min': 0.0, 'max': 71.18565}, 327: {'min': 4.69641, 'max': 82.93289}, 328: {'min': -38.59029, 'max': 26.8052}, 329: {'min': -48.82934, 'max': 36.63859}, 330: {'min': 0.0, 'max': 1483.2484}, 331: {'min': 0.00952, 'max': 4.2334}, 332: {'min': -39.38141, 'max': 26.8052}, 333: {'min': -48.82934, 'max': 38.69923}, 334: {'min': 0.0, 'max': 71.18565}, 335: {'min': 4.42194, 'max': 77.88287}, 336: {'min': -38.854, 'max': 26.46956}, 337: {'min': -48.10005, 'max': 24.90815}, 338: {'min': 0.0, 'max': 1483.2484}, 339: {'min': 0.01086, 'max': 4.24023}, 340: {'min': -39.38141, 'max': 30.23671}, 341: {'min': -47.37077, 'max': 24.90815}, 342: {'min': 0.0, 'max': 71.18565}, 343: {'min': 4.42194, 'max': 77.88287}, 344: {'min': -36.86157, 'max': 25.17448}, 345: {'min': -45.63573, 'max': 23.01093}, 346: {'min': 0.0, 'max': 1483.2484}, 347: {'min': 0.0127, 'max': 4.24707}, 348: {'min': -31.82189, 'max': 22.58433}, 349: {'min': -40.70709, 'max': 23.01093}, 350: {'min': 0.0, 'max': 71.18565}, 351: {'min': 4.47987, 'max': 77.88287}, 352: {'min': -30.51137, 'max': 22.66129}, 353: {'min': -39.03241, 'max': 27.1872}, 354: {'min': 0.0, 'max': 1483.2484}, 355: {'min': 0.01306, 'max': 4.25342}, 356: {'min': -29.81033, 'max': 29.8148}, 357: {'min': -35.68939, 'max': 34.77146}, 358: {'min': 0.0, 'max': 71.18565}, 359: {'min': 4.40878, 'max': 77.88287}, 360: {'min': -28.39464, 'max': 23.9703}, 361: {'min': -32.18962, 'max': 28.5793}, 362: {'min': 0.0, 'max': 1483.2484}, 363: {'min': 0.01123, 'max': 4.26025}, 364: {'min': -34.76727, 'max': 29.8148}, 365: {'min': -30.6186, 'max': 37.62122}, 366: {'min': 0.0, 'max': 71.18565}, 367: {'min': 4.49309, 'max': 77.88287}, 368: {'min': -25.47912, 'max': 26.58833}, 369: {'min': -26.31063, 'max': 31.36348}, 370: {'min': 0.0, 'max': 1483.2484}, 371: {'min': 0.01135, 'max': 4.26709}, 372: {'min': -35.62983, 'max': 26.58833}, 373: {'min': -36.31362, 'max': 31.36348}, 374: {'min': 0.0, 'max': 71.18565}, 375: {'min': 4.23903, 'max': 77.88287}, 376: {'min': -33.72584, 'max': 21.13974}, 377: {'min': -42.63425, 'max': 25.04523}, 378: {'min': 0.0, 'max': 1483.2484}, 379: {'min': 0.01282, 'max': 4.27393}, 380: {'min': -61.02475, 'max': 29.69063}, 381: {'min': -111.83044, 'max': 34.65604}, 382: {'min': 0.0, 'max': 71.18565}, 383: {'min': 4.3019, 'max': 77.88287}, 384: {'min': -52.23614, 'max': 22.27815}, 385: {'min': -80.39252, 'max': 29.62831}, 386: {'min': 0.0, 'max': 1483.2484}, 387: {'min': 0.01294, 'max': 4.28076}, 388: {'min': -70.8593, 'max': 24.55497}, 389: {'min': -111.83044, 'max': 45.28257}, 390: {'min': 0.0, 'max': 75.8474}, 391: {'min': 4.2393, 'max': 77.88287}, 392: {'min': -70.43185, 'max': 72.0694}, 393: {'min': -96.41946, 'max': 17.6926}, 394: {'min': 0.0, 'max': 1483.2484}, 395: {'min': 0.0127, 'max': 4.2876}, 396: {'min': -88.62756, 'max': 144.34401}, 397: {'min': -295.51587, 'max': 43.92981}, 398: {'min': 0.0, 'max': 75.8474}, 399: {'min': 4.35552, 'max': 77.88287}, 400: {'min': -76.01972, 'max': 96.16094}, 401: {'min': -195.96767, 'max': 10.12156}, 402: {'min': 0.0, 'max': 1483.2484}, 403: {'min': 0.00842, 'max': 4.29395}, 404: {'min': -87.19547, 'max': 144.34401}, 405: {'min': -295.51587, 'max': 22.79928}, 406: {'min': 0.0, 'max': 75.8474}, 407: {'min': 4.55083, 'max': 77.88287}, 408: {'min': -87.67284, 'max': 96.39459}, 409: {'min': -130.26987, 'max': 14.13465}, 410: {'min': 0.0, 'max': 1483.2484}, 411: {'min': 0.01306, 'max': 4.30078}, 412: {'min': -87.19547, 'max': 31.75742}, 413: {'min': -87.30433, 'max': 22.79928}, 414: {'min': 0.0, 'max': 75.8474}, 415: {'min': 4.46181, 'max': 77.88287}, 416: {'min': -89.73198, 'max': 48.32415}, 417: {'min': -196.65692, 'max': 23.22577}, 418: {'min': 0.0, 'max': 1483.4222}, 419: {'min': 0.01099, 'max': 4.30713}, 420: {'min': -91.79111, 'max': 34.0101}, 421: {'min': -445.82098, 'max': 24.07876}, 422: {'min': 0.0, 'max': 75.8474}, 423: {'min': 4.3097, 'max': 77.88287}, 424: {'min': -89.9977, 'max': 32.30067}, 425: {'min': -246.11877, 'max': 23.22577}, 426: {'min': 0.0, 'max': 1483.4222}, 427: {'min': 0.01294, 'max': 4.31396}, 428: {'min': -90.52911, 'max': 33.27264}, 429: {'min': -295.58063, 'max': 24.07876}, 430: {'min': 0.0, 'max': 75.8474}, 431: {'min': 4.3097, 'max': 77.88287}, 432: {'min': -85.41795, 'max': 33.51846}, 433: {'min': -163.82951, 'max': 23.22577}, 434: {'min': 0.0, 'max': 1483.4222}, 435: {'min': 0.00952, 'max': 4.3208}, 436: {'min': -73.93361, 'max': 33.27264}, 437: {'min': -74.0808, 'max': 24.07876}, 438: {'min': 0.0, 'max': 113.44984}, 439: {'min': 4.13583, 'max': 77.88287}, 440: {'min': -64.5461, 'max': 41.25205}, 441: {'min': -97.7532, 'max': 13.94661}, 442: {'min': 0.0, 'max': 1483.4222}, 443: {'min': 0.01123, 'max': 4.32764}, 444: {'min': -44.10818, 'max': 48.98565}, 445: {'min': -89.93092, 'max': 17.0296}, 446: {'min': 0.0, 'max': 113.44984}, 447: {'min': 4.56866, 'max': 77.88287}, 448: {'min': -50.59399, 'max': 37.26093}, 449: {'min': -68.14658, 'max': 14.36658}, 450: {'min': 0.0, 'max': 1483.4222}, 451: {'min': 0.01099, 'max': 4.33398}, 452: {'min': -70.50581, 'max': 29.27867}, 453: {'min': -133.92677, 'max': 22.32371}, 454: {'min': 0.0, 'max': 113.44984}, 455: {'min': 4.46465, 'max': 77.88287}, 456: {'min': -95.43536, 'max': 35.84766}, 457: {'min': -445.35434, 'max': 15.13838}, 458: {'min': 0.0, 'max': 1483.4222}, 459: {'min': 0.01294, 'max': 4.34082}, 460: {'min': -287.5036, 'max': 29.27867}, 461: {'min': -1338.6283, 'max': 16.83554}, 462: {'min': 0.0, 'max': 113.44984}, 463: {'min': 4.35358, 'max': 77.88287}, 464: {'min': -47.64359, 'max': 32.04138}, 465: {'min': -221.96886, 'max': 13.32575}, 466: {'min': 0.0, 'max': 1483.4222}, 467: {'min': 0.0094, 'max': 4.34766}, 468: {'min': -29.81033, 'max': 28.2351}, 469: {'min': -51.3184, 'max': 18.59416}, 470: {'min': 0.0, 'max': 113.44984}, 471: {'min': 4.32429, 'max': 77.88287}, 472: {'min': -31.71299, 'max': 30.90181}, 473: {'min': -147.50703, 'max': 17.34664}, 474: {'min': 0.0, 'max': 1483.4222}, 475: {'min': 0.01294, 'max': 4.354}, 476: {'min': -29.81033, 'max': 28.62265}, 477: {'min': -52.19678, 'max': 22.17192}, 478: {'min': 0.0, 'max': 113.44984}, 479: {'min': 4.47555, 'max': 107.14943}, 480: {'min': -16.09494, 'max': 24.50271}, 481: {'min': -44.63684, 'max': 15.11146}, 482: {'min': 0.0, 'max': 1483.4222}, 483: {'min': 0.01184, 'max': 4.36182}, 484: {'min': -29.81033, 'max': 17.97006}, 485: {'min': -30.50183, 'max': 22.17192}, 486: {'min': 0.0, 'max': 124.04291}, 487: {'min': 4.44614, 'max': 105.19846}, 488: {'min': -15.79886, 'max': 17.72834}, 489: {'min': -32.47882, 'max': 22.10658}, 490: {'min': 0.0, 'max': 1483.4222}, 491: {'min': 0.00861, 'max': 4.36768}, 492: {'min': -29.81033, 'max': 17.80129}, 493: {'min': -30.50183, 'max': 22.10658}, 494: {'min': 0.0, 'max': 113.44984}, 495: {'min': 4.47555, 'max': 152.49142}, 496: {'min': -1333697.2, 'max': 15.86796}, 497: {'min': -4659087.5, 'max': 22.10658}, 498: {'min': 0.0, 'max': 1483.4222}, 499: {'min': 0.01099, 'max': 4.37451}, 500: {'min': -2667395.2, 'max': 21.66643}, 501: {'min': -9318176.0, 'max': 23.75259}, 502: {'min': 0.0, 'max': 113.44984}, 503: {'min': 4.32861, 'max': 105.19846}, 504: {'min': -1778263.4, 'max': 16.75938}, 505: {'min': -6212117.5, 'max': 18.60124}, 506: {'min': 0.0, 'max': 1483.4222}, 507: {'min': 0.01196, 'max': 4.38184}, 508: {'min': -2667395.2, 'max': 29.28431}, 509: {'min': -9318176.0, 'max': 23.75259}, 510: {'min': 0.0, 'max': 113.44984}, 511: {'min': 4.47555, 'max': 105.19846}, 512: {'min': -1778263.4, 'max': 20.18364}, 513: {'min': -6212117.0, 'max': 24.67029}, 514: {'min': 0.0, 'max': 1483.4222}, 515: {'min': 0.01099, 'max': 4.3877}, 516: {'min': -29.81033, 'max': 30.183}, 517: {'min': -30.50183, 'max': 26.50569}, 518: {'min': 0.0, 'max': 113.44984}, 519: {'min': 4.36444, 'max': 105.19846}, 520: {'min': -889131.25, 'max': 20.51782}, 521: {'min': -3106058.2, 'max': 25.02766}, 522: {'min': 0.0, 'max': 1483.4222}, 523: {'min': 0.00977, 'max': 4.39453}, 524: {'min': -127.27333, 'max': 20.85199}, 525: {'min': -275.52518, 'max': 25.38503}, 526: {'min': 0.0, 'max': 113.44984}, 527: {'min': 4.47555, 'max': 105.19846}, 528: {'min': -592753.9, 'max': 25.39013}, 529: {'min': -2070705.4, 'max': 25.14678}, 530: {'min': 0.0, 'max': 1483.4222}, 531: {'min': 0.01123, 'max': 4.40088}, 532: {'min': -127.27333, 'max': 75.48418}, 533: {'min': -275.52518, 'max': 33.92534}, 534: {'min': 0.0, 'max': 113.44984}, 535: {'min': 4.35794, 'max': 173.73126}, 536: {'min': -275.02484, 'max': 50.42647}, 537: {'min': -810.1242, 'max': 22.80772}, 538: {'min': 0.0, 'max': 1483.4222}, 539: {'min': 0.01306, 'max': 4.40771}, 540: {'min': -827.0123, 'max': 75.48418}, 541: {'min': -2433.1802, 'max': 33.92534}, 542: {'min': 0.0, 'max': 113.44984}, 543: {'min': 4.47555, 'max': 173.73126}, 544: {'min': -551.01855, 'max': 25.40761}, 545: {'min': -1621.6522, 'max': 13.37783}, 546: {'min': 0.0, 'max': 1483.4222}, 547: {'min': 0.01147, 'max': 4.41455}, 548: {'min': -827.0123, 'max': 17.64091}, 549: {'min': -2433.1802, 'max': 12.592265}, 550: {'min': 0.0, 'max': 113.44984}, 551: {'min': 2.41818, 'max': 173.73126}, 552: {'min': -367.05438, 'max': 18.06695}, 553: {'min': -1080.4485, 'max': 31.87079}, 554: {'min': 0.0, 'max': 1483.4222}, 555: {'min': 0.01294, 'max': 4.4209}, 556: {'min': -37.76155, 'max': 28.56683}, 557: {'min': -53.54108, 'max': 92.73811}, 558: {'min': 0.0, 'max': 113.44984}, 559: {'min': 4.39544, 'max': 173.73126}, 560: {'min': -275.06195, 'max': 238.29015}, 561: {'min': -809.92114, 'max': 16.37098}, 562: {'min': 0.0, 'max': 1483.4222}, 563: {'min': 0.01172, 'max': 4.42822}, 564: {'min': -37.76155, 'max': 713.88495}, 565: {'min': -516.264, 'max': 13.20015}, 566: {'min': 0.0, 'max': 113.44984}, 567: {'min': 4.47555, 'max': 173.73126}, 568: {'min': -145.69618, 'max': 476.14194}, 569: {'min': -404.23184, 'max': 12.67362}, 570: {'min': 0.0, 'max': 1483.5494}, 571: {'min': 0.01147, 'max': 4.43506}, 572: {'min': -290.99066, 'max': 713.88495}, 573: {'min': -516.264, 'max': 16.78551}, 574: {'min': 0.0, 'max': 120.70314}, 575: {'min': 4.47555, 'max': 173.73126}, 576: {'min': -96.35835, 'max': 238.2247}, 577: {'min': -269.0017, 'max': 15.2054}, 578: {'min': 0.0, 'max': 1483.5494}, 579: {'min': 0.01343, 'max': 4.44141}, 580: {'min': -37.76155, 'max': 248.47101}, 581: {'min': -193.98975, 'max': 16.78551}, 582: {'min': 0.0, 'max': 120.70314}, 583: {'min': 4.17938, 'max': 173.73126}, 584: {'min': -95.45201, 'max': 158.89932}, 585: {'min': -114.15302, 'max': 15.8119}, 586: {'min': 0.0, 'max': 1483.5494}, 587: {'min': 0.00977, 'max': 4.44824}, 588: {'min': -37.76155, 'max': 40.9241}, 589: {'min': -74.82268, 'max': 16.41839}, 590: {'min': 0.0, 'max': 120.70314}, 591: {'min': 4.28174, 'max': 173.73126}, 592: {'min': -47.42076, 'max': 42.39059}, 593: {'min': -103.23438, 'max': 14.19813}, 594: {'min': 0.0, 'max': 1483.5494}, 595: {'min': 0.01135, 'max': 4.45459}, 596: {'min': -37.76155, 'max': 38.57898}, 597: {'min': -208.19109, 'max': 14.65003}, 598: {'min': 4.49964, 'max': 120.70314}, 599: {'min': 4.43518, 'max': 173.73126}}"
class 0 weight: 2.7207462571185608
class 1 weight: 1.5811432079907755
*****************************************
stacked_lstm_v2
Model: "sequential_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_11 (LSTM)              (None, 2, 600)            2882400   
                                                                 
 lstm_12 (LSTM)              (None, 2, 600)            2882400   
                                                                 
 lstm_13 (LSTM)              (None, 2, 600)            2882400   
                                                                 
 lstm_14 (LSTM)              (None, 2, 600)            2882400   
                                                                 
 lstm_15 (LSTM)              (None, 600)               2882400   
                                                                 
 dense_21 (Dense)            (None, 1200)              721200    
                                                                 
 leaky_re_lu_1 (LeakyReLU)   (None, 1200)              0         
                                                                 
 dense_22 (Dense)            (None, 600)               720600    
                                                                 
 leaky_re_lu_2 (LeakyReLU)   (None, 600)               0         
                                                                 
 dense_23 (Dense)            (None, 1)                 601       
                                                                 
=================================================================
Total params: 15,854,401
Trainable params: 15,854,401
Non-trainable params: 0
_________________________________________________________________
*****************************************
-------------------------------
unique model id: stacked_lstm_v2-Adagrad0,008 wdecay: None ema:False
optimizer: Adagrad<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.008>
-------------------------------
trainig on file: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0206422018348624
class 1 weight: 49.44444444444444
Score for fold 0: loss of 0.6956198811531067; accuracy of 1.9999999552965164%
(495, 2, 600)
class 0 weight: 1.0206422018348624
class 1 weight: 49.44444444444444
Score for fold 1: loss of 0.6957100033760071; accuracy of 1.9999999552965164%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0206422018348624
class 1 weight: 49.44444444444444
Score for fold 2: loss of 0.6984703540802002; accuracy of 1.9999999552965164%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0206422018348624
class 1 weight: 49.44444444444444
Score for fold 3: loss of 0.6999380588531494; accuracy of 1.9999999552965164%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0206422018348624
class 1 weight: 49.44444444444444
Score for fold 4: loss of 0.7003598809242249; accuracy of 1.9999999552965164%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0205949656750573
class 1 weight: 49.55555555555555
Score for fold 5: loss of 0.7002682089805603; accuracy of 2.0408162847161293%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0205949656750573
class 1 weight: 49.55555555555555
Score for fold 6: loss of 0.6996866464614868; accuracy of 2.0408162847161293%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0205949656750573
class 1 weight: 49.55555555555555
Score for fold 7: loss of 0.6985109448432922; accuracy of 26.530611515045166%
(495, 2, 600)
class 0 weight: 1.0205949656750573
class 1 weight: 49.55555555555555
Score for fold 8: loss of 0.6987379193305969; accuracy of 16.326530277729034%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P52 p52.anatomy.matrix.csv.arff
(495, 2, 600)
class 0 weight: 1.0205949656750573
class 1 weight: 49.55555555555555
Score for fold 9: loss of 0.6957873702049255; accuracy of 30.612245202064514%
trainig on file: trainData_500.0mssec_P55 p55.anatomy.list.csv.arff
(367, 2, 600)
class 0 weight: 2.0245398773006134
class 1 weight: 1.9760479041916168
Score for fold 0: loss of 0.6950583457946777; accuracy of 48.64864945411682%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P55 p55.anatomy.list.csv.arff
(367, 2, 600)
class 0 weight: 2.0245398773006134
class 1 weight: 1.9760479041916168
Score for fold 1: loss of 0.6859153509140015; accuracy of 48.64864945411682%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P55 p55.anatomy.list.csv.arff
(367, 2, 600)
class 0 weight: 2.0245398773006134
class 1 weight: 1.9760479041916168
Score for fold 2: loss of 0.5377796292304993; accuracy of 70.27027010917664%
(367, 2, 600)
class 0 weight: 2.0245398773006134
class 1 weight: 1.9760479041916168
Score for fold 3: loss of 0.6111575961112976; accuracy of 59.45945978164673%
(367, 2, 600)
class 0 weight: 2.0245398773006134
class 1 weight: 1.9760479041916168
Score for fold 4: loss of 0.6439386606216431; accuracy of 56.75675868988037%
(367, 2, 600)
class 0 weight: 2.0245398773006134
class 1 weight: 1.9760479041916168
Score for fold 5: loss of 0.574447512626648; accuracy of 62.162160873413086%
(367, 2, 600)
class 0 weight: 2.0370370370370368
class 1 weight: 1.9642857142857142
Score for fold 6: loss of 0.5458403825759888; accuracy of 64.86486196517944%
(367, 2, 600)
class 0 weight: 2.030674846625767
class 1 weight: 1.9702380952380951
Score for fold 7: loss of 0.5094017386436462; accuracy of 69.44444179534912%
(367, 2, 600)
class 0 weight: 2.030674846625767
class 1 weight: 1.9702380952380951
Score for fold 8: loss of 0.47637155652046204; accuracy of 72.22222089767456%
(367, 2, 600)
class 0 weight: 2.030674846625767
class 1 weight: 1.9702380952380951
Score for fold 9: loss of 0.5117297172546387; accuracy of 72.22222089767456%
trainig on file: trainData_500.0mssec_P59 p59.conf.list.csv.arff
(1534, 2, 600)
class 0 weight: 3.8016528925619837
class 1 weight: 1.3569321533923304
Score for fold 0: loss of 0.7048688530921936; accuracy of 26.623377203941345%
(1534, 2, 600)
class 0 weight: 3.8016528925619837
class 1 weight: 1.3569321533923304
Score for fold 1: loss of 0.705836296081543; accuracy of 69.48052048683167%
(1534, 2, 600)
class 0 weight: 3.8016528925619837
class 1 weight: 1.3569321533923304
Score for fold 2: loss of 0.6967483162879944; accuracy of 74.02597665786743%
(1534, 2, 600)
class 0 weight: 3.8016528925619837
class 1 weight: 1.3569321533923304
Score for fold 3: loss of 0.7046398520469666; accuracy of 70.77922224998474%
(1534, 2, 600)
class 0 weight: 3.7939560439560442
class 1 weight: 1.3579154375614553
Score for fold 4: loss of 0.6756510734558105; accuracy of 76.47058963775635%
(1534, 2, 600)
class 0 weight: 3.7939560439560442
class 1 weight: 1.3579154375614553
Score for fold 5: loss of 0.6666256785392761; accuracy of 77.1241843700409%
(1534, 2, 600)
class 0 weight: 3.7939560439560442
class 1 weight: 1.3579154375614553
Score for fold 6: loss of 0.6843065023422241; accuracy of 74.50980544090271%
(1534, 2, 600)
class 0 weight: 3.7939560439560442
class 1 weight: 1.3579154375614553
Score for fold 7: loss of 0.6855140924453735; accuracy of 74.50980544090271%
(1534, 2, 600)
class 0 weight: 3.7939560439560442
class 1 weight: 1.3579154375614553
Score for fold 8: loss of 0.7043117880821228; accuracy of 69.93464231491089%
(1534, 2, 600)
class 0 weight: 3.7939560439560442
class 1 weight: 1.3579154375614553
Score for fold 9: loss of 0.6757969260215759; accuracy of 73.20261597633362%
trainig on file: trainData_500.0mssec_P17 p17.conf.list.csv.arff
(792, 2, 600)
class 0 weight: 1.9453551912568305
class 1 weight: 2.0578034682080926
Score for fold 0: loss of 0.6417120695114136; accuracy of 58.74999761581421%
(792, 2, 600)
class 0 weight: 1.9453551912568305
class 1 weight: 2.0578034682080926
Score for fold 1: loss of 0.6052233576774597; accuracy of 57.499998807907104%
(792, 2, 600)
class 0 weight: 1.9427792915531334
class 1 weight: 2.060693641618497
Score for fold 2: loss of 0.48099711537361145; accuracy of 67.08860993385315%
(792, 2, 600)
class 0 weight: 1.9427792915531334
class 1 weight: 2.060693641618497
Score for fold 3: loss of 0.5472924113273621; accuracy of 67.08860993385315%
(792, 2, 600)
class 0 weight: 1.9427792915531334
class 1 weight: 2.060693641618497
Score for fold 4: loss of 0.48519274592399597; accuracy of 68.35442781448364%
(792, 2, 600)
class 0 weight: 1.9480874316939891
class 1 weight: 2.0547550432276656
Score for fold 5: loss of 0.5219452381134033; accuracy of 68.35442781448364%
(792, 2, 600)
class 0 weight: 1.9480874316939891
class 1 weight: 2.0547550432276656
Score for fold 6: loss of 0.5274961590766907; accuracy of 63.29113841056824%
(792, 2, 600)
class 0 weight: 1.9480874316939891
class 1 weight: 2.0547550432276656
Score for fold 7: loss of 0.4878653883934021; accuracy of 68.35442781448364%
(792, 2, 600)
class 0 weight: 1.9480874316939891
class 1 weight: 2.0547550432276656
Score for fold 8: loss of 0.5632734894752502; accuracy of 62.02531456947327%
(792, 2, 600)
class 0 weight: 1.9480874316939891
class 1 weight: 2.0547550432276656
Score for fold 9: loss of 0.5253868699073792; accuracy of 59.4936728477478%
trainig on file: trainData_500.0mssec_P8 p8.anatomy.matrix.csv.arff
(527, 2, 600)
class 0 weight: 8.777777777777777
class 1 weight: 1.1285714285714288
Score for fold 0: loss of 0.35005444288253784; accuracy of 79.24528121948242%
(527, 2, 600)
class 0 weight: 8.777777777777777
class 1 weight: 1.1285714285714288
Score for fold 1: loss of 0.3983313739299774; accuracy of 77.35849022865295%
(527, 2, 600)
class 0 weight: 8.777777777777777
class 1 weight: 1.1285714285714288
Score for fold 2: loss of 0.2798641324043274; accuracy of 90.56603908538818%
(527, 2, 600)
class 0 weight: 8.777777777777777
class 1 weight: 1.1285714285714288
Score for fold 3: loss of 0.4099988341331482; accuracy of 62.26415038108826%
(527, 2, 600)
class 0 weight: 8.777777777777777
class 1 weight: 1.1285714285714288
Score for fold 4: loss of 0.29104772210121155; accuracy of 67.92452931404114%
(527, 2, 600)
class 0 weight: 8.777777777777777
class 1 weight: 1.1285714285714288
Score for fold 5: loss of 0.5215199589729309; accuracy of 66.03773832321167%
(527, 2, 600)
class 0 weight: 8.777777777777777
class 1 weight: 1.1285714285714288
Score for fold 6: loss of 0.4103687107563019; accuracy of 64.15094137191772%
(527, 2, 600)
class 0 weight: 8.796296296296296
class 1 weight: 1.1282660332541568
Score for fold 7: loss of 0.2626939117908478; accuracy of 80.7692289352417%
(527, 2, 600)
class 0 weight: 8.796296296296296
class 1 weight: 1.1282660332541568
Score for fold 8: loss of 0.338765412569046; accuracy of 71.15384340286255%
(527, 2, 600)
class 0 weight: 8.796296296296296
class 1 weight: 1.1282660332541568
Score for fold 9: loss of 0.33739471435546875; accuracy of 73.07692170143127%
trainig on file: trainData_500.0mssec_P58 p58.conf.matrix.csv.arff
(1309, 2, 600)
class 0 weight: 1.6876790830945558
class 1 weight: 2.4541666666666666
Score for fold 0: loss of 0.4566463530063629; accuracy of 83.20610523223877%
(1309, 2, 600)
class 0 weight: 1.6876790830945558
class 1 weight: 2.4541666666666666
Score for fold 1: loss of 0.5140566825866699; accuracy of 74.80915784835815%
(1309, 2, 600)
class 0 weight: 1.6876790830945558
class 1 weight: 2.4541666666666666
Score for fold 2: loss of 0.4758392572402954; accuracy of 75.57252049446106%
(1309, 2, 600)
class 0 weight: 1.6876790830945558
class 1 weight: 2.4541666666666666
Score for fold 3: loss of 0.6051504015922546; accuracy of 60.30534505844116%
(1309, 2, 600)
class 0 weight: 1.6876790830945558
class 1 weight: 2.4541666666666666
Score for fold 4: loss of 0.5374196171760559; accuracy of 70.22900581359863%
(1309, 2, 600)
class 0 weight: 1.6876790830945558
class 1 weight: 2.4541666666666666
Score for fold 5: loss of 0.3869225084781647; accuracy of 80.15267252922058%
(1309, 2, 600)
class 0 weight: 1.6852646638054363
class 1 weight: 2.4592901878914404
Score for fold 6: loss of 0.528190553188324; accuracy of 71.75572514533997%
(1309, 2, 600)
class 0 weight: 1.6852646638054363
class 1 weight: 2.4592901878914404
Score for fold 7: loss of 0.40348079800605774; accuracy of 78.62595319747925%
(1309, 2, 600)
class 0 weight: 1.6852646638054363
class 1 weight: 2.4592901878914404
Score for fold 8: loss of 0.5344426035881042; accuracy of 73.2824444770813%
(1309, 2, 600)
class 0 weight: 1.686695278969957
class 1 weight: 2.45625
Score for fold 9: loss of 0.3823188841342926; accuracy of 80.0000011920929%
trainig on file: trainData_500.0mssec_P55 p55.conf.list.csv.arff
(969, 2, 600)
class 0 weight: 1.7723577235772359
class 1 weight: 2.294736842105263
Score for fold 0: loss of 0.5863563418388367; accuracy of 67.01030731201172%
(969, 2, 600)
class 0 weight: 1.7723577235772359
class 1 weight: 2.294736842105263
Score for fold 1: loss of 0.438602477312088; accuracy of 78.35051417350769%
(969, 2, 600)
class 0 weight: 1.7723577235772359
class 1 weight: 2.294736842105263
Score for fold 2: loss of 0.5469123125076294; accuracy of 72.16494679450989%
(969, 2, 600)
class 0 weight: 1.775967413441955
class 1 weight: 2.288713910761155
Score for fold 3: loss of 0.4155849516391754; accuracy of 78.35051417350769%
(969, 2, 600)
class 0 weight: 1.775967413441955
class 1 weight: 2.288713910761155
Score for fold 4: loss of 0.43541210889816284; accuracy of 78.35051417350769%
(969, 2, 600)
class 0 weight: 1.775967413441955
class 1 weight: 2.288713910761155
Score for fold 5: loss of 0.3544974625110626; accuracy of 82.47422575950623%
(969, 2, 600)
class 0 weight: 1.775967413441955
class 1 weight: 2.288713910761155
Score for fold 6: loss of 0.3780919909477234; accuracy of 78.35051417350769%
(969, 2, 600)
class 0 weight: 1.775967413441955
class 1 weight: 2.288713910761155
Score for fold 7: loss of 0.41854771971702576; accuracy of 79.38144207000732%
(969, 2, 600)
class 0 weight: 1.775967413441955
class 1 weight: 2.288713910761155
Score for fold 8: loss of 0.3826698958873749; accuracy of 81.44329786300659%
(969, 2, 600)
class 0 weight: 1.7743902439024393
class 1 weight: 2.2913385826771653
Score for fold 9: loss of 0.3972128629684448; accuracy of 82.29166865348816%
trainig on file: trainData_500.0mssec_P7 p7.conf.list.csv.arff
(1218, 2, 600)
class 0 weight: 4.08955223880597
class 1 weight: 1.323671497584541
Score for fold 0: loss of 0.45556649565696716; accuracy of 79.50819730758667%
(1218, 2, 600)
class 0 weight: 4.104868913857678
class 1 weight: 1.322074788902292
Score for fold 1: loss of 0.5131706595420837; accuracy of 75.40983557701111%
(1218, 2, 600)
class 0 weight: 4.104868913857678
class 1 weight: 1.322074788902292
Score for fold 2: loss of 0.4377375543117523; accuracy of 80.32786846160889%
(1218, 2, 600)
class 0 weight: 4.104868913857678
class 1 weight: 1.322074788902292
Score for fold 3: loss of 0.4321320354938507; accuracy of 77.04917788505554%
(1218, 2, 600)
class 0 weight: 4.104868913857678
class 1 weight: 1.322074788902292
Score for fold 4: loss of 0.4371074438095093; accuracy of 78.68852615356445%
(1218, 2, 600)
class 0 weight: 4.104868913857678
class 1 weight: 1.322074788902292
Score for fold 5: loss of 0.3930285573005676; accuracy of 78.68852615356445%
(1218, 2, 600)
class 0 weight: 4.104868913857678
class 1 weight: 1.322074788902292
Score for fold 6: loss of 0.5212129950523376; accuracy of 74.59016442298889%
(1218, 2, 600)
class 0 weight: 4.104868913857678
class 1 weight: 1.322074788902292
Score for fold 7: loss of 0.35679614543914795; accuracy of 80.32786846160889%
(1218, 2, 600)
class 0 weight: 4.093283582089552
class 1 weight: 1.3232810615199033
Score for fold 8: loss of 0.39268115162849426; accuracy of 80.99173307418823%
(1218, 2, 600)
class 0 weight: 4.093283582089552
class 1 weight: 1.3232810615199033
Score for fold 9: loss of 0.4688001573085785; accuracy of 76.03305578231812%
trainig on file: trainData_500.0mssec_P34 p34.conf.matrix.csv.arff
(903, 2, 600)
class 0 weight: 1.5952848722986246
class 1 weight: 2.67986798679868
Score for fold 0: loss of 0.520075261592865; accuracy of 71.42857313156128%
(903, 2, 600)
class 0 weight: 1.5952848722986246
class 1 weight: 2.67986798679868
Score for fold 1: loss of 0.4738718271255493; accuracy of 69.2307710647583%
(903, 2, 600)
class 0 weight: 1.5952848722986246
class 1 weight: 2.67986798679868
Score for fold 2: loss of 0.43192994594573975; accuracy of 79.1208803653717%
(903, 2, 600)
class 0 weight: 1.5941176470588234
class 1 weight: 2.6831683168316833
Score for fold 3: loss of 0.4715113043785095; accuracy of 76.66666507720947%
(903, 2, 600)
class 0 weight: 1.5941176470588234
class 1 weight: 2.6831683168316833
Score for fold 4: loss of 0.46118131279945374; accuracy of 80.0000011920929%
(903, 2, 600)
class 0 weight: 1.5941176470588234
class 1 weight: 2.6831683168316833
Score for fold 5: loss of 0.42143380641937256; accuracy of 73.33333492279053%
(903, 2, 600)
class 0 weight: 1.5941176470588234
class 1 weight: 2.6831683168316833
Score for fold 6: loss of 0.4264311194419861; accuracy of 80.0000011920929%
(903, 2, 600)
class 0 weight: 1.5972495088408643
class 1 weight: 2.674342105263158
Score for fold 7: loss of 0.340839684009552; accuracy of 82.22222328186035%
(903, 2, 600)
class 0 weight: 1.5972495088408643
class 1 weight: 2.674342105263158
Score for fold 8: loss of 0.47824570536613464; accuracy of 73.33333492279053%
(903, 2, 600)
class 0 weight: 1.5972495088408643
class 1 weight: 2.674342105263158
Score for fold 9: loss of 0.4221611022949219; accuracy of 78.88888716697693%
trainig on file: trainData_500.0mssec_P20 p20.anatomy.matrix.csv.arff
(1036, 2, 600)
class 0 weight: 1.9376299376299377
class 1 weight: 2.066518847006652
Score for fold 0: loss of 0.5771617293357849; accuracy of 69.2307710647583%
(1036, 2, 600)
class 0 weight: 1.9376299376299377
class 1 weight: 2.066518847006652
Score for fold 1: loss of 0.4548894464969635; accuracy of 77.88461446762085%
(1036, 2, 600)
class 0 weight: 1.9376299376299377
class 1 weight: 2.066518847006652
Score for fold 2: loss of 0.39655816555023193; accuracy of 82.69230723381042%
(1036, 2, 600)
class 0 weight: 1.9376299376299377
class 1 weight: 2.066518847006652
Score for fold 3: loss of 0.5216923952102661; accuracy of 77.88461446762085%
(1036, 2, 600)
class 0 weight: 1.9376299376299377
class 1 weight: 2.066518847006652
Score for fold 4: loss of 0.495765745639801; accuracy of 75.0%
(1036, 2, 600)
class 0 weight: 1.933609958506224
class 1 weight: 2.071111111111111
Score for fold 5: loss of 0.4369059205055237; accuracy of 75.0%
(1036, 2, 600)
class 0 weight: 1.9356846473029046
class 1 weight: 2.0687361419068737
Score for fold 6: loss of 0.4110275208950043; accuracy of 81.5533995628357%
(1036, 2, 600)
class 0 weight: 1.9356846473029046
class 1 weight: 2.0687361419068737
Score for fold 7: loss of 0.38285863399505615; accuracy of 80.58252334594727%
(1036, 2, 600)
class 0 weight: 1.9356846473029046
class 1 weight: 2.0687361419068737
Score for fold 8: loss of 0.4070301353931427; accuracy of 77.66990065574646%
(1036, 2, 600)
class 0 weight: 1.9356846473029046
class 1 weight: 2.0687361419068737
Score for fold 9: loss of 0.43557649850845337; accuracy of 75.72815418243408%
trainig on file: trainData_500.0mssec_P30 p30.conf.matrix.csv.arff
(1032, 2, 600)
class 0 weight: 1.518821603927987
class 1 weight: 2.9274447949526814
Score for fold 0: loss of 0.645677924156189; accuracy of 60.576921701431274%
(1032, 2, 600)
class 0 weight: 1.518821603927987
class 1 weight: 2.9274447949526814
Score for fold 1: loss of 0.5462955832481384; accuracy of 71.15384340286255%
(1032, 2, 600)
class 0 weight: 1.5179738562091503
class 1 weight: 2.9305993690851735
Score for fold 2: loss of 0.5958731770515442; accuracy of 66.01941585540771%
(1032, 2, 600)
class 0 weight: 1.5204582651391163
class 1 weight: 2.9213836477987423
Score for fold 3: loss of 0.5481014251708984; accuracy of 70.8737850189209%
(1032, 2, 600)
class 0 weight: 1.5204582651391163
class 1 weight: 2.9213836477987423
Score for fold 4: loss of 0.5435352325439453; accuracy of 67.96116232872009%
(1032, 2, 600)
class 0 weight: 1.5204582651391163
class 1 weight: 2.9213836477987423
Score for fold 5: loss of 0.49118226766586304; accuracy of 71.84466123580933%
(1032, 2, 600)
class 0 weight: 1.5204582651391163
class 1 weight: 2.9213836477987423
Score for fold 6: loss of 0.4851478934288025; accuracy of 69.90291476249695%
(1032, 2, 600)
class 0 weight: 1.5204582651391163
class 1 weight: 2.9213836477987423
Score for fold 7: loss of 0.5374914407730103; accuracy of 65.04854559898376%
(1032, 2, 600)
class 0 weight: 1.5204582651391163
class 1 weight: 2.9213836477987423
Score for fold 8: loss of 0.5090080499649048; accuracy of 72.81553149223328%
(1032, 2, 600)
class 0 weight: 1.5204582651391163
class 1 weight: 2.9213836477987423
Score for fold 9: loss of 0.47959789633750916; accuracy of 78.64077687263489%
trainig on file: trainData_500.0mssec_P13 p13.anatomy.list.csv.arff
(345, 2, 600)
class 0 weight: 3.2291666666666665
class 1 weight: 1.4485981308411213
Score for fold 0: loss of 0.4020020663738251; accuracy of 82.85714387893677%
(345, 2, 600)
class 0 weight: 3.2291666666666665
class 1 weight: 1.4485981308411213
Score for fold 1: loss of 0.20338594913482666; accuracy of 91.42857193946838%
(345, 2, 600)
class 0 weight: 3.2291666666666665
class 1 weight: 1.4485981308411213
Score for fold 2: loss of 0.3393699526786804; accuracy of 82.85714387893677%
(345, 2, 600)
class 0 weight: 3.2291666666666665
class 1 weight: 1.4485981308411213
Score for fold 3: loss of 0.17704367637634277; accuracy of 91.42857193946838%
(345, 2, 600)
class 0 weight: 3.2291666666666665
class 1 weight: 1.4485981308411213
Score for fold 4: loss of 0.35232678055763245; accuracy of 80.0000011920929%
(345, 2, 600)
class 0 weight: 3.2061855670103094
class 1 weight: 1.453271028037383
Score for fold 5: loss of 0.33248868584632874; accuracy of 82.35294222831726%
(345, 2, 600)
class 0 weight: 3.2061855670103094
class 1 weight: 1.453271028037383
Score for fold 6: loss of 0.42862552404403687; accuracy of 73.52941036224365%
(345, 2, 600)
class 0 weight: 3.2061855670103094
class 1 weight: 1.453271028037383
Score for fold 7: loss of 0.34130069613456726; accuracy of 82.35294222831726%
(345, 2, 600)
class 0 weight: 3.239583333333333
class 1 weight: 1.4465116279069767
Score for fold 8: loss of 0.335296630859375; accuracy of 82.35294222831726%
(345, 2, 600)
class 0 weight: 3.239583333333333
class 1 weight: 1.4465116279069767
Score for fold 9: loss of 0.12321232259273529; accuracy of 97.0588207244873%
trainig on file: trainData_500.0mssec_P64 p64.anatomy.matrix.csv.arff
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 0: loss of 0.4380384683609009; accuracy of 72.72727489471436%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 1: loss of 0.321219801902771; accuracy of 83.33333134651184%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 2: loss of 0.5200931429862976; accuracy of 71.21211886405945%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 3: loss of 0.3575696647167206; accuracy of 78.78788113594055%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 4: loss of 0.38224685192108154; accuracy of 78.78788113594055%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 5: loss of 0.3769717812538147; accuracy of 80.30303120613098%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 6: loss of 0.3564301133155823; accuracy of 80.30303120613098%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 7: loss of 0.406735360622406; accuracy of 77.27272510528564%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 8: loss of 0.3311821520328522; accuracy of 80.30303120613098%
(660, 2, 600)
class 0 weight: 9.428571428571429
class 1 weight: 1.11864406779661
Score for fold 9: loss of 0.30022603273391724; accuracy of 83.33333134651184%
trainig on file: trainData_500.0mssec_P59 p59.anatomy.list.csv.arff
(772, 2, 600)
class 0 weight: 14.163265306122447
class 1 weight: 1.075968992248062
Score for fold 0: loss of 0.23188742995262146; accuracy of 89.74359035491943%
(772, 2, 600)
class 0 weight: 14.163265306122447
class 1 weight: 1.075968992248062
Score for fold 1: loss of 0.06935428082942963; accuracy of 98.71794581413269%
(772, 2, 600)
class 0 weight: 13.9
class 1 weight: 1.0775193798449612
Score for fold 2: loss of 0.06464071571826935; accuracy of 98.70129823684692%
(772, 2, 600)
class 0 weight: 13.9
class 1 weight: 1.0775193798449612
Score for fold 3: loss of 0.02387213334441185; accuracy of 98.70129823684692%
(772, 2, 600)
class 0 weight: 13.9
class 1 weight: 1.0775193798449612
Score for fold 4: loss of 0.044829361140728; accuracy of 97.40259647369385%
(772, 2, 600)
class 0 weight: 13.9
class 1 weight: 1.0775193798449612
Score for fold 5: loss of 0.12986896932125092; accuracy of 96.10389471054077%
(772, 2, 600)
class 0 weight: 13.9
class 1 weight: 1.0775193798449612
Score for fold 6: loss of 0.2260621041059494; accuracy of 94.8051929473877%
(772, 2, 600)
class 0 weight: 14.183673469387754
class 1 weight: 1.0758513931888545
Score for fold 7: loss of 0.3265005946159363; accuracy of 88.31169009208679%
(772, 2, 600)
class 0 weight: 14.183673469387754
class 1 weight: 1.0758513931888545
Score for fold 8: loss of 0.32478487491607666; accuracy of 89.61039185523987%
(772, 2, 600)
class 0 weight: 14.183673469387754
class 1 weight: 1.0758513931888545
Score for fold 9: loss of 0.01681591011583805; accuracy of 100.0%
trainig on file: trainData_500.0mssec_P3 p3.conf.list.csv.arff
(875, 2, 600)
class 0 weight: 2.947565543071161
class 1 weight: 1.5134615384615386
Score for fold 0: loss of 0.5225375890731812; accuracy of 65.90909361839294%
(875, 2, 600)
class 0 weight: 2.947565543071161
class 1 weight: 1.5134615384615386
Score for fold 1: loss of 0.5778385400772095; accuracy of 59.090906381607056%
(875, 2, 600)
class 0 weight: 2.947565543071161
class 1 weight: 1.5134615384615386
Score for fold 2: loss of 0.5197667479515076; accuracy of 63.63636255264282%
(875, 2, 600)
class 0 weight: 2.947565543071161
class 1 weight: 1.5134615384615386
Score for fold 3: loss of 0.5043860673904419; accuracy of 69.31818127632141%
(875, 2, 600)
class 0 weight: 2.947565543071161
class 1 weight: 1.5134615384615386
Score for fold 4: loss of 0.5759852528572083; accuracy of 63.63636255264282%
(875, 2, 600)
class 0 weight: 2.9402985074626864
class 1 weight: 1.5153846153846156
Score for fold 5: loss of 0.6073219776153564; accuracy of 60.919541120529175%
(875, 2, 600)
class 0 weight: 2.9402985074626864
class 1 weight: 1.5153846153846156
Score for fold 6: loss of 0.5676353573799133; accuracy of 65.51724076271057%
(875, 2, 600)
class 0 weight: 2.9402985074626864
class 1 weight: 1.5153846153846156
Score for fold 7: loss of 0.48339685797691345; accuracy of 68.96551847457886%
(875, 2, 600)
class 0 weight: 2.951310861423221
class 1 weight: 1.5124760076775432
Score for fold 8: loss of 0.45599907636642456; accuracy of 68.96551847457886%
(875, 2, 600)
class 0 weight: 2.951310861423221
class 1 weight: 1.5124760076775432
Score for fold 9: loss of 0.504108190536499; accuracy of 67.81609058380127%
trainig on file: trainData_500.0mssec_P38 p38.conf.matrix.csv.arff
(1084, 2, 600)
class 0 weight: 1.7791970802919708
class 1 weight: 2.2833723653395785
Score for fold 0: loss of 0.5028316378593445; accuracy of 70.64220309257507%
(1084, 2, 600)
class 0 weight: 1.7791970802919708
class 1 weight: 2.2833723653395785
Score for fold 1: loss of 0.5537773966789246; accuracy of 70.64220309257507%
(1084, 2, 600)
class 0 weight: 1.7791970802919708
class 1 weight: 2.2833723653395785
Score for fold 2: loss of 0.4764026701450348; accuracy of 76.14678740501404%
(1084, 2, 600)
class 0 weight: 1.7791970802919708
class 1 weight: 2.2833723653395785
Score for fold 3: loss of 0.4868161678314209; accuracy of 80.73394298553467%
(1084, 2, 600)
class 0 weight: 1.7777777777777777
class 1 weight: 2.2857142857142856
Score for fold 4: loss of 0.4902516007423401; accuracy of 69.44444179534912%
(1084, 2, 600)
class 0 weight: 1.781021897810219
class 1 weight: 2.2803738317757007
Score for fold 5: loss of 0.4875379800796509; accuracy of 73.14814925193787%
(1084, 2, 600)
class 0 weight: 1.781021897810219
class 1 weight: 2.2803738317757007
Score for fold 6: loss of 0.4213738739490509; accuracy of 79.62962985038757%
(1084, 2, 600)
class 0 weight: 1.781021897810219
class 1 weight: 2.2803738317757007
Score for fold 7: loss of 0.4615432322025299; accuracy of 78.70370149612427%
(1084, 2, 600)
class 0 weight: 1.781021897810219
class 1 weight: 2.2803738317757007
Score for fold 8: loss of 0.3495124876499176; accuracy of 85.18518805503845%
(1084, 2, 600)
class 0 weight: 1.781021897810219
class 1 weight: 2.2803738317757007
Score for fold 9: loss of 0.5041168332099915; accuracy of 72.22222089767456%
trainig on file: trainData_500.0mssec_P3 p3.anatomy.list.csv.arff
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 0: loss of 0.3624912202358246; accuracy of 72.91666865348816%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 1: loss of 0.29902222752571106; accuracy of 87.5%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 2: loss of 0.2444416731595993; accuracy of 87.5%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 3: loss of 0.4513837397098541; accuracy of 79.16666865348816%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 4: loss of 0.26964566111564636; accuracy of 83.33333134651184%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 5: loss of 0.47567257285118103; accuracy of 72.91666865348816%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 6: loss of 0.2618648111820221; accuracy of 87.5%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 7: loss of 0.34414076805114746; accuracy of 83.33333134651184%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 8: loss of 0.16806362569332123; accuracy of 91.66666865348816%
(480, 2, 600)
class 0 weight: 3.4285714285714284
class 1 weight: 1.411764705882353
Score for fold 9: loss of 0.22606699168682098; accuracy of 89.58333134651184%
trainig on file: trainData_500.0mssec_P13 p13.conf.list.csv.arff
(762, 2, 600)
class 0 weight: 2.6653696498054473
class 1 weight: 1.6004672897196262
Score for fold 0: loss of 0.596000611782074; accuracy of 64.9350643157959%
(762, 2, 600)
class 0 weight: 2.6653696498054473
class 1 weight: 1.6004672897196262
Score for fold 1: loss of 0.4679695665836334; accuracy of 77.92207598686218%
(762, 2, 600)
class 0 weight: 2.6589147286821704
class 1 weight: 1.602803738317757
Score for fold 2: loss of 0.43495258688926697; accuracy of 73.68420958518982%
(762, 2, 600)
class 0 weight: 2.6589147286821704
class 1 weight: 1.602803738317757
Score for fold 3: loss of 0.4152061939239502; accuracy of 81.57894611358643%
(762, 2, 600)
class 0 weight: 2.6589147286821704
class 1 weight: 1.602803738317757
Score for fold 4: loss of 0.5633134841918945; accuracy of 72.36841917037964%
(762, 2, 600)
class 0 weight: 2.6589147286821704
class 1 weight: 1.602803738317757
Score for fold 5: loss of 0.3997490406036377; accuracy of 82.8947365283966%
(762, 2, 600)
class 0 weight: 2.669260700389105
class 1 weight: 1.599067599067599
Score for fold 6: loss of 0.5582422018051147; accuracy of 68.42105388641357%
(762, 2, 600)
class 0 weight: 2.669260700389105
class 1 weight: 1.599067599067599
Score for fold 7: loss of 0.5898483395576477; accuracy of 71.05262875556946%
(762, 2, 600)
class 0 weight: 2.669260700389105
class 1 weight: 1.599067599067599
Score for fold 8: loss of 0.5506769418716431; accuracy of 72.36841917037964%
(762, 2, 600)
class 0 weight: 2.669260700389105
class 1 weight: 1.599067599067599
Score for fold 9: loss of 0.5030935406684875; accuracy of 68.42105388641357%
trainig on file: trainData_500.0mssec_P1 p1.conf.list.csv.arff
(1155, 2, 600)
class 0 weight: 3.876865671641791
class 1 weight: 1.3476005188067444
Score for fold 0: loss of 0.4636891484260559; accuracy of 74.13793206214905%
(1155, 2, 600)
class 0 weight: 3.876865671641791
class 1 weight: 1.3476005188067444
Score for fold 1: loss of 0.5145668387413025; accuracy of 74.13793206214905%
(1155, 2, 600)
class 0 weight: 3.876865671641791
class 1 weight: 1.3476005188067444
Score for fold 2: loss of 0.36334335803985596; accuracy of 80.17241358757019%
(1155, 2, 600)
class 0 weight: 3.876865671641791
class 1 weight: 1.3476005188067444
Score for fold 3: loss of 0.3450391888618469; accuracy of 83.62069129943848%
(1155, 2, 600)
class 0 weight: 3.876865671641791
class 1 weight: 1.3476005188067444
Score for fold 4: loss of 0.2820218801498413; accuracy of 82.75862336158752%
(1155, 2, 600)
class 0 weight: 3.866171003717472
class 1 weight: 1.3488975356679636
Score for fold 5: loss of 0.3256185054779053; accuracy of 81.73912763595581%
(1155, 2, 600)
class 0 weight: 3.866171003717472
class 1 weight: 1.3488975356679636
Score for fold 6: loss of 0.4157445430755615; accuracy of 76.52173638343811%
(1155, 2, 600)
class 0 weight: 3.8805970149253732
class 1 weight: 1.3471502590673576
Score for fold 7: loss of 0.32064563035964966; accuracy of 81.73912763595581%
(1155, 2, 600)
class 0 weight: 3.8805970149253732
class 1 weight: 1.3471502590673576
Score for fold 8: loss of 0.28637754917144775; accuracy of 84.3478262424469%
(1155, 2, 600)
class 0 weight: 3.8805970149253732
class 1 weight: 1.3471502590673576
Score for fold 9: loss of 0.284064382314682; accuracy of 87.8260850906372%
trainig on file: trainData_500.0mssec_P60 p60.conf.matrix.csv.arff
(1605, 2, 600)
class 0 weight: 1.6559633027522938
class 1 weight: 2.5244755244755246
Score for fold 0: loss of 0.6017749309539795; accuracy of 65.21739363670349%
(1605, 2, 600)
class 0 weight: 1.6559633027522938
class 1 weight: 2.5244755244755246
Score for fold 1: loss of 0.5309075117111206; accuracy of 69.5652186870575%
(1605, 2, 600)
class 0 weight: 1.6559633027522938
class 1 weight: 2.5244755244755246
Score for fold 2: loss of 0.47689297795295715; accuracy of 71.42857313156128%
(1605, 2, 600)
class 0 weight: 1.6559633027522938
class 1 weight: 2.5244755244755246
Score for fold 3: loss of 0.5073453187942505; accuracy of 70.80745100975037%
(1605, 2, 600)
class 0 weight: 1.6559633027522938
class 1 weight: 2.5244755244755246
Score for fold 4: loss of 0.5592066049575806; accuracy of 72.67080545425415%
(1605, 2, 600)
class 0 weight: 1.6571100917431194
class 1 weight: 2.5218150087260036
Score for fold 5: loss of 0.5184426307678223; accuracy of 69.9999988079071%
(1605, 2, 600)
class 0 weight: 1.6571100917431194
class 1 weight: 2.5218150087260036
Score for fold 6: loss of 0.5489317774772644; accuracy of 70.6250011920929%
(1605, 2, 600)
class 0 weight: 1.6571100917431194
class 1 weight: 2.5218150087260036
Score for fold 7: loss of 0.5329087376594543; accuracy of 68.12499761581421%
(1605, 2, 600)
class 0 weight: 1.6571100917431194
class 1 weight: 2.5218150087260036
Score for fold 8: loss of 0.5208687782287598; accuracy of 74.37499761581421%
(1605, 2, 600)
class 0 weight: 1.6552119129438718
class 1 weight: 2.5262237762237763
Score for fold 9: loss of 0.4575643539428711; accuracy of 77.49999761581421%
trainig on file: trainData_500.0mssec_P39 p39.anatomy.list.csv.arff
(589, 2, 600)
class 0 weight: 106.0
class 1 weight: 1.0095238095238095
Score for fold 0: loss of 0.4780314564704895; accuracy of 76.27118825912476%
(589, 2, 600)
class 0 weight: 106.0
class 1 weight: 1.0095238095238095
Score for fold 1: loss of 0.4075930416584015; accuracy of 79.6610176563263%
(589, 2, 600)
class 0 weight: 106.0
class 1 weight: 1.0095238095238095
Score for fold 2: loss of 0.5698082447052002; accuracy of 64.40678238868713%
(589, 2, 600)
class 0 weight: 106.0
class 1 weight: 1.0095238095238095
Score for fold 3: loss of 0.18786022067070007; accuracy of 84.74576473236084%
(589, 2, 600)
class 0 weight: 132.5
class 1 weight: 1.0076045627376427
Score for fold 4: loss of 0.3482837975025177; accuracy of 74.57627058029175%
(589, 2, 600)
class 0 weight: 132.5
class 1 weight: 1.0076045627376427
Score for fold 5: loss of 0.45102325081825256; accuracy of 66.10169410705566%
(589, 2, 600)
class 0 weight: 132.5
class 1 weight: 1.0076045627376427
Score for fold 6: loss of 0.7695434093475342; accuracy of 54.23728823661804%
(589, 2, 600)
class 0 weight: 132.5
class 1 weight: 1.0076045627376427
Score for fold 7: loss of 0.5252931118011475; accuracy of 55.93220591545105%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P39 p39.anatomy.list.csv.arff
(589, 2, 600)
class 0 weight: 132.5
class 1 weight: 1.0076045627376427
Score for fold 8: loss of 0.37127450108528137; accuracy of 72.88135886192322%
(589, 2, 600)
class 0 weight: 106.2
class 1 weight: 1.0095057034220531
Score for fold 9: loss of 0.3745105564594269; accuracy of 70.68965435028076%
trainig on file: trainData_500.0mssec_P19 p19.conf.list.csv.arff
(1276, 2, 600)
class 0 weight: 1.7341389728096677
class 1 weight: 2.3621399176954734
Score for fold 0: loss of 0.5712411403656006; accuracy of 71.875%
(1276, 2, 600)
class 0 weight: 1.7341389728096677
class 1 weight: 2.3621399176954734
Score for fold 1: loss of 0.48459696769714355; accuracy of 74.21875%
(1276, 2, 600)
class 0 weight: 1.7341389728096677
class 1 weight: 2.3621399176954734
Score for fold 2: loss of 0.5154248476028442; accuracy of 73.4375%
(1276, 2, 600)
class 0 weight: 1.7341389728096677
class 1 weight: 2.3621399176954734
Score for fold 3: loss of 0.38067516684532166; accuracy of 81.25%
(1276, 2, 600)
class 0 weight: 1.7341389728096677
class 1 weight: 2.3621399176954734
Score for fold 4: loss of 0.47371140122413635; accuracy of 75.0%
(1276, 2, 600)
class 0 weight: 1.7341389728096677
class 1 weight: 2.3621399176954734
Score for fold 5: loss of 0.3417063355445862; accuracy of 83.59375%
(1276, 2, 600)
class 0 weight: 1.7330316742081449
class 1 weight: 2.364197530864198
Score for fold 6: loss of 0.3472498953342438; accuracy of 85.03937125205994%
(1276, 2, 600)
class 0 weight: 1.7330316742081449
class 1 weight: 2.364197530864198
Score for fold 7: loss of 0.44359567761421204; accuracy of 81.10235929489136%
(1276, 2, 600)
class 0 weight: 1.7330316742081449
class 1 weight: 2.364197530864198
Score for fold 8: loss of 0.33516639471054077; accuracy of 88.18897604942322%
(1276, 2, 600)
class 0 weight: 1.7330316742081449
class 1 weight: 2.364197530864198
Score for fold 9: loss of 0.36567533016204834; accuracy of 81.88976645469666%
trainig on file: trainData_500.0mssec_P63 p63.conf.list.csv.arff
(664, 2, 600)
class 0 weight: 1.5669291338582676
class 1 weight: 2.763888888888889
Score for fold 0: loss of 0.6419005393981934; accuracy of 49.25373196601868%
(664, 2, 600)
class 0 weight: 1.5669291338582676
class 1 weight: 2.763888888888889
Score for fold 1: loss of 0.5836289525032043; accuracy of 62.68656849861145%
(664, 2, 600)
class 0 weight: 1.5669291338582676
class 1 weight: 2.763888888888889
Score for fold 2: loss of 0.5494536757469177; accuracy of 67.16417670249939%
(664, 2, 600)
class 0 weight: 1.5669291338582676
class 1 weight: 2.763888888888889
Score for fold 3: loss of 0.5511022806167603; accuracy of 65.67164063453674%
(664, 2, 600)
class 0 weight: 1.5654450261780106
class 1 weight: 2.768518518518518
Score for fold 4: loss of 0.4930451214313507; accuracy of 74.24242496490479%
(664, 2, 600)
class 0 weight: 1.5654450261780106
class 1 weight: 2.768518518518518
Score for fold 5: loss of 0.5127063989639282; accuracy of 69.69696879386902%
(664, 2, 600)
class 0 weight: 1.5654450261780106
class 1 weight: 2.768518518518518
Score for fold 6: loss of 0.4527888894081116; accuracy of 72.72727489471436%
(664, 2, 600)
class 0 weight: 1.5654450261780106
class 1 weight: 2.768518518518518
Score for fold 7: loss of 0.637020468711853; accuracy of 60.60606241226196%
(664, 2, 600)
class 0 weight: 1.5654450261780106
class 1 weight: 2.768518518518518
Score for fold 8: loss of 0.8180246353149414; accuracy of 56.060606241226196%
(664, 2, 600)
class 0 weight: 1.5654450261780106
class 1 weight: 2.768518518518518
Score for fold 9: loss of 0.37580445408821106; accuracy of 78.78788113594055%
trainig on file: trainData_500.0mssec_P68 p68.anatomy.matrix.csv.arff
(626, 2, 600)
class 0 weight: 3.703947368421052
class 1 weight: 1.3698296836982968
Score for fold 0: loss of 0.2083909511566162; accuracy of 88.88888955116272%
(626, 2, 600)
class 0 weight: 3.703947368421052
class 1 weight: 1.3698296836982968
Score for fold 1: loss of 0.20032253861427307; accuracy of 92.0634925365448%
(626, 2, 600)
class 0 weight: 3.703947368421052
class 1 weight: 1.3698296836982968
Score for fold 2: loss of 0.11470621079206467; accuracy of 92.0634925365448%
(626, 2, 600)
class 0 weight: 3.703947368421052
class 1 weight: 1.3698296836982968
Score for fold 3: loss of 0.11546296626329422; accuracy of 93.65079402923584%
(626, 2, 600)
class 0 weight: 3.703947368421052
class 1 weight: 1.3698296836982968
Score for fold 4: loss of 0.22210660576820374; accuracy of 90.47619104385376%
(626, 2, 600)
class 0 weight: 3.703947368421052
class 1 weight: 1.3698296836982968
Score for fold 5: loss of 0.16888603568077087; accuracy of 92.0634925365448%
(626, 2, 600)
class 0 weight: 3.7105263157894735
class 1 weight: 1.3689320388349513
Score for fold 6: loss of 0.17164692282676697; accuracy of 96.77419066429138%
(626, 2, 600)
class 0 weight: 3.7105263157894735
class 1 weight: 1.3689320388349513
Score for fold 7: loss of 0.13115698099136353; accuracy of 93.54838728904724%
(626, 2, 600)
class 0 weight: 3.7105263157894735
class 1 weight: 1.3689320388349513
Score for fold 8: loss of 0.2383161336183548; accuracy of 90.32257795333862%
(626, 2, 600)
class 0 weight: 3.686274509803922
class 1 weight: 1.3722627737226278
Score for fold 9: loss of 0.14507168531417847; accuracy of 96.77419066429138%
trainig on file: trainData_500.0mssec_P24 p24.conf.matrix.csv.arff
(1742, 2, 600)
class 0 weight: 1.8788968824940047
class 1 weight: 2.1377899045020463
Score for fold 0: loss of 0.43689021468162537; accuracy of 78.85714173316956%
(1742, 2, 600)
class 0 weight: 1.8788968824940047
class 1 weight: 2.1377899045020463
Score for fold 1: loss of 0.4434058368206024; accuracy of 75.42856931686401%
(1742, 2, 600)
class 0 weight: 1.8800959232613907
class 1 weight: 2.136239782016349
Score for fold 2: loss of 0.3973105549812317; accuracy of 78.73563170433044%
(1742, 2, 600)
class 0 weight: 1.8800959232613907
class 1 weight: 2.136239782016349
Score for fold 3: loss of 0.4568800628185272; accuracy of 77.58620977401733%
(1742, 2, 600)
class 0 weight: 1.8800959232613907
class 1 weight: 2.136239782016349
Score for fold 4: loss of 0.4006369709968567; accuracy of 79.88505959510803%
(1742, 2, 600)
class 0 weight: 1.8800959232613907
class 1 weight: 2.136239782016349
Score for fold 5: loss of 0.37002480030059814; accuracy of 81.03448152542114%
(1742, 2, 600)
class 0 weight: 1.8800959232613907
class 1 weight: 2.136239782016349
Score for fold 6: loss of 0.4434133470058441; accuracy of 75.28735399246216%
(1742, 2, 600)
class 0 weight: 1.8778443113772454
class 1 weight: 2.139154160982265
Score for fold 7: loss of 0.40006116032600403; accuracy of 78.73563170433044%
(1742, 2, 600)
class 0 weight: 1.8778443113772454
class 1 weight: 2.139154160982265
Score for fold 8: loss of 0.4587375223636627; accuracy of 73.56321811676025%
(1742, 2, 600)
class 0 weight: 1.8778443113772454
class 1 weight: 2.139154160982265
Score for fold 9: loss of 0.45461422204971313; accuracy of 74.13793206214905%
trainig on file: trainData_500.0mssec_P61 p61.anatomy.list.csv.arff
(577, 2, 600)
class 0 weight: 12.975000000000001
class 1 weight: 1.0835073068893528
Score for fold 0: loss of 0.32607975602149963; accuracy of 82.75862336158752%
(577, 2, 600)
class 0 weight: 12.975000000000001
class 1 weight: 1.0835073068893528
Score for fold 1: loss of 0.5100805163383484; accuracy of 75.86206793785095%
(577, 2, 600)
class 0 weight: 12.975000000000001
class 1 weight: 1.0835073068893528
Score for fold 2: loss of 0.16507036983966827; accuracy of 93.1034505367279%
(577, 2, 600)
class 0 weight: 13.307692307692307
class 1 weight: 1.08125
Score for fold 3: loss of 0.5559874773025513; accuracy of 72.41379022598267%
(577, 2, 600)
class 0 weight: 13.307692307692307
class 1 weight: 1.08125
Score for fold 4: loss of 0.27412304282188416; accuracy of 81.03448152542114%
(577, 2, 600)
class 0 weight: 13.307692307692307
class 1 weight: 1.08125
Score for fold 5: loss of 0.291426420211792; accuracy of 81.03448152542114%
(577, 2, 600)
class 0 weight: 13.307692307692307
class 1 weight: 1.08125
Score for fold 6: loss of 0.3947432041168213; accuracy of 74.13793206214905%
(577, 2, 600)
class 0 weight: 13.0
class 1 weight: 1.0833333333333333
Score for fold 7: loss of 0.38029414415359497; accuracy of 77.19298005104065%
(577, 2, 600)
class 0 weight: 13.0
class 1 weight: 1.0833333333333333
Score for fold 8: loss of 0.13757699728012085; accuracy of 92.98245906829834%
(577, 2, 600)
class 0 weight: 13.0
class 1 weight: 1.0833333333333333
Score for fold 9: loss of 0.11046089231967926; accuracy of 94.73684430122375%
trainig on file: trainData_500.0mssec_P71 p71.anatomy.list.csv.arff
(564, 2, 600)
class 0 weight: 1.707070707070707
class 1 weight: 2.4142857142857146
Score for fold 0: loss of 0.2643281817436218; accuracy of 91.22806787490845%
(564, 2, 600)
class 0 weight: 1.707070707070707
class 1 weight: 2.4142857142857146
Score for fold 1: loss of 0.25188347697257996; accuracy of 92.98245906829834%
(564, 2, 600)
class 0 weight: 1.707070707070707
class 1 weight: 2.4142857142857146
Score for fold 2: loss of 0.31381136178970337; accuracy of 89.47368264198303%
(564, 2, 600)
class 0 weight: 1.707070707070707
class 1 weight: 2.4142857142857146
Score for fold 3: loss of 0.17046791315078735; accuracy of 94.73684430122375%
(564, 2, 600)
class 0 weight: 1.7104377104377104
class 1 weight: 2.4075829383886256
Score for fold 4: loss of 0.2909448444843292; accuracy of 89.28571343421936%
(564, 2, 600)
class 0 weight: 1.7104377104377104
class 1 weight: 2.4075829383886256
Score for fold 5: loss of 0.30040407180786133; accuracy of 87.5%
(564, 2, 600)
class 0 weight: 1.7104377104377104
class 1 weight: 2.4075829383886256
Score for fold 6: loss of 0.22119870781898499; accuracy of 87.5%
(564, 2, 600)
class 0 weight: 1.7104377104377104
class 1 weight: 2.4075829383886256
Score for fold 7: loss of 0.2112564593553543; accuracy of 89.28571343421936%
(564, 2, 600)
class 0 weight: 1.7104377104377104
class 1 weight: 2.4075829383886256
Score for fold 8: loss of 0.36452680826187134; accuracy of 83.92857313156128%
(564, 2, 600)
class 0 weight: 1.7104377104377104
class 1 weight: 2.4075829383886256
Score for fold 9: loss of 0.256596177816391; accuracy of 89.28571343421936%
trainig on file: trainData_500.0mssec_P56 p56.anatomy.matrix.csv.arff
(472, 2, 600)
class 0 weight: 4.930232558139535
class 1 weight: 1.2544378698224852
Score for fold 0: loss of 0.1746707558631897; accuracy of 93.75%
(472, 2, 600)
class 0 weight: 4.930232558139535
class 1 weight: 1.2544378698224852
Score for fold 1: loss of 0.17291919887065887; accuracy of 93.75%
(472, 2, 600)
class 0 weight: 4.885057471264368
class 1 weight: 1.2573964497041419
Score for fold 2: loss of 0.162005215883255; accuracy of 95.7446813583374%
(472, 2, 600)
class 0 weight: 4.885057471264368
class 1 weight: 1.2573964497041419
Score for fold 3: loss of 0.3199017345905304; accuracy of 85.10638475418091%
(472, 2, 600)
class 0 weight: 4.885057471264368
class 1 weight: 1.2573964497041419
Score for fold 4: loss of 0.2344498187303543; accuracy of 91.4893627166748%
(472, 2, 600)
class 0 weight: 4.885057471264368
class 1 weight: 1.2573964497041419
Score for fold 5: loss of 0.07345755398273468; accuracy of 97.8723406791687%
(472, 2, 600)
class 0 weight: 4.941860465116279
class 1 weight: 1.2536873156342183
Score for fold 6: loss of 0.15583525598049164; accuracy of 95.7446813583374%
(472, 2, 600)
class 0 weight: 4.941860465116279
class 1 weight: 1.2536873156342183
Score for fold 7: loss of 0.33745989203453064; accuracy of 89.3617033958435%
(472, 2, 600)
class 0 weight: 4.941860465116279
class 1 weight: 1.2536873156342183
Score for fold 8: loss of 0.19884850084781647; accuracy of 93.6170220375061%
(472, 2, 600)
class 0 weight: 4.941860465116279
class 1 weight: 1.2536873156342183
Score for fold 9: loss of 0.13122344017028809; accuracy of 91.4893627166748%
trainig on file: trainData_500.0mssec_P26 p26.anatomy.matrix.csv.arff
(461, 2, 600)
class 0 weight: 10.615384615384615
class 1 weight: 1.1039999999999999
Score for fold 0: loss of 0.3281813859939575; accuracy of 85.10638475418091%
(461, 2, 600)
class 0 weight: 10.375
class 1 weight: 1.1066666666666667
Score for fold 1: loss of 0.23550474643707275; accuracy of 91.30434989929199%
(461, 2, 600)
class 0 weight: 10.375
class 1 weight: 1.1066666666666667
Score for fold 2: loss of 0.19311513006687164; accuracy of 91.30434989929199%
(461, 2, 600)
class 0 weight: 10.375
class 1 weight: 1.1066666666666667
Score for fold 3: loss of 0.35744547843933105; accuracy of 82.6086938381195%
(461, 2, 600)
class 0 weight: 10.375
class 1 weight: 1.1066666666666667
Score for fold 4: loss of 0.25954118371009827; accuracy of 89.13043737411499%
(461, 2, 600)
class 0 weight: 10.375
class 1 weight: 1.1066666666666667
Score for fold 5: loss of 0.31821364164352417; accuracy of 84.78260636329651%
(461, 2, 600)
class 0 weight: 10.375
class 1 weight: 1.1066666666666667
Score for fold 6: loss of 0.07055110484361649; accuracy of 97.826087474823%
(461, 2, 600)
class 0 weight: 10.64102564102564
class 1 weight: 1.1037234042553192
Score for fold 7: loss of 0.22616718709468842; accuracy of 91.30434989929199%
(461, 2, 600)
class 0 weight: 10.64102564102564
class 1 weight: 1.1037234042553192
Score for fold 8: loss of 0.31986457109451294; accuracy of 84.78260636329651%
(461, 2, 600)
class 0 weight: 10.64102564102564
class 1 weight: 1.1037234042553192
Score for fold 9: loss of 0.4974035620689392; accuracy of 76.0869562625885%
trainig on file: trainData_500.0mssec_P5 p5.anatomy.list.csv.arff
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 0: loss of 0.3434160053730011; accuracy of 82.97872543334961%
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 1: loss of 0.49401628971099854; accuracy of 76.59574747085571%
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 2: loss of 0.39091920852661133; accuracy of 85.10638475418091%
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 3: loss of 0.48382773995399475; accuracy of 80.85106611251831%
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 4: loss of 0.42316290736198425; accuracy of 78.72340679168701%
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 5: loss of 0.6005343198776245; accuracy of 68.08510422706604%
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 6: loss of 0.23874995112419128; accuracy of 95.7446813583374%
(468, 2, 600)
class 0 weight: 5.847222222222222
class 1 weight: 1.2063037249283668
Score for fold 7: loss of 0.2596989572048187; accuracy of 89.3617033958435%
(468, 2, 600)
class 0 weight: 5.861111111111111
class 1 weight: 1.2057142857142857
Score for fold 8: loss of 0.35129645466804504; accuracy of 86.95651888847351%
(468, 2, 600)
class 0 weight: 5.861111111111111
class 1 weight: 1.2057142857142857
Score for fold 9: loss of 0.4484266936779022; accuracy of 80.4347813129425%
trainig on file: trainData_500.0mssec_P11 p11.conf.list.csv.arff
(1623, 2, 600)
class 0 weight: 4.9324324324324325
class 1 weight: 1.254295532646048
Score for fold 0: loss of 0.5169575214385986; accuracy of 74.84662532806396%
(1623, 2, 600)
class 0 weight: 4.9324324324324325
class 1 weight: 1.254295532646048
Score for fold 1: loss of 0.3807678520679474; accuracy of 79.75460290908813%
(1623, 2, 600)
class 0 weight: 4.9324324324324325
class 1 weight: 1.254295532646048
Score for fold 2: loss of 0.4342525899410248; accuracy of 78.52760553359985%
(1623, 2, 600)
class 0 weight: 4.919191919191919
class 1 weight: 1.2551546391752577
Score for fold 3: loss of 0.49463245272636414; accuracy of 72.22222089767456%
(1623, 2, 600)
class 0 weight: 4.935810810810811
class 1 weight: 1.254077253218884
Score for fold 4: loss of 0.44880279898643494; accuracy of 77.16049551963806%
(1623, 2, 600)
class 0 weight: 4.935810810810811
class 1 weight: 1.254077253218884
Score for fold 5: loss of 0.40186089277267456; accuracy of 80.24691343307495%
(1623, 2, 600)
class 0 weight: 4.935810810810811
class 1 weight: 1.254077253218884
Score for fold 6: loss of 0.39000770449638367; accuracy of 82.09876418113708%
(1623, 2, 600)
class 0 weight: 4.935810810810811
class 1 weight: 1.254077253218884
Score for fold 7: loss of 0.4857296943664551; accuracy of 74.69135522842407%
(1623, 2, 600)
class 0 weight: 4.935810810810811
class 1 weight: 1.254077253218884
Score for fold 8: loss of 0.3024824857711792; accuracy of 85.18518805503845%
(1623, 2, 600)
class 0 weight: 4.935810810810811
class 1 weight: 1.254077253218884
Score for fold 9: loss of 0.31377148628234863; accuracy of 86.41975522041321%
trainig on file: trainData_500.0mssec_P80 p80.anatomy.list.csv.arff
(767, 2, 600)
class 0 weight: 2.247557003257329
class 1 weight: 1.8015665796344649
Score for fold 0: loss of 0.4860782027244568; accuracy of 76.6233742237091%
(767, 2, 600)
class 0 weight: 2.247557003257329
class 1 weight: 1.8015665796344649
Score for fold 1: loss of 0.3604768216609955; accuracy of 84.41558480262756%
(767, 2, 600)
class 0 weight: 2.247557003257329
class 1 weight: 1.8015665796344649
Score for fold 2: loss of 0.4208715260028839; accuracy of 77.92207598686218%
(767, 2, 600)
class 0 weight: 2.247557003257329
class 1 weight: 1.8015665796344649
Score for fold 3: loss of 0.37419816851615906; accuracy of 79.22077775001526%
(767, 2, 600)
class 0 weight: 2.247557003257329
class 1 weight: 1.8015665796344649
Score for fold 4: loss of 0.30794113874435425; accuracy of 81.81818127632141%
(767, 2, 600)
class 0 weight: 2.247557003257329
class 1 weight: 1.8015665796344649
Score for fold 5: loss of 0.33886271715164185; accuracy of 84.41558480262756%
(767, 2, 600)
class 0 weight: 2.2549019607843137
class 1 weight: 1.796875
Score for fold 6: loss of 0.44833001494407654; accuracy of 80.51947951316833%
(767, 2, 600)
class 0 weight: 2.250814332247557
class 1 weight: 1.7994791666666665
Score for fold 7: loss of 0.368537038564682; accuracy of 84.21052694320679%
(767, 2, 600)
class 0 weight: 2.250814332247557
class 1 weight: 1.7994791666666665
Score for fold 8: loss of 0.4245794117450714; accuracy of 73.68420958518982%
(767, 2, 600)
class 0 weight: 2.250814332247557
class 1 weight: 1.7994791666666665
Score for fold 9: loss of 0.4789152145385742; accuracy of 80.26315569877625%
trainig on file: trainData_500.0mssec_P53 p53.anatomy.list.csv.arff
(366, 2, 600)
class 0 weight: 3.655555555555556
class 1 weight: 1.3765690376569037
Score for fold 0: loss of 0.2565319538116455; accuracy of 86.48648858070374%
(366, 2, 600)
class 0 weight: 3.655555555555556
class 1 weight: 1.3765690376569037
Score for fold 1: loss of 0.3267095685005188; accuracy of 86.48648858070374%
(366, 2, 600)
class 0 weight: 3.655555555555556
class 1 weight: 1.3765690376569037
Score for fold 2: loss of 0.18456301093101501; accuracy of 91.89189076423645%
(366, 2, 600)
class 0 weight: 3.655555555555556
class 1 weight: 1.3765690376569037
Score for fold 3: loss of 0.2528131604194641; accuracy of 83.7837815284729%
(366, 2, 600)
class 0 weight: 3.655555555555556
class 1 weight: 1.3765690376569037
Score for fold 4: loss of 0.3635413646697998; accuracy of 83.7837815284729%
(366, 2, 600)
class 0 weight: 3.655555555555556
class 1 weight: 1.3765690376569037
Score for fold 5: loss of 0.16968129575252533; accuracy of 94.59459185600281%
(366, 2, 600)
class 0 weight: 3.666666666666667
class 1 weight: 1.375
Score for fold 6: loss of 0.45423808693885803; accuracy of 80.55555820465088%
(366, 2, 600)
class 0 weight: 3.666666666666667
class 1 weight: 1.375
Score for fold 7: loss of 0.19914522767066956; accuracy of 88.88888955116272%
(366, 2, 600)
class 0 weight: 3.666666666666667
class 1 weight: 1.375
Score for fold 8: loss of 0.27670249342918396; accuracy of 86.11111044883728%
(366, 2, 600)
class 0 weight: 3.666666666666667
class 1 weight: 1.375
Score for fold 9: loss of 0.2816220223903656; accuracy of 86.11111044883728%
trainig on file: trainData_500.0mssec_P78 p78.conf.matrix.csv.arff
(1155, 2, 600)
class 0 weight: 3.607638888888889
class 1 weight: 1.3834886817576566
Score for fold 0: loss of 0.6252866983413696; accuracy of 64.65517282485962%
(1155, 2, 600)
class 0 weight: 3.607638888888889
class 1 weight: 1.3834886817576566
Score for fold 1: loss of 0.5164490938186646; accuracy of 68.1034505367279%
(1155, 2, 600)
class 0 weight: 3.607638888888889
class 1 weight: 1.3834886817576566
Score for fold 2: loss of 0.49955064058303833; accuracy of 68.96551847457886%
(1155, 2, 600)
class 0 weight: 3.607638888888889
class 1 weight: 1.3834886817576566
Score for fold 3: loss of 0.5113842487335205; accuracy of 66.37930870056152%
(1155, 2, 600)
class 0 weight: 3.607638888888889
class 1 weight: 1.3834886817576566
Score for fold 4: loss of 0.5220333337783813; accuracy of 65.51724076271057%
(1155, 2, 600)
class 0 weight: 3.6111111111111107
class 1 weight: 1.3829787234042552
Score for fold 5: loss of 0.49364912509918213; accuracy of 66.9565200805664%
(1155, 2, 600)
class 0 weight: 3.6111111111111107
class 1 weight: 1.3829787234042552
Score for fold 6: loss of 0.48050016164779663; accuracy of 71.30434513092041%
(1155, 2, 600)
class 0 weight: 3.6111111111111107
class 1 weight: 1.3829787234042552
Score for fold 7: loss of 0.5561603903770447; accuracy of 60.00000238418579%
(1155, 2, 600)
class 0 weight: 3.6111111111111107
class 1 weight: 1.3829787234042552
Score for fold 8: loss of 0.46721982955932617; accuracy of 70.43478488922119%
(1155, 2, 600)
class 0 weight: 3.6111111111111107
class 1 weight: 1.3829787234042552
Score for fold 9: loss of 0.47559481859207153; accuracy of 70.43478488922119%
trainig on file: trainData_500.0mssec_P34 p34.anatomy.matrix.csv.arff
(362, 2, 600)
class 0 weight: 1.7567567567567568
class 1 weight: 2.321428571428571
Score for fold 0: loss of 0.42338475584983826; accuracy of 78.37837934494019%
(362, 2, 600)
class 0 weight: 1.7567567567567568
class 1 weight: 2.321428571428571
Score for fold 1: loss of 0.5327093601226807; accuracy of 72.972971200943%
(362, 2, 600)
class 0 weight: 1.752688172043011
class 1 weight: 2.3285714285714283
Score for fold 2: loss of 0.4597896933555603; accuracy of 75.0%
(362, 2, 600)
class 0 weight: 1.752688172043011
class 1 weight: 2.3285714285714283
Score for fold 3: loss of 0.2814190089702606; accuracy of 86.11111044883728%
(362, 2, 600)
class 0 weight: 1.752688172043011
class 1 weight: 2.3285714285714283
Score for fold 4: loss of 0.2989208698272705; accuracy of 86.11111044883728%
(362, 2, 600)
class 0 weight: 1.752688172043011
class 1 weight: 2.3285714285714283
Score for fold 5: loss of 0.2399349808692932; accuracy of 88.88888955116272%
(362, 2, 600)
class 0 weight: 1.7621621621621624
class 1 weight: 2.3120567375886525
Score for fold 6: loss of 0.30723822116851807; accuracy of 88.88888955116272%
(362, 2, 600)
class 0 weight: 1.7621621621621624
class 1 weight: 2.3120567375886525
Score for fold 7: loss of 0.33821576833724976; accuracy of 83.33333134651184%
(362, 2, 600)
class 0 weight: 1.7621621621621624
class 1 weight: 2.3120567375886525
Score for fold 8: loss of 0.33264610171318054; accuracy of 77.77777910232544%
(362, 2, 600)
class 0 weight: 1.7621621621621624
class 1 weight: 2.3120567375886525
Score for fold 9: loss of 0.26213890314102173; accuracy of 91.66666865348816%
trainig on file: trainData_500.0mssec_P27 p27.conf.list.csv.arff
(2317, 2, 600)
class 0 weight: 3.5160202360876895
class 1 weight: 1.3974530831099197
Score for fold 0: loss of 0.924943208694458; accuracy of 52.15517282485962%
(2317, 2, 600)
class 0 weight: 3.5160202360876895
class 1 weight: 1.3974530831099197
Score for fold 1: loss of 0.8361034393310547; accuracy of 53.01724076271057%
(2317, 2, 600)
class 0 weight: 3.5160202360876895
class 1 weight: 1.3974530831099197
Score for fold 2: loss of 0.8071609139442444; accuracy of 61.63793206214905%
(2317, 2, 600)
class 0 weight: 3.5160202360876895
class 1 weight: 1.3974530831099197
Score for fold 3: loss of 0.687480092048645; accuracy of 62.068963050842285%
(2317, 2, 600)
class 0 weight: 3.5160202360876895
class 1 weight: 1.3974530831099197
Score for fold 4: loss of 0.6230717897415161; accuracy of 68.53448152542114%
(2317, 2, 600)
class 0 weight: 3.5160202360876895
class 1 weight: 1.3974530831099197
Score for fold 5: loss of 0.6077833771705627; accuracy of 68.1034505367279%
(2317, 2, 600)
class 0 weight: 3.5160202360876895
class 1 weight: 1.3974530831099197
Score for fold 6: loss of 0.5438886284828186; accuracy of 74.13793206214905%
(2317, 2, 600)
class 0 weight: 3.5117845117845117
class 1 weight: 1.3981233243967828
Score for fold 7: loss of 0.7232147455215454; accuracy of 67.09956526756287%
(2317, 2, 600)
class 0 weight: 3.517706576728499
class 1 weight: 1.3971868720696583
Score for fold 8: loss of 0.6464760303497314; accuracy of 65.80086350440979%
(2317, 2, 600)
class 0 weight: 3.517706576728499
class 1 weight: 1.3971868720696583
Score for fold 9: loss of 0.7647038698196411; accuracy of 64.50216174125671%
trainig on file: trainData_500.0mssec_P4 p4.conf.matrix.csv.arff
(1145, 2, 600)
class 0 weight: 6.73202614379085
class 1 weight: 1.1744583808437856
Score for fold 0: loss of 0.41121533513069153; accuracy of 89.5652174949646%
(1145, 2, 600)
class 0 weight: 6.73202614379085
class 1 weight: 1.1744583808437856
Score for fold 1: loss of 0.2710420787334442; accuracy of 87.8260850906372%
(1145, 2, 600)
class 0 weight: 6.73202614379085
class 1 weight: 1.1744583808437856
Score for fold 2: loss of 0.3025453984737396; accuracy of 87.8260850906372%
(1145, 2, 600)
class 0 weight: 6.73202614379085
class 1 weight: 1.1744583808437856
Score for fold 3: loss of 0.33940935134887695; accuracy of 86.95651888847351%
(1145, 2, 600)
class 0 weight: 6.73202614379085
class 1 weight: 1.1744583808437856
Score for fold 4: loss of 0.48170241713523865; accuracy of 80.86956739425659%
(1145, 2, 600)
class 0 weight: 6.738562091503268
class 1 weight: 1.174259681093394
Score for fold 5: loss of 0.38053828477859497; accuracy of 84.21052694320679%
(1145, 2, 600)
class 0 weight: 6.738562091503268
class 1 weight: 1.174259681093394
Score for fold 6: loss of 0.2740374207496643; accuracy of 88.59649300575256%
(1145, 2, 600)
class 0 weight: 6.738562091503268
class 1 weight: 1.174259681093394
Score for fold 7: loss of 0.42365139722824097; accuracy of 82.45614171028137%
(1145, 2, 600)
class 0 weight: 6.738562091503268
class 1 weight: 1.174259681093394
Score for fold 8: loss of 0.3568934202194214; accuracy of 83.33333134651184%
(1145, 2, 600)
class 0 weight: 6.738562091503268
class 1 weight: 1.174259681093394
Score for fold 9: loss of 0.3297552764415741; accuracy of 85.9649121761322%
trainig on file: trainData_500.0mssec_P69 p69.conf.list.csv.arff
(1123, 2, 600)
class 0 weight: 3.6594202898550727
class 1 weight: 1.3760217983651226
Score for fold 0: loss of 0.6009618639945984; accuracy of 70.79645991325378%
(1123, 2, 600)
class 0 weight: 3.6594202898550727
class 1 weight: 1.3760217983651226
Score for fold 1: loss of 0.4841906428337097; accuracy of 75.2212405204773%
(1123, 2, 600)
class 0 weight: 3.6594202898550727
class 1 weight: 1.3760217983651226
Score for fold 2: loss of 0.41598501801490784; accuracy of 78.76105904579163%
(1123, 2, 600)
class 0 weight: 3.6498194945848375
class 1 weight: 1.377384196185286
Score for fold 3: loss of 0.48812219500541687; accuracy of 82.14285969734192%
(1123, 2, 600)
class 0 weight: 3.6498194945848375
class 1 weight: 1.377384196185286
Score for fold 4: loss of 0.5433603525161743; accuracy of 79.46428656578064%
(1123, 2, 600)
class 0 weight: 3.6498194945848375
class 1 weight: 1.377384196185286
Score for fold 5: loss of 0.5841008424758911; accuracy of 73.21428656578064%
(1123, 2, 600)
class 0 weight: 3.6630434782608696
class 1 weight: 1.3755102040816325
Score for fold 6: loss of 0.5160978436470032; accuracy of 82.14285969734192%
(1123, 2, 600)
class 0 weight: 3.6630434782608696
class 1 weight: 1.3755102040816325
Score for fold 7: loss of 0.3882564902305603; accuracy of 83.03571343421936%
(1123, 2, 600)
class 0 weight: 3.6630434782608696
class 1 weight: 1.3755102040816325
Score for fold 8: loss of 0.4201808273792267; accuracy of 83.03571343421936%
(1123, 2, 600)
class 0 weight: 3.6630434782608696
class 1 weight: 1.3755102040816325
Score for fold 9: loss of 0.46017009019851685; accuracy of 77.67857313156128%
trainig on file: trainData_500.0mssec_P74 p74.conf.matrix.csv.arff
(970, 2, 600)
class 0 weight: 5.743421052631579
class 1 weight: 1.2108183079056867
Score for fold 0: loss of 0.5206218957901001; accuracy of 83.50515365600586%
(970, 2, 600)
class 0 weight: 5.743421052631579
class 1 weight: 1.2108183079056867
Score for fold 1: loss of 0.41558244824409485; accuracy of 77.31958627700806%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 2: loss of 0.5443965792655945; accuracy of 80.41236996650696%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 3: loss of 0.6163530349731445; accuracy of 74.22680258750916%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 4: loss of 0.3866109848022461; accuracy of 86.59793734550476%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 5: loss of 0.31489065289497375; accuracy of 88.65979313850403%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 6: loss of 0.4516012370586395; accuracy of 81.44329786300659%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 7: loss of 0.45401182770729065; accuracy of 84.5360815525055%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 8: loss of 0.24928751587867737; accuracy of 90.7216489315033%
(970, 2, 600)
class 0 weight: 5.781456953642384
class 1 weight: 1.2091412742382273
Score for fold 9: loss of 0.4531450569629669; accuracy of 77.31958627700806%
trainig on file: trainData_500.0mssec_P1 p1.anatomy.list.csv.arff
(468, 2, 600)
class 0 weight: 140.33333333333331
class 1 weight: 1.007177033492823
Score for fold 0: loss of 0.28227075934410095; accuracy of 82.97872543334961%
(468, 2, 600)
class 0 weight: 140.33333333333331
class 1 weight: 1.007177033492823
Score for fold 1: loss of 0.4074920117855072; accuracy of 72.34042286872864%
(468, 2, 600)
class 0 weight: 140.33333333333331
class 1 weight: 1.007177033492823
Score for fold 2: loss of 0.3895634114742279; accuracy of 85.10638475418091%
(468, 2, 600)
class 0 weight: 140.33333333333331
class 1 weight: 1.007177033492823
Score for fold 3: loss of 0.011969374492764473; accuracy of 100.0%
(468, 2, 600)
class 0 weight: 140.33333333333331
class 1 weight: 1.007177033492823
Score for fold 4: loss of 0.25878724455833435; accuracy of 87.2340440750122%
(468, 2, 600)
class 0 weight: 210.5
class 1 weight: 1.0047732696897376
Score for fold 5: loss of 0.32501089572906494; accuracy of 82.97872543334961%
(468, 2, 600)
class 0 weight: 210.5
class 1 weight: 1.0047732696897376
Score for fold 6: loss of 0.2555577754974365; accuracy of 85.10638475418091%
(468, 2, 600)
class 0 weight: 210.5
class 1 weight: 1.0047732696897376
Score for fold 7: loss of 0.4666263163089752; accuracy of 68.08510422706604%
(468, 2, 600)
class 0 weight: 140.66666666666666
class 1 weight: 1.0071599045346062
Score for fold 8: loss of 0.225402370095253; accuracy of 86.95651888847351%
(468, 2, 600)
class 0 weight: 140.66666666666666
class 1 weight: 1.0071599045346062
Score for fold 9: loss of 0.6235988140106201; accuracy of 52.173912525177%
trainig on file: trainData_500.0mssec_P62 p62.conf.matrix.csv.arff
(1952, 2, 600)
class 0 weight: 3.1134751773049643
class 1 weight: 1.4731543624161074
Score for fold 0: loss of 0.6262751817703247; accuracy of 62.244898080825806%
(1952, 2, 600)
class 0 weight: 3.1134751773049643
class 1 weight: 1.4731543624161074
Score for fold 1: loss of 0.6330092549324036; accuracy of 64.79591727256775%
(1952, 2, 600)
class 0 weight: 3.1097345132743364
class 1 weight: 1.473993288590604
Score for fold 2: loss of 0.6537102460861206; accuracy of 58.461540937423706%
(1952, 2, 600)
class 0 weight: 3.1097345132743364
class 1 weight: 1.473993288590604
Score for fold 3: loss of 0.6443161368370056; accuracy of 66.66666865348816%
(1952, 2, 600)
class 0 weight: 3.1097345132743364
class 1 weight: 1.473993288590604
Score for fold 4: loss of 0.6620498299598694; accuracy of 61.02564334869385%
(1952, 2, 600)
class 0 weight: 3.1152482269503547
class 1 weight: 1.4727577535624476
Score for fold 5: loss of 0.5646130442619324; accuracy of 68.20513010025024%
(1952, 2, 600)
class 0 weight: 3.1152482269503547
class 1 weight: 1.4727577535624476
Score for fold 6: loss of 0.6431369185447693; accuracy of 61.538463830947876%
(1952, 2, 600)
class 0 weight: 3.1152482269503547
class 1 weight: 1.4727577535624476
Score for fold 7: loss of 0.6938923597335815; accuracy of 57.43589997291565%
(1952, 2, 600)
class 0 weight: 3.1152482269503547
class 1 weight: 1.4727577535624476
Score for fold 8: loss of 0.5823807716369629; accuracy of 68.20513010025024%
(1952, 2, 600)
class 0 weight: 3.1152482269503547
class 1 weight: 1.4727577535624476
Score for fold 9: loss of 0.56695955991745; accuracy of 66.66666865348816%
trainig on file: trainData_500.0mssec_P30 p30.anatomy.matrix.csv.arff
(470, 2, 600)
class 0 weight: 1.5666666666666667
class 1 weight: 2.764705882352941
Score for fold 0: loss of 0.41018182039260864; accuracy of 80.85106611251831%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 1: loss of 0.38707205653190613; accuracy of 80.85106611251831%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 2: loss of 0.49961626529693604; accuracy of 78.72340679168701%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 3: loss of 0.40785035490989685; accuracy of 76.59574747085571%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 4: loss of 0.37213921546936035; accuracy of 80.85106611251831%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 5: loss of 0.3781888484954834; accuracy of 82.97872543334961%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 6: loss of 0.5284899473190308; accuracy of 70.21276354789734%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 7: loss of 0.37022295594215393; accuracy of 80.85106611251831%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 8: loss of 0.4317643940448761; accuracy of 78.72340679168701%
(470, 2, 600)
class 0 weight: 1.5724907063197024
class 1 weight: 2.7467532467532467
Score for fold 9: loss of 0.3475063741207123; accuracy of 78.72340679168701%
trainig on file: trainData_500.0mssec_P49 p49.anatomy.list.csv.arff
(386, 2, 600)
class 0 weight: 2.1158536585365852
class 1 weight: 1.8961748633879782
Score for fold 0: loss of 0.3519730865955353; accuracy of 89.74359035491943%
(386, 2, 600)
class 0 weight: 2.1158536585365852
class 1 weight: 1.8961748633879782
Score for fold 1: loss of 0.36964935064315796; accuracy of 79.48718070983887%
(386, 2, 600)
class 0 weight: 2.1158536585365852
class 1 weight: 1.8961748633879782
Score for fold 2: loss of 0.3303810656070709; accuracy of 87.17948794364929%
(386, 2, 600)
class 0 weight: 2.1158536585365852
class 1 weight: 1.8961748633879782
Score for fold 3: loss of 0.2199142426252365; accuracy of 94.87179517745972%
(386, 2, 600)
class 0 weight: 2.128834355828221
class 1 weight: 1.8858695652173914
Score for fold 4: loss of 0.1858929991722107; accuracy of 92.30769276618958%
(386, 2, 600)
class 0 weight: 2.128834355828221
class 1 weight: 1.8858695652173914
Score for fold 5: loss of 0.18436557054519653; accuracy of 92.30769276618958%
(386, 2, 600)
class 0 weight: 2.1219512195121952
class 1 weight: 1.891304347826087
Score for fold 6: loss of 0.09617254883050919; accuracy of 97.36841917037964%
(386, 2, 600)
class 0 weight: 2.1219512195121952
class 1 weight: 1.891304347826087
Score for fold 7: loss of 0.34735387563705444; accuracy of 89.47368264198303%
(386, 2, 600)
class 0 weight: 2.1219512195121952
class 1 weight: 1.891304347826087
Score for fold 8: loss of 0.4000042974948883; accuracy of 84.21052694320679%
(386, 2, 600)
class 0 weight: 2.1219512195121952
class 1 weight: 1.891304347826087
Score for fold 9: loss of 0.29154637455940247; accuracy of 89.47368264198303%
trainig on file: trainData_500.0mssec_P65 p65.anatomy.list.csv.arff
(646, 2, 600)
class 0 weight: 4.469230769230769
class 1 weight: 1.288248337028825
Score for fold 0: loss of 0.3782742917537689; accuracy of 87.69230842590332%
(646, 2, 600)
class 0 weight: 4.469230769230769
class 1 weight: 1.288248337028825
Score for fold 1: loss of 0.3312270939350128; accuracy of 86.15384697914124%
(646, 2, 600)
class 0 weight: 4.503875968992248
class 1 weight: 1.2853982300884956
Score for fold 2: loss of 0.22415891289710999; accuracy of 87.69230842590332%
(646, 2, 600)
class 0 weight: 4.503875968992248
class 1 weight: 1.2853982300884956
Score for fold 3: loss of 0.14694038033485413; accuracy of 90.76923131942749%
(646, 2, 600)
class 0 weight: 4.503875968992248
class 1 weight: 1.2853982300884956
Score for fold 4: loss of 0.32197436690330505; accuracy of 81.53846263885498%
(646, 2, 600)
class 0 weight: 4.503875968992248
class 1 weight: 1.2853982300884956
Score for fold 5: loss of 0.14021824300289154; accuracy of 95.38461565971375%
(646, 2, 600)
class 0 weight: 4.476923076923077
class 1 weight: 1.2876106194690264
Score for fold 6: loss of 0.074863001704216; accuracy of 98.4375%
(646, 2, 600)
class 0 weight: 4.476923076923077
class 1 weight: 1.2876106194690264
Score for fold 7: loss of 0.2995683252811432; accuracy of 84.375%
(646, 2, 600)
class 0 weight: 4.476923076923077
class 1 weight: 1.2876106194690264
Score for fold 8: loss of 0.27820003032684326; accuracy of 84.375%
(646, 2, 600)
class 0 weight: 4.476923076923077
class 1 weight: 1.2876106194690264
Score for fold 9: loss of 0.2533379793167114; accuracy of 85.9375%
trainig on file: trainData_500.0mssec_P60 p60.anatomy.matrix.csv.arff
(823, 2, 600)
class 0 weight: 2.1264367816091956
class 1 weight: 1.887755102040816
Score for fold 0: loss of 0.33018970489501953; accuracy of 81.92771077156067%
(823, 2, 600)
class 0 weight: 2.1264367816091956
class 1 weight: 1.887755102040816
Score for fold 1: loss of 0.4132649600505829; accuracy of 81.92771077156067%
(823, 2, 600)
class 0 weight: 2.1264367816091956
class 1 weight: 1.887755102040816
Score for fold 2: loss of 0.3998583257198334; accuracy of 81.92771077156067%
(823, 2, 600)
class 0 weight: 2.1293103448275863
class 1 weight: 1.885496183206107
Score for fold 3: loss of 0.3753774166107178; accuracy of 80.48780560493469%
(823, 2, 600)
class 0 weight: 2.1293103448275863
class 1 weight: 1.885496183206107
Score for fold 4: loss of 0.3001468777656555; accuracy of 81.70731663703918%
(823, 2, 600)
class 0 weight: 2.1293103448275863
class 1 weight: 1.885496183206107
Score for fold 5: loss of 0.29090774059295654; accuracy of 81.70731663703918%
(823, 2, 600)
class 0 weight: 2.1293103448275863
class 1 weight: 1.885496183206107
Score for fold 6: loss of 0.37346789240837097; accuracy of 78.04877758026123%
(823, 2, 600)
class 0 weight: 2.1232091690544412
class 1 weight: 1.8903061224489794
Score for fold 7: loss of 0.42031794786453247; accuracy of 79.2682945728302%
(823, 2, 600)
class 0 weight: 2.1232091690544412
class 1 weight: 1.8903061224489794
Score for fold 8: loss of 0.35226356983184814; accuracy of 84.14633870124817%
(823, 2, 600)
class 0 weight: 2.1232091690544412
class 1 weight: 1.8903061224489794
Score for fold 9: loss of 0.26968133449554443; accuracy of 89.02438879013062%
trainig on file: trainData_500.0mssec_P2 p2.conf.matrix.csv.arff
(1119, 2, 600)
class 0 weight: 5.164102564102564
class 1 weight: 1.2401477832512315
Score for fold 0: loss of 0.5732884407043457; accuracy of 63.39285969734192%
(1119, 2, 600)
class 0 weight: 5.164102564102564
class 1 weight: 1.2401477832512315
Score for fold 1: loss of 0.5451909899711609; accuracy of 62.5%
(1119, 2, 600)
class 0 weight: 5.164102564102564
class 1 weight: 1.2401477832512315
Score for fold 2: loss of 0.555243194103241; accuracy of 63.39285969734192%
(1119, 2, 600)
class 0 weight: 5.190721649484536
class 1 weight: 1.2386223862238623
Score for fold 3: loss of 0.5212859511375427; accuracy of 68.75%
(1119, 2, 600)
class 0 weight: 5.190721649484536
class 1 weight: 1.2386223862238623
Score for fold 4: loss of 0.5400005578994751; accuracy of 68.75%
(1119, 2, 600)
class 0 weight: 5.190721649484536
class 1 weight: 1.2386223862238623
Score for fold 5: loss of 0.4141838848590851; accuracy of 72.32142686843872%
(1119, 2, 600)
class 0 weight: 5.190721649484536
class 1 weight: 1.2386223862238623
Score for fold 6: loss of 0.49825170636177063; accuracy of 68.75%
(1119, 2, 600)
class 0 weight: 5.190721649484536
class 1 weight: 1.2386223862238623
Score for fold 7: loss of 0.5052205324172974; accuracy of 65.17857313156128%
(1119, 2, 600)
class 0 weight: 5.190721649484536
class 1 weight: 1.2386223862238623
Score for fold 8: loss of 0.49457430839538574; accuracy of 67.85714030265808%
(1119, 2, 600)
class 0 weight: 5.1692307692307695
class 1 weight: 1.2398523985239853
Score for fold 9: loss of 0.4213429391384125; accuracy of 71.17117047309875%
trainig on file: trainData_500.0mssec_P70 p70.anatomy.matrix.csv.arff
(1366, 2, 600)
class 0 weight: 3.735562310030395
class 1 weight: 1.3655555555555556
Score for fold 0: loss of 0.2835053503513336; accuracy of 88.32116723060608%
(1366, 2, 600)
class 0 weight: 3.7469512195121952
class 1 weight: 1.3640399556048834
Score for fold 1: loss of 0.2822157144546509; accuracy of 86.86131238937378%
(1366, 2, 600)
class 0 weight: 3.7469512195121952
class 1 weight: 1.3640399556048834
Score for fold 2: loss of 0.26751577854156494; accuracy of 89.05109763145447%
(1366, 2, 600)
class 0 weight: 3.7469512195121952
class 1 weight: 1.3640399556048834
Score for fold 3: loss of 0.23880264163017273; accuracy of 89.78102207183838%
(1366, 2, 600)
class 0 weight: 3.7469512195121952
class 1 weight: 1.3640399556048834
Score for fold 4: loss of 0.2659151256084442; accuracy of 89.05109763145447%
(1366, 2, 600)
class 0 weight: 3.7469512195121952
class 1 weight: 1.3640399556048834
Score for fold 5: loss of 0.2618849575519562; accuracy of 87.59124279022217%
(1366, 2, 600)
class 0 weight: 3.7386018237082066
class 1 weight: 1.365149833518313
Score for fold 6: loss of 0.292282372713089; accuracy of 90.4411792755127%
(1366, 2, 600)
class 0 weight: 3.7386018237082066
class 1 weight: 1.365149833518313
Score for fold 7: loss of 0.2746345102787018; accuracy of 90.4411792755127%
(1366, 2, 600)
class 0 weight: 3.7386018237082066
class 1 weight: 1.365149833518313
Score for fold 8: loss of 0.2687760889530182; accuracy of 86.76470518112183%
(1366, 2, 600)
class 0 weight: 3.7386018237082066
class 1 weight: 1.365149833518313
Score for fold 9: loss of 0.3314216732978821; accuracy of 84.5588207244873%
trainig on file: trainData_500.0mssec_P24 p24.anatomy.matrix.csv.arff
(1040, 2, 600)
class 0 weight: 2.098654708520179
class 1 weight: 1.9102040816326533
Score for fold 0: loss of 0.40058720111846924; accuracy of 75.96153616905212%
(1040, 2, 600)
class 0 weight: 2.098654708520179
class 1 weight: 1.9102040816326533
Score for fold 1: loss of 0.3036235570907593; accuracy of 79.80769276618958%
(1040, 2, 600)
class 0 weight: 2.098654708520179
class 1 weight: 1.9102040816326533
Score for fold 2: loss of 0.39574432373046875; accuracy of 72.11538553237915%
(1040, 2, 600)
class 0 weight: 2.098654708520179
class 1 weight: 1.9102040816326533
Score for fold 3: loss of 0.4225236475467682; accuracy of 73.07692170143127%
(1040, 2, 600)
class 0 weight: 2.098654708520179
class 1 weight: 1.9102040816326533
Score for fold 4: loss of 0.34619611501693726; accuracy of 75.96153616905212%
(1040, 2, 600)
class 0 weight: 2.098654708520179
class 1 weight: 1.9102040816326533
Score for fold 5: loss of 0.3858855962753296; accuracy of 79.80769276618958%
(1040, 2, 600)
class 0 weight: 2.0939597315436242
class 1 weight: 1.914110429447853
Score for fold 6: loss of 0.4181075096130371; accuracy of 78.84615659713745%
(1040, 2, 600)
class 0 weight: 2.0939597315436242
class 1 weight: 1.914110429447853
Score for fold 7: loss of 0.36528825759887695; accuracy of 81.7307710647583%
(1040, 2, 600)
class 0 weight: 2.0939597315436242
class 1 weight: 1.914110429447853
Score for fold 8: loss of 0.3479514420032501; accuracy of 81.7307710647583%
(1040, 2, 600)
class 0 weight: 2.0939597315436242
class 1 weight: 1.914110429447853
Score for fold 9: loss of 0.3694173991680145; accuracy of 77.88461446762085%
trainig on file: trainData_500.0mssec_P20 p20.conf.matrix.csv.arff
(1645, 2, 600)
class 0 weight: 2.1325648414985587
class 1 weight: 1.8829516539440205
Score for fold 0: loss of 0.677904486656189; accuracy of 63.63636255264282%
(1645, 2, 600)
class 0 weight: 2.1325648414985587
class 1 weight: 1.8829516539440205
Score for fold 1: loss of 0.563763439655304; accuracy of 74.54545497894287%
(1645, 2, 600)
class 0 weight: 2.129496402877698
class 1 weight: 1.8853503184713374
Score for fold 2: loss of 0.6100844144821167; accuracy of 70.90908885002136%
(1645, 2, 600)
class 0 weight: 2.129496402877698
class 1 weight: 1.8853503184713374
Score for fold 3: loss of 0.5420374274253845; accuracy of 69.69696879386902%
(1645, 2, 600)
class 0 weight: 2.129496402877698
class 1 weight: 1.8853503184713374
Score for fold 4: loss of 0.498593270778656; accuracy of 72.72727489471436%
(1645, 2, 600)
class 0 weight: 2.130935251798561
class 1 weight: 1.8842239185750638
Score for fold 5: loss of 0.5401901006698608; accuracy of 69.51219439506531%
(1645, 2, 600)
class 0 weight: 2.130935251798561
class 1 weight: 1.8842239185750638
Score for fold 6: loss of 0.5711422562599182; accuracy of 70.7317054271698%
(1645, 2, 600)
class 0 weight: 2.130935251798561
class 1 weight: 1.8842239185750638
Score for fold 7: loss of 0.4955788254737854; accuracy of 73.17073345184326%
(1645, 2, 600)
class 0 weight: 2.130935251798561
class 1 weight: 1.8842239185750638
Score for fold 8: loss of 0.611534059047699; accuracy of 69.51219439506531%
(1645, 2, 600)
class 0 weight: 2.130935251798561
class 1 weight: 1.8842239185750638
Score for fold 9: loss of 0.5412954092025757; accuracy of 74.39024448394775%
trainig on file: trainData_500.0mssec_P37 p37.anatomy.list.csv.arff
(485, 2, 600)
class 0 weight: 1.7098039215686274
class 1 weight: 2.408839779005525
Score for fold 0: loss of 0.5055983662605286; accuracy of 77.55101919174194%
(485, 2, 600)
class 0 weight: 1.7098039215686274
class 1 weight: 2.408839779005525
Score for fold 1: loss of 0.2179303914308548; accuracy of 91.8367326259613%
(485, 2, 600)
class 0 weight: 1.7165354330708662
class 1 weight: 2.395604395604396
Score for fold 2: loss of 0.27066493034362793; accuracy of 87.7551019191742%
(485, 2, 600)
class 0 weight: 1.7165354330708662
class 1 weight: 2.395604395604396
Score for fold 3: loss of 0.2074228972196579; accuracy of 89.79591727256775%
(485, 2, 600)
class 0 weight: 1.7165354330708662
class 1 weight: 2.395604395604396
Score for fold 4: loss of 0.22850316762924194; accuracy of 93.87755393981934%
(485, 2, 600)
class 0 weight: 1.7137254901960783
class 1 weight: 2.4010989010989015
Score for fold 5: loss of 0.1603199988603592; accuracy of 91.66666865348816%
(485, 2, 600)
class 0 weight: 1.7137254901960783
class 1 weight: 2.4010989010989015
Score for fold 6: loss of 0.3046131134033203; accuracy of 85.41666865348816%
(485, 2, 600)
class 0 weight: 1.7137254901960783
class 1 weight: 2.4010989010989015
Score for fold 7: loss of 0.3401525914669037; accuracy of 81.25%
(485, 2, 600)
class 0 weight: 1.7137254901960783
class 1 weight: 2.4010989010989015
Score for fold 8: loss of 0.157958522439003; accuracy of 91.66666865348816%
(485, 2, 600)
class 0 weight: 1.7137254901960783
class 1 weight: 2.4010989010989015
Score for fold 9: loss of 0.22155225276947021; accuracy of 85.41666865348816%
trainig on file: trainData_500.0mssec_P45 p45.anatomy.list.csv.arff
(497, 2, 600)
class 0 weight: 19.434782608695652
class 1 weight: 1.0542452830188678
Score for fold 0: loss of 0.42319464683532715; accuracy of 80.0000011920929%
(497, 2, 600)
class 0 weight: 19.434782608695652
class 1 weight: 1.0542452830188678
Score for fold 1: loss of 0.2388828992843628; accuracy of 87.99999952316284%
(497, 2, 600)
class 0 weight: 20.31818181818182
class 1 weight: 1.051764705882353
Score for fold 2: loss of 0.3734026849269867; accuracy of 80.0000011920929%
(497, 2, 600)
class 0 weight: 20.31818181818182
class 1 weight: 1.051764705882353
Score for fold 3: loss of 0.41656941175460815; accuracy of 75.99999904632568%
(497, 2, 600)
class 0 weight: 20.31818181818182
class 1 weight: 1.051764705882353
Score for fold 4: loss of 0.3670591413974762; accuracy of 80.0000011920929%
(497, 2, 600)
class 0 weight: 20.31818181818182
class 1 weight: 1.051764705882353
Score for fold 5: loss of 0.22108563780784607; accuracy of 87.99999952316284%
(497, 2, 600)
class 0 weight: 20.31818181818182
class 1 weight: 1.051764705882353
Score for fold 6: loss of 0.342864990234375; accuracy of 74.00000095367432%
(497, 2, 600)
class 0 weight: 19.47826086956522
class 1 weight: 1.0541176470588234
Score for fold 7: loss of 0.24549968540668488; accuracy of 83.67347121238708%
(497, 2, 600)
class 0 weight: 19.47826086956522
class 1 weight: 1.0541176470588234
Score for fold 8: loss of 0.18927058577537537; accuracy of 89.79591727256775%
(497, 2, 600)
class 0 weight: 19.47826086956522
class 1 weight: 1.0541176470588234
Score for fold 9: loss of 0.25882554054260254; accuracy of 87.7551019191742%
trainig on file: trainData_500.0mssec_P10 p10.anatomy.matrix.csv.arff
(586, 2, 600)
class 0 weight: 2.5336538461538463
class 1 weight: 1.6520376175548588
Score for fold 0: loss of 0.28610533475875854; accuracy of 91.52542352676392%
(586, 2, 600)
class 0 weight: 2.5336538461538463
class 1 weight: 1.6520376175548588
Score for fold 1: loss of 0.3253985047340393; accuracy of 86.44067645072937%
(586, 2, 600)
class 0 weight: 2.5215311004784686
class 1 weight: 1.657232704402516
Score for fold 2: loss of 0.3137192130088806; accuracy of 84.74576473236084%
(586, 2, 600)
class 0 weight: 2.5215311004784686
class 1 weight: 1.657232704402516
Score for fold 3: loss of 0.2770150601863861; accuracy of 91.52542352676392%
(586, 2, 600)
class 0 weight: 2.5215311004784686
class 1 weight: 1.657232704402516
Score for fold 4: loss of 0.349414199590683; accuracy of 86.44067645072937%
(586, 2, 600)
class 0 weight: 2.5215311004784686
class 1 weight: 1.657232704402516
Score for fold 5: loss of 0.2936606705188751; accuracy of 86.44067645072937%
(586, 2, 600)
class 0 weight: 2.526315789473684
class 1 weight: 1.6551724137931034
Score for fold 6: loss of 0.14458459615707397; accuracy of 96.55172228813171%
(586, 2, 600)
class 0 weight: 2.526315789473684
class 1 weight: 1.6551724137931034
Score for fold 7: loss of 0.2959209680557251; accuracy of 87.93103694915771%
(586, 2, 600)
class 0 weight: 2.526315789473684
class 1 weight: 1.6551724137931034
Score for fold 8: loss of 0.19251284003257751; accuracy of 93.1034505367279%
(586, 2, 600)
class 0 weight: 2.526315789473684
class 1 weight: 1.6551724137931034
Score for fold 9: loss of 0.32706743478775024; accuracy of 89.65517282485962%
trainig on file: trainData_500.0mssec_P69 p69.anatomy.list.csv.arff
(426, 2, 600)
class 0 weight: 4.614457831325302
class 1 weight: 1.2766666666666668
Score for fold 0: loss of 0.23591463267803192; accuracy of 88.37209343910217%
(426, 2, 600)
class 0 weight: 4.614457831325302
class 1 weight: 1.2766666666666668
Score for fold 1: loss of 0.315233051776886; accuracy of 88.37209343910217%
(426, 2, 600)
class 0 weight: 4.614457831325302
class 1 weight: 1.2766666666666668
Score for fold 2: loss of 0.17476573586463928; accuracy of 97.67441749572754%
(426, 2, 600)
class 0 weight: 4.614457831325302
class 1 weight: 1.2766666666666668
Score for fold 3: loss of 0.29945439100265503; accuracy of 90.69767594337463%
(426, 2, 600)
class 0 weight: 4.670731707317073
class 1 weight: 1.2724252491694352
Score for fold 4: loss of 0.13383187353610992; accuracy of 95.34883499145508%
(426, 2, 600)
class 0 weight: 4.670731707317073
class 1 weight: 1.2724252491694352
Score for fold 5: loss of 0.12845033407211304; accuracy of 93.0232584476471%
(426, 2, 600)
class 0 weight: 4.626506024096386
class 1 weight: 1.2757475083056478
Score for fold 6: loss of 0.16693712770938873; accuracy of 95.23809552192688%
(426, 2, 600)
class 0 weight: 4.626506024096386
class 1 weight: 1.2757475083056478
Score for fold 7: loss of 0.09844095259904861; accuracy of 95.23809552192688%
(426, 2, 600)
class 0 weight: 4.626506024096386
class 1 weight: 1.2757475083056478
Score for fold 8: loss of 0.1413809210062027; accuracy of 95.23809552192688%
(426, 2, 600)
class 0 weight: 4.626506024096386
class 1 weight: 1.2757475083056478
Score for fold 9: loss of 0.14485789835453033; accuracy of 95.23809552192688%
trainig on file: trainData_500.0mssec_P66 p66.conf.matrix.csv.arff
(1460, 2, 600)
class 0 weight: 2.1827242524916945
class 1 weight: 1.845505617977528
Score for fold 0: loss of 0.5994380116462708; accuracy of 71.91780805587769%
(1460, 2, 600)
class 0 weight: 2.1827242524916945
class 1 weight: 1.845505617977528
Score for fold 1: loss of 0.5257513523101807; accuracy of 78.08219194412231%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 2: loss of 0.5357320308685303; accuracy of 73.2876718044281%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 3: loss of 0.4812680184841156; accuracy of 72.60273694992065%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 4: loss of 0.4695010781288147; accuracy of 76.02739930152893%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 5: loss of 0.36207765340805054; accuracy of 86.30136847496033%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 6: loss of 0.4398106038570404; accuracy of 79.45205569267273%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 7: loss of 0.49915218353271484; accuracy of 73.97260069847107%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 8: loss of 0.5012767910957336; accuracy of 71.23287916183472%
(1460, 2, 600)
class 0 weight: 2.1863560732113143
class 1 weight: 1.8429172510518934
Score for fold 9: loss of 0.41570228338241577; accuracy of 77.39726305007935%
trainig on file: trainData_500.0mssec_P67 p67.anatomy.list.csv.arff
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 0: loss of 0.5309558510780334; accuracy of 78.57142686843872%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 1: loss of 0.16623783111572266; accuracy of 92.85714030265808%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 2: loss of 0.15370073914527893; accuracy of 96.42857313156128%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 3: loss of 0.2959827780723572; accuracy of 85.71428656578064%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 4: loss of 0.24147221446037292; accuracy of 85.71428656578064%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 5: loss of 0.10211334377527237; accuracy of 96.42857313156128%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 6: loss of 0.14022669196128845; accuracy of 92.85714030265808%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 7: loss of 0.35619375109672546; accuracy of 85.71428656578064%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 8: loss of 0.07905153930187225; accuracy of 96.42857313156128%
(280, 2, 600)
class 0 weight: 2.8000000000000003
class 1 weight: 1.5555555555555554
Score for fold 9: loss of 0.14123496413230896; accuracy of 96.42857313156128%
trainig on file: trainData_500.0mssec_P47 p47.conf.list.csv.arff
(793, 2, 600)
class 0 weight: 6.366071428571428
class 1 weight: 1.1863560732113145
Score for fold 0: loss of 0.40802136063575745; accuracy of 78.75000238418579%
(793, 2, 600)
class 0 weight: 6.366071428571428
class 1 weight: 1.1863560732113145
Score for fold 1: loss of 0.3503074645996094; accuracy of 81.25%
(793, 2, 600)
class 0 weight: 6.366071428571428
class 1 weight: 1.1863560732113145
Score for fold 2: loss of 0.44021517038345337; accuracy of 77.49999761581421%
(793, 2, 600)
class 0 weight: 6.31858407079646
class 1 weight: 1.1880199667221298
Score for fold 3: loss of 0.41358667612075806; accuracy of 75.94936490058899%
(793, 2, 600)
class 0 weight: 6.31858407079646
class 1 weight: 1.1880199667221298
Score for fold 4: loss of 0.4169653058052063; accuracy of 79.7468364238739%
(793, 2, 600)
class 0 weight: 6.31858407079646
class 1 weight: 1.1880199667221298
Score for fold 5: loss of 0.4199139177799225; accuracy of 79.7468364238739%
(793, 2, 600)
class 0 weight: 6.31858407079646
class 1 weight: 1.1880199667221298
Score for fold 6: loss of 0.3836332857608795; accuracy of 82.27847814559937%
(793, 2, 600)
class 0 weight: 6.31858407079646
class 1 weight: 1.1880199667221298
Score for fold 7: loss of 0.3577686548233032; accuracy of 79.7468364238739%
(793, 2, 600)
class 0 weight: 6.375
class 1 weight: 1.186046511627907
Score for fold 8: loss of 0.3727201521396637; accuracy of 82.27847814559937%
(793, 2, 600)
class 0 weight: 6.375
class 1 weight: 1.186046511627907
Score for fold 9: loss of 0.3630765974521637; accuracy of 84.8101258277893%
trainig on file: trainData_500.0mssec_P26 p26.conf.matrix.csv.arff
(1252, 2, 600)
class 0 weight: 9.229508196721312
class 1 weight: 1.1215139442231075
Score for fold 0: loss of 0.473513662815094; accuracy of 68.2539701461792%
(1252, 2, 600)
class 0 weight: 9.229508196721312
class 1 weight: 1.1215139442231075
Score for fold 1: loss of 0.41577646136283875; accuracy of 68.2539701461792%
(1252, 2, 600)
class 0 weight: 9.162601626016261
class 1 weight: 1.1225099601593624
Score for fold 2: loss of 0.46919792890548706; accuracy of 63.999998569488525%
(1252, 2, 600)
class 0 weight: 9.162601626016261
class 1 weight: 1.1225099601593624
Score for fold 3: loss of 0.28800198435783386; accuracy of 79.19999957084656%
(1252, 2, 600)
class 0 weight: 9.162601626016261
class 1 weight: 1.1225099601593624
Score for fold 4: loss of 0.4138045012950897; accuracy of 70.39999961853027%
(1252, 2, 600)
class 0 weight: 9.162601626016261
class 1 weight: 1.1225099601593624
Score for fold 5: loss of 0.23713891208171844; accuracy of 84.79999899864197%
(1252, 2, 600)
class 0 weight: 9.237704918032787
class 1 weight: 1.1213930348258705
Score for fold 6: loss of 0.4124712646007538; accuracy of 68.00000071525574%
(1252, 2, 600)
class 0 weight: 9.237704918032787
class 1 weight: 1.1213930348258705
Score for fold 7: loss of 0.4392459988594055; accuracy of 67.1999990940094%
(1252, 2, 600)
class 0 weight: 9.237704918032787
class 1 weight: 1.1213930348258705
Score for fold 8: loss of 0.3698851764202118; accuracy of 72.79999852180481%
(1252, 2, 600)
class 0 weight: 9.237704918032787
class 1 weight: 1.1213930348258705
Score for fold 9: loss of 0.4782642126083374; accuracy of 68.00000071525574%
trainig on file: trainData_500.0mssec_P56 p56.conf.matrix.csv.arff
(1032, 2, 600)
class 0 weight: 3.338129496402878
class 1 weight: 1.4276923076923076
Score for fold 0: loss of 0.7233823537826538; accuracy of 59.61538553237915%
(1032, 2, 600)
class 0 weight: 3.338129496402878
class 1 weight: 1.4276923076923076
Score for fold 1: loss of 0.6909925937652588; accuracy of 58.65384340286255%
(1032, 2, 600)
class 0 weight: 3.3297491039426523
class 1 weight: 1.4292307692307693
Score for fold 2: loss of 0.46082809567451477; accuracy of 71.84466123580933%
(1032, 2, 600)
class 0 weight: 3.341726618705036
class 1 weight: 1.4270353302611367
Score for fold 3: loss of 0.5969376564025879; accuracy of 62.13592290878296%
(1032, 2, 600)
class 0 weight: 3.341726618705036
class 1 weight: 1.4270353302611367
Score for fold 4: loss of 0.4864000082015991; accuracy of 74.75728392601013%
(1032, 2, 600)
class 0 weight: 3.341726618705036
class 1 weight: 1.4270353302611367
Score for fold 5: loss of 0.7012879252433777; accuracy of 62.13592290878296%
(1032, 2, 600)
class 0 weight: 3.341726618705036
class 1 weight: 1.4270353302611367
Score for fold 6: loss of 0.4952431321144104; accuracy of 66.99029207229614%
(1032, 2, 600)
class 0 weight: 3.341726618705036
class 1 weight: 1.4270353302611367
Score for fold 7: loss of 0.5851704478263855; accuracy of 70.8737850189209%
(1032, 2, 600)
class 0 weight: 3.341726618705036
class 1 weight: 1.4270353302611367
Score for fold 8: loss of 0.588251531124115; accuracy of 62.13592290878296%
(1032, 2, 600)
class 0 weight: 3.341726618705036
class 1 weight: 1.4270353302611367
Score for fold 9: loss of 0.4486151337623596; accuracy of 71.84466123580933%
trainig on file: trainData_500.0mssec_P6 p6.conf.matrix.csv.arff
(937, 2, 600)
class 0 weight: 1.8609271523178808
class 1 weight: 2.1615384615384614
Score for fold 0: loss of 0.4856991469860077; accuracy of 77.65957713127136%
(937, 2, 600)
class 0 weight: 1.8609271523178808
class 1 weight: 2.1615384615384614
Score for fold 1: loss of 0.42274409532546997; accuracy of 73.40425252914429%
(937, 2, 600)
class 0 weight: 1.8609271523178808
class 1 weight: 2.1615384615384614
Score for fold 2: loss of 0.41722217202186584; accuracy of 75.53191781044006%
(937, 2, 600)
class 0 weight: 1.8609271523178808
class 1 weight: 2.1615384615384614
Score for fold 3: loss of 0.4960534870624542; accuracy of 78.72340679168701%
(937, 2, 600)
class 0 weight: 1.8650442477876106
class 1 weight: 2.1560102301790285
Score for fold 4: loss of 0.36888009309768677; accuracy of 73.40425252914429%
(937, 2, 600)
class 0 weight: 1.8650442477876106
class 1 weight: 2.1560102301790285
Score for fold 5: loss of 0.3348468840122223; accuracy of 79.78723645210266%
(937, 2, 600)
class 0 weight: 1.8650442477876106
class 1 weight: 2.1560102301790285
Score for fold 6: loss of 0.3591247797012329; accuracy of 82.97872543334961%
(937, 2, 600)
class 0 weight: 1.8631346578366446
class 1 weight: 2.1585677749360617
Score for fold 7: loss of 0.3341417610645294; accuracy of 83.87096524238586%
(937, 2, 600)
class 0 weight: 1.8631346578366446
class 1 weight: 2.1585677749360617
Score for fold 8: loss of 0.3065721392631531; accuracy of 88.17204236984253%
(937, 2, 600)
class 0 weight: 1.8631346578366446
class 1 weight: 2.1585677749360617
Score for fold 9: loss of 0.2541009783744812; accuracy of 87.09677457809448%
trainig on file: trainData_500.0mssec_P11 p11.anatomy.list.csv.arff
(881, 2, 600)
class 0 weight: 9.31764705882353
class 1 weight: 1.1202263083451203
Score for fold 0: loss of 0.27885621786117554; accuracy of 84.26966071128845%
(881, 2, 600)
class 0 weight: 9.220930232558139
class 1 weight: 1.1216407355021216
Score for fold 1: loss of 0.19741320610046387; accuracy of 87.5%
(881, 2, 600)
class 0 weight: 9.220930232558139
class 1 weight: 1.1216407355021216
Score for fold 2: loss of 0.3066851794719696; accuracy of 82.95454382896423%
(881, 2, 600)
class 0 weight: 9.220930232558139
class 1 weight: 1.1216407355021216
Score for fold 3: loss of 0.2645949125289917; accuracy of 81.81818127632141%
(881, 2, 600)
class 0 weight: 9.220930232558139
class 1 weight: 1.1216407355021216
Score for fold 4: loss of 0.3278267979621887; accuracy of 85.22727489471436%
(881, 2, 600)
class 0 weight: 9.220930232558139
class 1 weight: 1.1216407355021216
Score for fold 5: loss of 0.27846840023994446; accuracy of 85.22727489471436%
(881, 2, 600)
class 0 weight: 9.329411764705883
class 1 weight: 1.1200564971751412
Score for fold 6: loss of 0.17641957104206085; accuracy of 87.5%
(881, 2, 600)
class 0 weight: 9.329411764705883
class 1 weight: 1.1200564971751412
Score for fold 7: loss of 0.31007736921310425; accuracy of 84.09090638160706%
(881, 2, 600)
class 0 weight: 9.329411764705883
class 1 weight: 1.1200564971751412
Score for fold 8: loss of 0.25058937072753906; accuracy of 84.09090638160706%
(881, 2, 600)
class 0 weight: 9.329411764705883
class 1 weight: 1.1200564971751412
Score for fold 9: loss of 0.3009946048259735; accuracy of 80.68181872367859%
trainig on file: trainData_500.0mssec_P25 p25.conf.list.csv.arff
(1132, 2, 600)
class 0 weight: 2.4708737864077666
class 1 weight: 1.6798679867986799
Score for fold 0: loss of 0.4218202531337738; accuracy of 78.0701756477356%
(1132, 2, 600)
class 0 weight: 2.4708737864077666
class 1 weight: 1.6798679867986799
Score for fold 1: loss of 0.4703260064125061; accuracy of 76.31579041481018%
(1132, 2, 600)
class 0 weight: 2.4733009708737863
class 1 weight: 1.6787479406919275
Score for fold 2: loss of 0.41921645402908325; accuracy of 76.9911527633667%
(1132, 2, 600)
class 0 weight: 2.4733009708737863
class 1 weight: 1.6787479406919275
Score for fold 3: loss of 0.5590271353721619; accuracy of 74.33628439903259%
(1132, 2, 600)
class 0 weight: 2.4733009708737863
class 1 weight: 1.6787479406919275
Score for fold 4: loss of 0.4911375343799591; accuracy of 80.53097128868103%
(1132, 2, 600)
class 0 weight: 2.4733009708737863
class 1 weight: 1.6787479406919275
Score for fold 5: loss of 0.4010680615901947; accuracy of 73.45132827758789%
(1132, 2, 600)
class 0 weight: 2.4733009708737863
class 1 weight: 1.6787479406919275
Score for fold 6: loss of 0.48309266567230225; accuracy of 69.91150379180908%
(1132, 2, 600)
class 0 weight: 2.4733009708737863
class 1 weight: 1.6787479406919275
Score for fold 7: loss of 0.44568532705307007; accuracy of 79.64601516723633%
(1132, 2, 600)
class 0 weight: 2.467312348668281
class 1 weight: 1.6815181518151816
Score for fold 8: loss of 0.33766552805900574; accuracy of 86.72566413879395%
(1132, 2, 600)
class 0 weight: 2.467312348668281
class 1 weight: 1.6815181518151816
Score for fold 9: loss of 0.31677138805389404; accuracy of 84.95575189590454%
trainig on file: trainData_500.0mssec_P65 p65.conf.list.csv.arff
(1416, 2, 600)
class 0 weight: 3.5887323943661973
class 1 weight: 1.3862894450489662
Score for fold 0: loss of 0.4934718608856201; accuracy of 74.64788556098938%
(1416, 2, 600)
class 0 weight: 3.5887323943661973
class 1 weight: 1.3862894450489662
Score for fold 1: loss of 0.43377768993377686; accuracy of 80.28169274330139%
(1416, 2, 600)
class 0 weight: 3.598870056497175
class 1 weight: 1.3847826086956523
Score for fold 2: loss of 0.3918731212615967; accuracy of 80.98591566085815%
(1416, 2, 600)
class 0 weight: 3.598870056497175
class 1 weight: 1.3847826086956523
Score for fold 3: loss of 0.5506801009178162; accuracy of 71.12675905227661%
(1416, 2, 600)
class 0 weight: 3.598870056497175
class 1 weight: 1.3847826086956523
Score for fold 4: loss of 0.4510738253593445; accuracy of 77.46478915214539%
(1416, 2, 600)
class 0 weight: 3.598870056497175
class 1 weight: 1.3847826086956523
Score for fold 5: loss of 0.3827934265136719; accuracy of 80.98591566085815%
(1416, 2, 600)
class 0 weight: 3.591549295774648
class 1 weight: 1.3858695652173914
Score for fold 6: loss of 0.5510873198509216; accuracy of 80.14184236526489%
(1416, 2, 600)
class 0 weight: 3.591549295774648
class 1 weight: 1.3858695652173914
Score for fold 7: loss of 0.3879334628582001; accuracy of 82.26950168609619%
(1416, 2, 600)
class 0 weight: 3.591549295774648
class 1 weight: 1.3858695652173914
Score for fold 8: loss of 0.3679385483264923; accuracy of 80.85106611251831%
(1416, 2, 600)
class 0 weight: 3.591549295774648
class 1 weight: 1.3858695652173914
Score for fold 9: loss of 0.457346647977829; accuracy of 74.46808218955994%
trainig on file: trainData_500.0mssec_P74 p74.anatomy.matrix.csv.arff
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 0: loss of 0.5722094774246216; accuracy of 58.139532804489136%
Participant reached > 0.8 val_acc: trainData_500.0mssec_P74 p74.anatomy.matrix.csv.arff
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 1: loss of 0.2997487187385559; accuracy of 83.72092843055725%
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 2: loss of 0.24504444003105164; accuracy of 86.04651093482971%
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 3: loss of 0.5203428268432617; accuracy of 65.11628031730652%
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 4: loss of 0.3189285695552826; accuracy of 83.72092843055725%
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 5: loss of 0.1686396449804306; accuracy of 88.37209343910217%
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 6: loss of 0.3424786925315857; accuracy of 76.74418687820435%
(428, 2, 600)
class 0 weight: 14.259259259259258
class 1 weight: 1.0754189944134078
Score for fold 7: loss of 0.3361193835735321; accuracy of 81.39534592628479%
(428, 2, 600)
class 0 weight: 14.296296296296296
class 1 weight: 1.075208913649025
Score for fold 8: loss of 0.47440123558044434; accuracy of 76.1904776096344%
(428, 2, 600)
class 0 weight: 14.296296296296296
class 1 weight: 1.075208913649025
Score for fold 9: loss of 0.37202367186546326; accuracy of 78.57142686843872%
trainig on file: trainData_500.0mssec_P23 p23.anatomy.list.csv.arff
(554, 2, 600)
class 0 weight: 2.3827751196172247
class 1 weight: 1.7231833910034602
Score for fold 0: loss of 0.35417094826698303; accuracy of 83.92857313156128%
(554, 2, 600)
class 0 weight: 2.3827751196172247
class 1 weight: 1.7231833910034602
Score for fold 1: loss of 0.4159530997276306; accuracy of 82.14285969734192%
(554, 2, 600)
class 0 weight: 2.394230769230769
class 1 weight: 1.7172413793103447
Score for fold 2: loss of 0.21276967227458954; accuracy of 89.28571343421936%
(554, 2, 600)
class 0 weight: 2.394230769230769
class 1 weight: 1.7172413793103447
Score for fold 3: loss of 0.39541342854499817; accuracy of 78.57142686843872%
(554, 2, 600)
class 0 weight: 2.38755980861244
class 1 weight: 1.7206896551724138
Score for fold 4: loss of 0.32872268557548523; accuracy of 89.09090757369995%
(554, 2, 600)
class 0 weight: 2.38755980861244
class 1 weight: 1.7206896551724138
Score for fold 5: loss of 0.29160821437835693; accuracy of 81.81818127632141%
(554, 2, 600)
class 0 weight: 2.38755980861244
class 1 weight: 1.7206896551724138
Score for fold 6: loss of 0.22314728796482086; accuracy of 92.72727370262146%
(554, 2, 600)
class 0 weight: 2.38755980861244
class 1 weight: 1.7206896551724138
Score for fold 7: loss of 0.2041754275560379; accuracy of 92.72727370262146%
(554, 2, 600)
class 0 weight: 2.38755980861244
class 1 weight: 1.7206896551724138
Score for fold 8: loss of 0.23071826994419098; accuracy of 81.81818127632141%
(554, 2, 600)
class 0 weight: 2.38755980861244
class 1 weight: 1.7206896551724138
Score for fold 9: loss of 0.29260748624801636; accuracy of 87.27272748947144%
trainig on file: trainData_500.0mssec_P50 p50.conf.matrix.csv.arff
(1468, 2, 600)
class 0 weight: 1.8296398891966759
class 1 weight: 2.2053422370617697
Score for fold 0: loss of 0.5401926636695862; accuracy of 77.55101919174194%
(1468, 2, 600)
class 0 weight: 1.8296398891966759
class 1 weight: 2.2053422370617697
Score for fold 1: loss of 0.4941037893295288; accuracy of 78.23129296302795%
(1468, 2, 600)
class 0 weight: 1.8296398891966759
class 1 weight: 2.2053422370617697
Score for fold 2: loss of 0.5279202461242676; accuracy of 75.51020383834839%
(1468, 2, 600)
class 0 weight: 1.8296398891966759
class 1 weight: 2.2053422370617697
Score for fold 3: loss of 0.315372109413147; accuracy of 83.67347121238708%
(1468, 2, 600)
class 0 weight: 1.8296398891966759
class 1 weight: 2.2053422370617697
Score for fold 4: loss of 0.45729994773864746; accuracy of 77.55101919174194%
(1468, 2, 600)
class 0 weight: 1.8296398891966759
class 1 weight: 2.2053422370617697
Score for fold 5: loss of 0.37545591592788696; accuracy of 79.5918345451355%
(1468, 2, 600)
class 0 weight: 1.8321775312066575
class 1 weight: 2.2016666666666667
Score for fold 6: loss of 0.3807503283023834; accuracy of 82.99319744110107%
(1468, 2, 600)
class 0 weight: 1.8321775312066575
class 1 weight: 2.2016666666666667
Score for fold 7: loss of 0.3755532503128052; accuracy of 85.03401279449463%
(1468, 2, 600)
class 0 weight: 1.8310249307479225
class 1 weight: 2.2033333333333336
Score for fold 8: loss of 0.416731595993042; accuracy of 84.24657583236694%
(1468, 2, 600)
class 0 weight: 1.8310249307479225
class 1 weight: 2.2033333333333336
Score for fold 9: loss of 0.30325615406036377; accuracy of 87.67123222351074%
trainig on file: trainData_500.0mssec_P22 p22.conf.matrix.csv.arff
(1286, 2, 600)
class 0 weight: 2.944020356234097
class 1 weight: 1.5143979057591623
Score for fold 0: loss of 0.41930797696113586; accuracy of 62.01550364494324%
(1286, 2, 600)
class 0 weight: 2.944020356234097
class 1 weight: 1.5143979057591623
Score for fold 1: loss of 0.4818430542945862; accuracy of 79.0697693824768%
(1286, 2, 600)
class 0 weight: 2.944020356234097
class 1 weight: 1.5143979057591623
Score for fold 2: loss of 0.41975778341293335; accuracy of 78.29457521438599%
(1286, 2, 600)
class 0 weight: 2.944020356234097
class 1 weight: 1.5143979057591623
Score for fold 3: loss of 0.37813955545425415; accuracy of 84.49612259864807%
(1286, 2, 600)
class 0 weight: 2.944020356234097
class 1 weight: 1.5143979057591623
Score for fold 4: loss of 0.3924073874950409; accuracy of 82.94573426246643%
(1286, 2, 600)
class 0 weight: 2.944020356234097
class 1 weight: 1.5143979057591623
Score for fold 5: loss of 0.4007818400859833; accuracy of 88.37209343910217%
(1286, 2, 600)
class 0 weight: 2.9465648854961835
class 1 weight: 1.5137254901960784
Score for fold 6: loss of 0.3500235676765442; accuracy of 84.375%
(1286, 2, 600)
class 0 weight: 2.9390862944162435
class 1 weight: 1.5157068062827226
Score for fold 7: loss of 0.3512563407421112; accuracy of 83.59375%
(1286, 2, 600)
class 0 weight: 2.9390862944162435
class 1 weight: 1.5157068062827226
Score for fold 8: loss of 0.4732089936733246; accuracy of 83.59375%
(1286, 2, 600)
class 0 weight: 2.9390862944162435
class 1 weight: 1.5157068062827226
Score for fold 9: loss of 0.3323619067668915; accuracy of 84.375%
trainig on file: trainData_500.0mssec_P52 p52.conf.matrix.csv.arff
(1138, 2, 600)
class 0 weight: 1.1352549889135255
class 1 weight: 8.39344262295082
Score for fold 0: loss of 0.3642338514328003; accuracy of 83.33333134651184%
(1138, 2, 600)
class 0 weight: 1.1352549889135255
class 1 weight: 8.39344262295082
Score for fold 1: loss of 0.4996212124824524; accuracy of 72.80701994895935%
(1138, 2, 600)
class 0 weight: 1.1352549889135255
class 1 weight: 8.39344262295082
Score for fold 2: loss of 0.4929139316082001; accuracy of 75.43859481811523%
(1138, 2, 600)
class 0 weight: 1.1339977851605758
class 1 weight: 8.462809917355372
Score for fold 3: loss of 0.5608767867088318; accuracy of 68.42105388641357%
(1138, 2, 600)
class 0 weight: 1.1339977851605758
class 1 weight: 8.462809917355372
Score for fold 4: loss of 0.3242749273777008; accuracy of 80.70175647735596%
(1138, 2, 600)
class 0 weight: 1.1339977851605758
class 1 weight: 8.462809917355372
Score for fold 5: loss of 0.4124716520309448; accuracy of 79.82456088066101%
(1138, 2, 600)
class 0 weight: 1.1339977851605758
class 1 weight: 8.462809917355372
Score for fold 6: loss of 0.4844205677509308; accuracy of 65.78947305679321%
(1138, 2, 600)
class 0 weight: 1.1339977851605758
class 1 weight: 8.462809917355372
Score for fold 7: loss of 0.4699278175830841; accuracy of 74.56140518188477%
(1138, 2, 600)
class 0 weight: 1.1351052048726467
class 1 weight: 8.401639344262296
Score for fold 8: loss of 0.864876925945282; accuracy of 54.86725568771362%
(1138, 2, 600)
class 0 weight: 1.1351052048726467
class 1 weight: 8.401639344262296
Score for fold 9: loss of 0.6896513104438782; accuracy of 56.63716793060303%
trainig on file: trainData_500.0mssec_P79 p79.anatomy.list.csv.arff
(330, 2, 600)
class 0 weight: 22.846153846153847
class 1 weight: 1.045774647887324
Score for fold 0: loss of 0.3173542022705078; accuracy of 84.84848737716675%
(330, 2, 600)
class 0 weight: 22.846153846153847
class 1 weight: 1.045774647887324
Score for fold 1: loss of 0.5671908855438232; accuracy of 75.75757503509521%
(330, 2, 600)
class 0 weight: 22.846153846153847
class 1 weight: 1.045774647887324
Score for fold 2: loss of 0.5457871556282043; accuracy of 75.75757503509521%
(330, 2, 600)
class 0 weight: 22.846153846153847
class 1 weight: 1.045774647887324
Score for fold 3: loss of 0.1342705637216568; accuracy of 90.90909361839294%
(330, 2, 600)
class 0 weight: 22.846153846153847
class 1 weight: 1.045774647887324
Score for fold 4: loss of 0.20195037126541138; accuracy of 90.90909361839294%
(330, 2, 600)
class 0 weight: 22.846153846153847
class 1 weight: 1.045774647887324
Score for fold 5: loss of 0.32267507910728455; accuracy of 84.84848737716675%
(330, 2, 600)
class 0 weight: 24.75
class 1 weight: 1.0421052631578946
Score for fold 6: loss of 0.3314109444618225; accuracy of 84.84848737716675%
(330, 2, 600)
class 0 weight: 24.75
class 1 weight: 1.0421052631578946
Score for fold 7: loss of 0.07215116173028946; accuracy of 96.96969985961914%
(330, 2, 600)
class 0 weight: 24.75
class 1 weight: 1.0421052631578946
Score for fold 8: loss of 0.1445031315088272; accuracy of 93.9393937587738%
(330, 2, 600)
class 0 weight: 24.75
class 1 weight: 1.0421052631578946
Score for fold 9: loss of 0.1381942182779312; accuracy of 93.9393937587738%
trainig on file: trainData_500.0mssec_P6 p6.anatomy.matrix.csv.arff
(527, 2, 600)
class 0 weight: 1.911290322580645
class 1 weight: 2.0973451327433628
Score for fold 0: loss of 0.3898167014122009; accuracy of 86.79245114326477%
(527, 2, 600)
class 0 weight: 1.911290322580645
class 1 weight: 2.0973451327433628
Score for fold 1: loss of 0.07632178068161011; accuracy of 100.0%
(527, 2, 600)
class 0 weight: 1.9190283400809718
class 1 weight: 2.088105726872247
Score for fold 2: loss of 0.24949872493743896; accuracy of 92.45283007621765%
(527, 2, 600)
class 0 weight: 1.9190283400809718
class 1 weight: 2.088105726872247
Score for fold 3: loss of 0.16116292774677277; accuracy of 98.11320900917053%
(527, 2, 600)
class 0 weight: 1.9190283400809718
class 1 weight: 2.088105726872247
Score for fold 4: loss of 0.1514338254928589; accuracy of 92.45283007621765%
(527, 2, 600)
class 0 weight: 1.9190283400809718
class 1 weight: 2.088105726872247
Score for fold 5: loss of 0.11828431487083435; accuracy of 96.22641801834106%
(527, 2, 600)
class 0 weight: 1.9190283400809718
class 1 weight: 2.088105726872247
Score for fold 6: loss of 0.04510318115353584; accuracy of 98.11320900917053%
(527, 2, 600)
class 0 weight: 1.9153225806451613
class 1 weight: 2.092511013215859
Score for fold 7: loss of 0.20479410886764526; accuracy of 88.46153616905212%
(527, 2, 600)
class 0 weight: 1.9153225806451613
class 1 weight: 2.092511013215859
Score for fold 8: loss of 0.2152947336435318; accuracy of 92.30769276618958%
(527, 2, 600)
class 0 weight: 1.9153225806451613
class 1 weight: 2.092511013215859
Score for fold 9: loss of 0.08100832998752594; accuracy of 96.15384340286255%
trainig on file: trainData_500.0mssec_P78 p78.anatomy.matrix.csv.arff
(574, 2, 600)
class 0 weight: 34.4
class 1 weight: 1.029940119760479
Score for fold 0: loss of 0.4506908357143402; accuracy of 81.03448152542114%
(574, 2, 600)
class 0 weight: 34.4
class 1 weight: 1.029940119760479
Score for fold 1: loss of 0.40471670031547546; accuracy of 81.03448152542114%
(574, 2, 600)
class 0 weight: 34.4
class 1 weight: 1.029940119760479
Score for fold 2: loss of 0.3060082495212555; accuracy of 91.37930870056152%
(574, 2, 600)
class 0 weight: 34.4
class 1 weight: 1.029940119760479
Score for fold 3: loss of 0.6020722985267639; accuracy of 79.31034564971924%
(574, 2, 600)
class 0 weight: 32.3125
class 1 weight: 1.031936127744511
Score for fold 4: loss of 0.3479350209236145; accuracy of 87.71929740905762%
(574, 2, 600)
class 0 weight: 32.3125
class 1 weight: 1.031936127744511
Score for fold 5: loss of 0.4011717736721039; accuracy of 85.9649121761322%
(574, 2, 600)
class 0 weight: 32.3125
class 1 weight: 1.031936127744511
Score for fold 6: loss of 0.40130046010017395; accuracy of 84.21052694320679%
(574, 2, 600)
class 0 weight: 34.46666666666667
class 1 weight: 1.0298804780876494
Score for fold 7: loss of 0.36689290404319763; accuracy of 80.70175647735596%
(574, 2, 600)
class 0 weight: 34.46666666666667
class 1 weight: 1.0298804780876494
Score for fold 8: loss of 0.31744250655174255; accuracy of 92.98245906829834%
(574, 2, 600)
class 0 weight: 34.46666666666667
class 1 weight: 1.0298804780876494
Score for fold 9: loss of 0.24400751292705536; accuracy of 91.22806787490845%
trainig on file: trainData_500.0mssec_P37 p37.conf.list.csv.arff
(1084, 2, 600)
class 0 weight: 1.9657258064516128
class 1 weight: 2.0354906054279747
Score for fold 0: loss of 0.7111300826072693; accuracy of 62.38532066345215%
(1084, 2, 600)
class 0 weight: 1.9657258064516128
class 1 weight: 2.0354906054279747
Score for fold 1: loss of 0.3840091824531555; accuracy of 80.73394298553467%
(1084, 2, 600)
class 0 weight: 1.9657258064516128
class 1 weight: 2.0354906054279747
Score for fold 2: loss of 0.5097813606262207; accuracy of 75.2293586730957%
(1084, 2, 600)
class 0 weight: 1.9696969696969697
class 1 weight: 2.03125
Score for fold 3: loss of 0.3996884822845459; accuracy of 79.81651425361633%
(1084, 2, 600)
class 0 weight: 1.967741935483871
class 1 weight: 2.033333333333333
Score for fold 4: loss of 0.47365134954452515; accuracy of 74.0740716457367%
(1084, 2, 600)
class 0 weight: 1.967741935483871
class 1 weight: 2.033333333333333
Score for fold 5: loss of 0.4723191559314728; accuracy of 70.37037014961243%
(1084, 2, 600)
class 0 weight: 1.967741935483871
class 1 weight: 2.033333333333333
Score for fold 6: loss of 0.39411547780036926; accuracy of 79.62962985038757%
(1084, 2, 600)
class 0 weight: 1.967741935483871
class 1 weight: 2.033333333333333
Score for fold 7: loss of 0.3696836233139038; accuracy of 78.70370149612427%
(1084, 2, 600)
class 0 weight: 1.967741935483871
class 1 weight: 2.033333333333333
Score for fold 8: loss of 0.4102271795272827; accuracy of 76.85185074806213%
(1084, 2, 600)
class 0 weight: 1.967741935483871
class 1 weight: 2.033333333333333
Score for fold 9: loss of 0.40234607458114624; accuracy of 80.55555820465088%
trainig on file: trainData_500.0mssec_P14 p14.conf.matrix.csv.arff
(1422, 2, 600)
class 0 weight: 3.8293413173652695
class 1 weight: 1.3534391534391534
Score for fold 0: loss of 0.6064809560775757; accuracy of 61.538463830947876%
(1422, 2, 600)
class 0 weight: 3.8293413173652695
class 1 weight: 1.3534391534391534
Score for fold 1: loss of 0.5144602656364441; accuracy of 66.43356680870056%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 2: loss of 0.48479557037353516; accuracy of 64.08450603485107%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 3: loss of 0.5587700009346008; accuracy of 60.563379526138306%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 4: loss of 0.5108476877212524; accuracy of 65.49295783042908%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 5: loss of 0.4990389049053192; accuracy of 66.90140962600708%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 6: loss of 0.472229927778244; accuracy of 68.30986142158508%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 7: loss of 0.41865354776382446; accuracy of 74.64788556098938%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 8: loss of 0.5321142673492432; accuracy of 66.19718074798584%
(1422, 2, 600)
class 0 weight: 3.8208955223880596
class 1 weight: 1.3544973544973546
Score for fold 9: loss of 0.7244256138801575; accuracy of 63.38028311729431%
trainig on file: trainData_500.0mssec_P51 p51.conf.list.csv.arff
(565, 2, 600)
class 0 weight: 1.727891156462585
class 1 weight: 2.3738317757009346
Score for fold 0: loss of 0.690797746181488; accuracy of 63.15789222717285%
(565, 2, 600)
class 0 weight: 1.727891156462585
class 1 weight: 2.3738317757009346
Score for fold 1: loss of 0.6611930727958679; accuracy of 77.19298005104065%
(565, 2, 600)
class 0 weight: 1.727891156462585
class 1 weight: 2.3738317757009346
Score for fold 2: loss of 0.5955673456192017; accuracy of 73.68420958518982%
(565, 2, 600)
class 0 weight: 1.727891156462585
class 1 weight: 2.3738317757009346
Score for fold 3: loss of 0.31646808981895447; accuracy of 89.47368264198303%
(565, 2, 600)
class 0 weight: 1.727891156462585
class 1 weight: 2.3738317757009346
Score for fold 4: loss of 0.3070167303085327; accuracy of 87.71929740905762%
(565, 2, 600)
class 0 weight: 1.7254237288135592
class 1 weight: 2.378504672897196
Score for fold 5: loss of 0.22249667346477509; accuracy of 92.85714030265808%
(565, 2, 600)
class 0 weight: 1.7254237288135592
class 1 weight: 2.378504672897196
Score for fold 6: loss of 0.26021090149879456; accuracy of 89.28571343421936%
(565, 2, 600)
class 0 weight: 1.7254237288135592
class 1 weight: 2.378504672897196
Score for fold 7: loss of 0.3360937535762787; accuracy of 87.5%
(565, 2, 600)
class 0 weight: 1.7312925170068028
class 1 weight: 2.3674418604651164
Score for fold 8: loss of 0.4214833676815033; accuracy of 82.14285969734192%
(565, 2, 600)
class 0 weight: 1.7312925170068028
class 1 weight: 2.3674418604651164
Score for fold 9: loss of 0.384467750787735; accuracy of 82.14285969734192%
trainig on file: trainData_500.0mssec_P76 p76.anatomy.matrix.csv.arff
(710, 2, 600)
class 0 weight: 3.944444444444444
class 1 weight: 1.339622641509434
Score for fold 0: loss of 0.2577926516532898; accuracy of 90.14084339141846%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 1: loss of 0.1733652502298355; accuracy of 94.36619877815247%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 2: loss of 0.19042612612247467; accuracy of 91.54929518699646%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 3: loss of 0.10190438479185104; accuracy of 97.183096408844%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 4: loss of 0.2232048213481903; accuracy of 88.73239159584045%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 5: loss of 0.10016290098428726; accuracy of 94.36619877815247%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 6: loss of 0.130613774061203; accuracy of 95.77465057373047%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 7: loss of 0.09891606867313385; accuracy of 95.77465057373047%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 8: loss of 0.06956566125154495; accuracy of 98.591548204422%
(710, 2, 600)
class 0 weight: 3.9689440993788816
class 1 weight: 1.3368200836820083
Score for fold 9: loss of 0.17906831204891205; accuracy of 91.54929518699646%
trainig on file: trainData_500.0mssec_P36 p36.conf.matrix.csv.arff
(915, 2, 600)
class 0 weight: 1.7004132231404958
class 1 weight: 2.4277286135693217
Score for fold 0: loss of 0.8050273656845093; accuracy of 61.95651888847351%
(915, 2, 600)
class 0 weight: 1.7004132231404958
class 1 weight: 2.4277286135693217
Score for fold 1: loss of 0.7168755531311035; accuracy of 52.173912525177%
(915, 2, 600)
class 0 weight: 1.7004132231404958
class 1 weight: 2.4277286135693217
Score for fold 2: loss of 0.6817131638526917; accuracy of 64.13043737411499%
(915, 2, 600)
class 0 weight: 1.7004132231404958
class 1 weight: 2.4277286135693217
Score for fold 3: loss of 0.5400609374046326; accuracy of 78.2608687877655%
(915, 2, 600)
class 0 weight: 1.7004132231404958
class 1 weight: 2.4277286135693217
Score for fold 4: loss of 0.4201648533344269; accuracy of 77.173912525177%
(915, 2, 600)
class 0 weight: 1.7024793388429753
class 1 weight: 2.4235294117647057
Score for fold 5: loss of 0.5114787817001343; accuracy of 70.32967209815979%
(915, 2, 600)
class 0 weight: 1.7024793388429753
class 1 weight: 2.4235294117647057
Score for fold 6: loss of 0.417654424905777; accuracy of 76.92307829856873%
(915, 2, 600)
class 0 weight: 1.7024793388429753
class 1 weight: 2.4235294117647057
Score for fold 7: loss of 0.4646933972835541; accuracy of 78.02197933197021%
(915, 2, 600)
class 0 weight: 1.6989690721649484
class 1 weight: 2.4306784660766962
Score for fold 8: loss of 0.5335473418235779; accuracy of 67.03296899795532%
(915, 2, 600)
class 0 weight: 1.6989690721649484
class 1 weight: 2.4306784660766962
Score for fold 9: loss of 0.5029552578926086; accuracy of 68.13187003135681%
trainig on file: trainData_500.0mssec_P39 p39.conf.list.csv.arff
(1362, 2, 600)
class 0 weight: 2.759009009009009
class 1 weight: 1.5685019206145967
Score for fold 0: loss of 0.6466972231864929; accuracy of 71.53284549713135%
(1362, 2, 600)
class 0 weight: 2.759009009009009
class 1 weight: 1.5685019206145967
Score for fold 1: loss of 0.6662898659706116; accuracy of 67.15328693389893%
(1362, 2, 600)
class 0 weight: 2.755056179775281
class 1 weight: 1.5697823303457106
Score for fold 2: loss of 0.5858142971992493; accuracy of 73.52941036224365%
(1362, 2, 600)
class 0 weight: 2.755056179775281
class 1 weight: 1.5697823303457106
Score for fold 3: loss of 0.5666922330856323; accuracy of 65.4411792755127%
(1362, 2, 600)
class 0 weight: 2.755056179775281
class 1 weight: 1.5697823303457106
Score for fold 4: loss of 0.6408712267875671; accuracy of 64.70588445663452%
(1362, 2, 600)
class 0 weight: 2.755056179775281
class 1 weight: 1.5697823303457106
Score for fold 5: loss of 0.501892626285553; accuracy of 78.67646813392639%
(1362, 2, 600)
class 0 weight: 2.755056179775281
class 1 weight: 1.5697823303457106
Score for fold 6: loss of 0.5478387475013733; accuracy of 72.0588207244873%
(1362, 2, 600)
class 0 weight: 2.755056179775281
class 1 weight: 1.5697823303457106
Score for fold 7: loss of 0.5805208086967468; accuracy of 71.32353186607361%
(1362, 2, 600)
class 0 weight: 2.7612612612612613
class 1 weight: 1.5677749360613813
Score for fold 8: loss of 0.5482107400894165; accuracy of 72.79411554336548%
(1362, 2, 600)
class 0 weight: 2.7612612612612613
class 1 weight: 1.5677749360613813
Score for fold 9: loss of 0.5897651314735413; accuracy of 75.0%
trainig on file: trainData_500.0mssec_P81 p81.anatomy.matrix.csv.arff
(617, 2, 600)
class 0 weight: 9.25
class 1 weight: 1.121212121212121
Score for fold 0: loss of 0.14601610600948334; accuracy of 96.77419066429138%
(617, 2, 600)
class 0 weight: 9.40677966101695
class 1 weight: 1.1189516129032258
Score for fold 1: loss of 0.2879045605659485; accuracy of 85.48387289047241%
(617, 2, 600)
class 0 weight: 9.40677966101695
class 1 weight: 1.1189516129032258
Score for fold 2: loss of 0.22456882894039154; accuracy of 91.93548560142517%
(617, 2, 600)
class 0 weight: 9.40677966101695
class 1 weight: 1.1189516129032258
Score for fold 3: loss of 0.2768292725086212; accuracy of 87.09677457809448%
(617, 2, 600)
class 0 weight: 9.40677966101695
class 1 weight: 1.1189516129032258
Score for fold 4: loss of 0.36050811409950256; accuracy of 82.2580635547638%
(617, 2, 600)
class 0 weight: 9.40677966101695
class 1 weight: 1.1189516129032258
Score for fold 5: loss of 0.5094696879386902; accuracy of 74.19354915618896%
(617, 2, 600)
class 0 weight: 9.40677966101695
class 1 weight: 1.1189516129032258
Score for fold 6: loss of 0.20204824209213257; accuracy of 91.93548560142517%
(617, 2, 600)
class 0 weight: 9.266666666666666
class 1 weight: 1.1209677419354838
Score for fold 7: loss of 0.28323987126350403; accuracy of 88.52459192276001%
(617, 2, 600)
class 0 weight: 9.266666666666666
class 1 weight: 1.1209677419354838
Score for fold 8: loss of 0.26506438851356506; accuracy of 88.52459192276001%
(617, 2, 600)
class 0 weight: 9.266666666666666
class 1 weight: 1.1209677419354838
Score for fold 9: loss of 0.48082876205444336; accuracy of 78.68852615356445%
trainig on file: trainData_500.0mssec_P35 p35.anatomy.list.csv.arff
(593, 2, 600)
class 0 weight: 1.8506944444444444
class 1 weight: 2.175510204081633
Score for fold 0: loss of 0.44020408391952515; accuracy of 73.33333492279053%
(593, 2, 600)
class 0 weight: 1.8506944444444444
class 1 weight: 2.175510204081633
Score for fold 1: loss of 0.43478038907051086; accuracy of 73.33333492279053%
(593, 2, 600)
class 0 weight: 1.8506944444444444
class 1 weight: 2.175510204081633
Score for fold 2: loss of 0.3664827048778534; accuracy of 78.33333611488342%
(593, 2, 600)
class 0 weight: 1.8541666666666665
class 1 weight: 2.1707317073170733
Score for fold 3: loss of 0.4345588684082031; accuracy of 74.57627058029175%
(593, 2, 600)
class 0 weight: 1.8541666666666665
class 1 weight: 2.1707317073170733
Score for fold 4: loss of 0.29492470622062683; accuracy of 84.74576473236084%
(593, 2, 600)
class 0 weight: 1.8541666666666665
class 1 weight: 2.1707317073170733
Score for fold 5: loss of 0.24887758493423462; accuracy of 88.13559412956238%
(593, 2, 600)
class 0 weight: 1.8541666666666665
class 1 weight: 2.1707317073170733
Score for fold 6: loss of 0.286334753036499; accuracy of 86.44067645072937%
(593, 2, 600)
class 0 weight: 1.8541666666666665
class 1 weight: 2.1707317073170733
Score for fold 7: loss of 0.38981497287750244; accuracy of 81.35592937469482%
(593, 2, 600)
class 0 weight: 1.8541666666666665
class 1 weight: 2.1707317073170733
Score for fold 8: loss of 0.333835244178772; accuracy of 79.6610176563263%
(593, 2, 600)
class 0 weight: 1.8541666666666665
class 1 weight: 2.1707317073170733
Score for fold 9: loss of 0.3763246238231659; accuracy of 76.27118825912476%
trainig on file: trainData_500.0mssec_P17 p17.anatomy.list.csv.arff
(385, 2, 600)
class 0 weight: 2.1490683229813663
class 1 weight: 1.8702702702702703
Score for fold 0: loss of 0.37489914894104004; accuracy of 84.61538553237915%
(385, 2, 600)
class 0 weight: 2.1490683229813663
class 1 weight: 1.8702702702702703
Score for fold 1: loss of 0.2962920367717743; accuracy of 87.17948794364929%
(385, 2, 600)
class 0 weight: 2.1490683229813663
class 1 weight: 1.8702702702702703
Score for fold 2: loss of 0.2584133744239807; accuracy of 84.61538553237915%
(385, 2, 600)
class 0 weight: 2.1490683229813663
class 1 weight: 1.8702702702702703
Score for fold 3: loss of 0.19398316740989685; accuracy of 87.17948794364929%
(385, 2, 600)
class 0 weight: 2.1490683229813663
class 1 weight: 1.8702702702702703
Score for fold 4: loss of 0.3389720320701599; accuracy of 84.61538553237915%
(385, 2, 600)
class 0 weight: 2.141975308641975
class 1 weight: 1.8756756756756758
Score for fold 5: loss of 0.20981068909168243; accuracy of 92.1052634716034%
(385, 2, 600)
class 0 weight: 2.15527950310559
class 1 weight: 1.8655913978494625
Score for fold 6: loss of 0.28838223218917847; accuracy of 92.1052634716034%
(385, 2, 600)
class 0 weight: 2.15527950310559
class 1 weight: 1.8655913978494625
Score for fold 7: loss of 0.2526535093784332; accuracy of 92.1052634716034%
(385, 2, 600)
class 0 weight: 2.15527950310559
class 1 weight: 1.8655913978494625
Score for fold 8: loss of 0.32038506865501404; accuracy of 86.84210777282715%
(385, 2, 600)
class 0 weight: 2.15527950310559
class 1 weight: 1.8655913978494625
Score for fold 9: loss of 0.08183679729700089; accuracy of 97.36841917037964%
trainig on file: trainData_500.0mssec_P7 p7.anatomy.list.csv.arff
(698, 2, 600)
class 0 weight: 6.9010989010989015
class 1 weight: 1.1694599627560522
Score for fold 0: loss of 0.4074268639087677; accuracy of 78.57142686843872%
(698, 2, 600)
class 0 weight: 6.9010989010989015
class 1 weight: 1.1694599627560522
Score for fold 1: loss of 0.3559667766094208; accuracy of 82.85714387893677%
(698, 2, 600)
class 0 weight: 6.9010989010989015
class 1 weight: 1.1694599627560522
Score for fold 2: loss of 0.3259218633174896; accuracy of 82.85714387893677%
(698, 2, 600)
class 0 weight: 6.9010989010989015
class 1 weight: 1.1694599627560522
Score for fold 3: loss of 0.3618222773075104; accuracy of 81.42856955528259%
(698, 2, 600)
class 0 weight: 6.9010989010989015
class 1 weight: 1.1694599627560522
Score for fold 4: loss of 0.28793588280677795; accuracy of 84.28571224212646%
(698, 2, 600)
class 0 weight: 6.9010989010989015
class 1 weight: 1.1694599627560522
Score for fold 5: loss of 0.4932788908481598; accuracy of 72.85714149475098%
(698, 2, 600)
class 0 weight: 6.9010989010989015
class 1 weight: 1.1694599627560522
Score for fold 6: loss of 0.24979335069656372; accuracy of 85.71428656578064%
(698, 2, 600)
class 0 weight: 6.977777777777778
class 1 weight: 1.1672862453531598
Score for fold 7: loss of 0.31954097747802734; accuracy of 82.85714387893677%
(698, 2, 600)
class 0 weight: 6.912087912087912
class 1 weight: 1.1691449814126393
Score for fold 8: loss of 0.22463373839855194; accuracy of 88.40579986572266%
(698, 2, 600)
class 0 weight: 6.912087912087912
class 1 weight: 1.1691449814126393
Score for fold 9: loss of 0.31603360176086426; accuracy of 88.40579986572266%
trainig on file: trainData_500.0mssec_P54 p54.conf.matrix.csv.arff
(1067, 2, 600)
class 0 weight: 1.350210970464135
class 1 weight: 3.8554216867469875
Score for fold 0: loss of 0.5758569240570068; accuracy of 77.57009267807007%
(1067, 2, 600)
class 0 weight: 1.350210970464135
class 1 weight: 3.8554216867469875
Score for fold 1: loss of 0.5443271994590759; accuracy of 72.89719581604004%
(1067, 2, 600)
class 0 weight: 1.350210970464135
class 1 weight: 3.8554216867469875
Score for fold 2: loss of 0.40951526165008545; accuracy of 77.57009267807007%
(1067, 2, 600)
class 0 weight: 1.350210970464135
class 1 weight: 3.8554216867469875
Score for fold 3: loss of 0.4663073718547821; accuracy of 79.43925261497498%
(1067, 2, 600)
class 0 weight: 1.350210970464135
class 1 weight: 3.8554216867469875
Score for fold 4: loss of 0.5218939185142517; accuracy of 71.96261882781982%
(1067, 2, 600)
class 0 weight: 1.350210970464135
class 1 weight: 3.8554216867469875
Score for fold 5: loss of 0.4901435673236847; accuracy of 72.89719581604004%
(1067, 2, 600)
class 0 weight: 1.350210970464135
class 1 weight: 3.8554216867469875
Score for fold 6: loss of 0.39932990074157715; accuracy of 83.17757248878479%
(1067, 2, 600)
class 0 weight: 1.3516174402250352
class 1 weight: 3.844
Score for fold 7: loss of 0.4235346019268036; accuracy of 74.52830076217651%
(1067, 2, 600)
class 0 weight: 1.3516174402250352
class 1 weight: 3.844
Score for fold 8: loss of 0.5017836689949036; accuracy of 71.69811129570007%
(1067, 2, 600)
class 0 weight: 1.3516174402250352
class 1 weight: 3.844
Score for fold 9: loss of 0.43745696544647217; accuracy of 77.35849022865295%
trainig on file: trainData_500.0mssec_P22 p22.anatomy.matrix.csv.arff
(605, 2, 600)
class 0 weight: 5.787234042553191
class 1 weight: 1.208888888888889
Score for fold 0: loss of 0.21920940279960632; accuracy of 88.52459192276001%
(605, 2, 600)
class 0 weight: 5.787234042553191
class 1 weight: 1.208888888888889
Score for fold 1: loss of 0.23992489278316498; accuracy of 93.44262480735779%
(605, 2, 600)
class 0 weight: 5.787234042553191
class 1 weight: 1.208888888888889
Score for fold 2: loss of 0.23796884715557098; accuracy of 90.16393423080444%
(605, 2, 600)
class 0 weight: 5.787234042553191
class 1 weight: 1.208888888888889
Score for fold 3: loss of 0.2294398546218872; accuracy of 91.80327653884888%
(605, 2, 600)
class 0 weight: 5.787234042553191
class 1 weight: 1.208888888888889
Score for fold 4: loss of 0.2998174726963043; accuracy of 93.44262480735779%
(605, 2, 600)
class 0 weight: 5.7368421052631575
class 1 weight: 1.211111111111111
Score for fold 5: loss of 0.1290663778781891; accuracy of 96.66666388511658%
(605, 2, 600)
class 0 weight: 5.7368421052631575
class 1 weight: 1.211111111111111
Score for fold 6: loss of 0.16466136276721954; accuracy of 94.9999988079071%
(605, 2, 600)
class 0 weight: 5.7368421052631575
class 1 weight: 1.211111111111111
Score for fold 7: loss of 0.11800907552242279; accuracy of 96.66666388511658%
(605, 2, 600)
class 0 weight: 5.7368421052631575
class 1 weight: 1.211111111111111
Score for fold 8: loss of 0.16637298464775085; accuracy of 96.66666388511658%
(605, 2, 600)
class 0 weight: 5.7368421052631575
class 1 weight: 1.211111111111111
Score for fold 9: loss of 0.15613150596618652; accuracy of 91.66666865348816%
trainig on file: trainData_500.0mssec_P49 p49.conf.list.csv.arff
(727, 2, 600)
class 0 weight: 1.8319327731092436
class 1 weight: 2.202020202020202
Score for fold 0: loss of 0.8479357361793518; accuracy of 61.64383292198181%
(727, 2, 600)
class 0 weight: 1.8319327731092436
class 1 weight: 2.202020202020202
Score for fold 1: loss of 0.6636388301849365; accuracy of 65.75342416763306%
(727, 2, 600)
class 0 weight: 1.8319327731092436
class 1 weight: 2.202020202020202
Score for fold 2: loss of 0.5753443241119385; accuracy of 68.49315166473389%
(727, 2, 600)
class 0 weight: 1.8319327731092436
class 1 weight: 2.202020202020202
Score for fold 3: loss of 0.4888407588005066; accuracy of 73.97260069847107%
(727, 2, 600)
class 0 weight: 1.8319327731092436
class 1 weight: 2.202020202020202
Score for fold 4: loss of 0.5304099917411804; accuracy of 68.49315166473389%
(727, 2, 600)
class 0 weight: 1.8319327731092436
class 1 weight: 2.202020202020202
Score for fold 5: loss of 0.49039319157600403; accuracy of 76.7123281955719%
(727, 2, 600)
class 0 weight: 1.8319327731092436
class 1 weight: 2.202020202020202
Score for fold 6: loss of 0.5987839102745056; accuracy of 69.8630154132843%
(727, 2, 600)
class 0 weight: 1.829608938547486
class 1 weight: 2.2053872053872055
Score for fold 7: loss of 0.5607417821884155; accuracy of 72.22222089767456%
(727, 2, 600)
class 0 weight: 1.829608938547486
class 1 weight: 2.2053872053872055
Score for fold 8: loss of 0.5367052555084229; accuracy of 70.83333134651184%
(727, 2, 600)
class 0 weight: 1.829608938547486
class 1 weight: 2.2053872053872055
Score for fold 9: loss of 0.48874446749687195; accuracy of 75.0%
trainig on file: trainData_500.0mssec_P14 p14.anatomy.matrix.csv.arff
(632, 2, 600)
class 0 weight: 5.071428571428571
class 1 weight: 1.2456140350877192
Score for fold 0: loss of 0.6895953416824341; accuracy of 70.3125%
(632, 2, 600)
class 0 weight: 5.071428571428571
class 1 weight: 1.2456140350877192
Score for fold 1: loss of 0.3384295701980591; accuracy of 84.375%
(632, 2, 600)
class 0 weight: 5.035398230088496
class 1 weight: 1.2478070175438596
Score for fold 2: loss of 0.46450483798980713; accuracy of 77.77777910232544%
(632, 2, 600)
class 0 weight: 5.035398230088496
class 1 weight: 1.2478070175438596
Score for fold 3: loss of 0.5245822668075562; accuracy of 73.01587462425232%
(632, 2, 600)
class 0 weight: 5.035398230088496
class 1 weight: 1.2478070175438596
Score for fold 4: loss of 0.45339235663414; accuracy of 74.60317611694336%
(632, 2, 600)
class 0 weight: 5.035398230088496
class 1 weight: 1.2478070175438596
Score for fold 5: loss of 0.5259703397750854; accuracy of 66.66666865348816%
(632, 2, 600)
class 0 weight: 5.035398230088496
class 1 weight: 1.2478070175438596
Score for fold 6: loss of 0.5530935525894165; accuracy of 69.84127163887024%
(632, 2, 600)
class 0 weight: 5.080357142857142
class 1 weight: 1.2450765864332602
Score for fold 7: loss of 0.30835652351379395; accuracy of 85.71428656578064%
(632, 2, 600)
class 0 weight: 5.080357142857142
class 1 weight: 1.2450765864332602
Score for fold 8: loss of 0.3752656877040863; accuracy of 80.95238208770752%
(632, 2, 600)
class 0 weight: 5.080357142857142
class 1 weight: 1.2450765864332602
Score for fold 9: loss of 0.5203567147254944; accuracy of 73.01587462425232%
trainig on file: trainData_500.0mssec_P4 p4.anatomy.matrix.csv.arff
(415, 2, 600)
class 0 weight: 0
class 1 weight: 1.0
Score for fold 0: loss of 0.002659055171534419; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 1.0
Score for fold 1: loss of 0.002134751295670867; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 1.0
Score for fold 2: loss of 0.00039740794454701245; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 1.0
Score for fold 3: loss of 0.00019116370822302997; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 1.0
Score for fold 4: loss of 0.0003299820236861706; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 0.9999999999999999
Score for fold 5: loss of 0.0006511512328870595; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 0.9999999999999999
Score for fold 6: loss of 0.00012972445983905345; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 0.9999999999999999
Score for fold 7: loss of 0.0002047653979388997; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 0.9999999999999999
Score for fold 8: loss of 0.000660044839605689; accuracy of 100.0%
(415, 2, 600)
class 0 weight: 0
class 1 weight: 0.9999999999999999
Score for fold 9: loss of 5.387705459725112e-05; accuracy of 100.0%
trainig on file: trainData_500.0mssec_P76 p76.conf.matrix.csv.arff
(1196, 2, 600)
class 0 weight: 3.0309859154929577
class 1 weight: 1.492371705963939
Score for fold 0: loss of 0.39898625016212463; accuracy of 83.33333134651184%
(1196, 2, 600)
class 0 weight: 3.0309859154929577
class 1 weight: 1.492371705963939
Score for fold 1: loss of 0.4577520191669464; accuracy of 78.33333611488342%
(1196, 2, 600)
class 0 weight: 3.0395480225988702
class 1 weight: 1.4903047091412742
Score for fold 2: loss of 0.3644702136516571; accuracy of 85.00000238418579%
(1196, 2, 600)
class 0 weight: 3.0395480225988702
class 1 weight: 1.4903047091412742
Score for fold 3: loss of 0.28841859102249146; accuracy of 84.16666388511658%
(1196, 2, 600)
class 0 weight: 3.0395480225988702
class 1 weight: 1.4903047091412742
Score for fold 4: loss of 0.3056483268737793; accuracy of 87.5%
(1196, 2, 600)
class 0 weight: 3.0395480225988702
class 1 weight: 1.4903047091412742
Score for fold 5: loss of 0.3578230142593384; accuracy of 86.66666746139526%
(1196, 2, 600)
class 0 weight: 3.0338028169014084
class 1 weight: 1.4916897506925209
Score for fold 6: loss of 0.4033597707748413; accuracy of 84.03361439704895%
(1196, 2, 600)
class 0 weight: 3.0338028169014084
class 1 weight: 1.4916897506925209
Score for fold 7: loss of 0.3880990147590637; accuracy of 80.67227005958557%
(1196, 2, 600)
class 0 weight: 3.0338028169014084
class 1 weight: 1.4916897506925209
Score for fold 8: loss of 0.3028431236743927; accuracy of 82.35294222831726%
(1196, 2, 600)
class 0 weight: 3.0338028169014084
class 1 weight: 1.4916897506925209
Score for fold 9: loss of 0.3270983099937439; accuracy of 78.15126180648804%
trainig on file: trainData_500.0mssec_P79 p79.conf.list.csv.arff
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 0: loss of 0.6328729391098022; accuracy of 69.2307710647583%
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 1: loss of 0.37003904581069946; accuracy of 75.96153616905212%
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 2: loss of 0.45863527059555054; accuracy of 76.92307829856873%
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 3: loss of 0.38867059350013733; accuracy of 76.92307829856873%
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 4: loss of 0.32648834586143494; accuracy of 82.69230723381042%
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 5: loss of 0.37584176659584045; accuracy of 79.80769276618958%
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 6: loss of 0.45215779542922974; accuracy of 74.03846383094788%
(1040, 2, 600)
class 0 weight: 6.038709677419355
class 1 weight: 1.1984635083226631
Score for fold 7: loss of 0.3652202785015106; accuracy of 80.7692289352417%
(1040, 2, 600)
class 0 weight: 6.077922077922079
class 1 weight: 1.1969309462915603
Score for fold 8: loss of 0.6123844385147095; accuracy of 73.07692170143127%
(1040, 2, 600)
class 0 weight: 6.077922077922079
class 1 weight: 1.1969309462915603
Score for fold 9: loss of 0.5124803185462952; accuracy of 75.96153616905212%
trainig on file: trainData_500.0mssec_P58 p58.anatomy.matrix.csv.arff
(577, 2, 600)
class 0 weight: 1.6424050632911391
class 1 weight: 2.5566502463054186
Score for fold 0: loss of 0.42770087718963623; accuracy of 84.48275923728943%
(577, 2, 600)
class 0 weight: 1.6424050632911391
class 1 weight: 2.5566502463054186
Score for fold 1: loss of 0.3380671441555023; accuracy of 81.03448152542114%
(577, 2, 600)
class 0 weight: 1.637223974763407
class 1 weight: 2.5693069306930694
Score for fold 2: loss of 0.32776927947998047; accuracy of 82.75862336158752%
(577, 2, 600)
class 0 weight: 1.637223974763407
class 1 weight: 2.5693069306930694
Score for fold 3: loss of 0.24478474259376526; accuracy of 84.48275923728943%
(577, 2, 600)
class 0 weight: 1.637223974763407
class 1 weight: 2.5693069306930694
Score for fold 4: loss of 0.15428802371025085; accuracy of 89.65517282485962%
(577, 2, 600)
class 0 weight: 1.637223974763407
class 1 weight: 2.5693069306930694
Score for fold 5: loss of 0.3048500716686249; accuracy of 82.75862336158752%
(577, 2, 600)
class 0 weight: 1.637223974763407
class 1 weight: 2.5693069306930694
Score for fold 6: loss of 0.309439092874527; accuracy of 82.75862336158752%
(577, 2, 600)
class 0 weight: 1.640378548895899
class 1 weight: 2.561576354679803
Score for fold 7: loss of 0.2526831030845642; accuracy of 84.21052694320679%
(577, 2, 600)
class 0 weight: 1.640378548895899
class 1 weight: 2.561576354679803
Score for fold 8: loss of 0.25173261761665344; accuracy of 82.45614171028137%
(577, 2, 600)
class 0 weight: 1.640378548895899
class 1 weight: 2.561576354679803
Score for fold 9: loss of 0.15998853743076324; accuracy of 89.47368264198303%
trainig on file: trainData_500.0mssec_P10 p10.conf.matrix.csv.arff
(1631, 2, 600)
class 0 weight: 1.5911062906724514
class 1 weight: 2.691743119266055
Score for fold 0: loss of 0.4572940468788147; accuracy of 73.78048896789551%
(1631, 2, 600)
class 0 weight: 1.592190889370933
class 1 weight: 2.688644688644689
Score for fold 1: loss of 0.44544869661331177; accuracy of 73.61963391304016%
(1631, 2, 600)
class 0 weight: 1.592190889370933
class 1 weight: 2.688644688644689
Score for fold 2: loss of 0.42789289355278015; accuracy of 80.98159432411194%
(1631, 2, 600)
class 0 weight: 1.592190889370933
class 1 weight: 2.688644688644689
Score for fold 3: loss of 0.44322845339775085; accuracy of 80.3680956363678%
(1631, 2, 600)
class 0 weight: 1.592190889370933
class 1 weight: 2.688644688644689
Score for fold 4: loss of 0.4581983685493469; accuracy of 76.68711543083191%
(1631, 2, 600)
class 0 weight: 1.590465872156013
class 1 weight: 2.6935779816513765
Score for fold 5: loss of 0.44576749205589294; accuracy of 78.52760553359985%
(1631, 2, 600)
class 0 weight: 1.590465872156013
class 1 weight: 2.6935779816513765
Score for fold 6: loss of 0.4196626543998718; accuracy of 79.75460290908813%
(1631, 2, 600)
class 0 weight: 1.590465872156013
class 1 weight: 2.6935779816513765
Score for fold 7: loss of 0.4547630250453949; accuracy of 81.59509301185608%
(1631, 2, 600)
class 0 weight: 1.590465872156013
class 1 weight: 2.6935779816513765
Score for fold 8: loss of 0.3955763876438141; accuracy of 80.3680956363678%
(1631, 2, 600)
class 0 weight: 1.590465872156013
class 1 weight: 2.6935779816513765
Score for fold 9: loss of 0.48062172532081604; accuracy of 76.68711543083191%
trainig on file: trainData_500.0mssec_P35 p35.conf.list.csv.arff
(1220, 2, 600)
class 0 weight: 2.495454545454545
class 1 weight: 1.668693009118541
Score for fold 0: loss of 0.597015380859375; accuracy of 71.31147384643555%
(1220, 2, 600)
class 0 weight: 2.495454545454545
class 1 weight: 1.668693009118541
Score for fold 1: loss of 0.6113835573196411; accuracy of 63.1147563457489%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 2: loss of 0.4723473787307739; accuracy of 77.86885499954224%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 3: loss of 0.5680233240127563; accuracy of 68.03278923034668%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 4: loss of 0.5443409085273743; accuracy of 73.77049326896667%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 5: loss of 0.6259510517120361; accuracy of 63.93442749977112%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 6: loss of 0.5263993144035339; accuracy of 68.8524603843689%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 7: loss of 0.5431807041168213; accuracy of 72.95082211494446%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 8: loss of 0.5235569477081299; accuracy of 73.77049326896667%
(1220, 2, 600)
class 0 weight: 2.501138952164009
class 1 weight: 1.6661608497723825
Score for fold 9: loss of 0.5145779848098755; accuracy of 74.59016442298889%
trainig on file: trainData_500.0mssec_P66 p66.anatomy.matrix.csv.arff
(471, 2, 600)
class 0 weight: 2.579268292682927
class 1 weight: 1.6332046332046333
Score for fold 0: loss of 0.8574299812316895; accuracy of 64.58333134651184%
(471, 2, 600)
class 0 weight: 2.56969696969697
class 1 weight: 1.6370656370656371
Score for fold 1: loss of 0.23031234741210938; accuracy of 89.3617033958435%
(471, 2, 600)
class 0 weight: 2.56969696969697
class 1 weight: 1.6370656370656371
Score for fold 2: loss of 0.18432684242725372; accuracy of 95.7446813583374%
(471, 2, 600)
class 0 weight: 2.56969696969697
class 1 weight: 1.6370656370656371
Score for fold 3: loss of 0.22019004821777344; accuracy of 89.3617033958435%
(471, 2, 600)
class 0 weight: 2.56969696969697
class 1 weight: 1.6370656370656371
Score for fold 4: loss of 0.39585307240486145; accuracy of 87.2340440750122%
(471, 2, 600)
class 0 weight: 2.56969696969697
class 1 weight: 1.6370656370656371
Score for fold 5: loss of 0.4100659191608429; accuracy of 82.97872543334961%
(471, 2, 600)
class 0 weight: 2.56969696969697
class 1 weight: 1.6370656370656371
Score for fold 6: loss of 0.44419771432876587; accuracy of 89.3617033958435%
(471, 2, 600)
class 0 weight: 2.56969696969697
class 1 weight: 1.6370656370656371
Score for fold 7: loss of 0.13735683262348175; accuracy of 93.6170220375061%
(471, 2, 600)
class 0 weight: 2.5853658536585367
class 1 weight: 1.6307692307692307
Score for fold 8: loss of 0.20811036229133606; accuracy of 93.6170220375061%
(471, 2, 600)
class 0 weight: 2.5853658536585367
class 1 weight: 1.6307692307692307
Score for fold 9: loss of 0.18936222791671753; accuracy of 91.4893627166748%
trainig on file: trainData_500.0mssec_P47 p47.anatomy.list.csv.arff
(474, 2, 600)
class 0 weight: 6.5538461538461545
class 1 weight: 1.18005540166205
Score for fold 0: loss of 0.4344084560871124; accuracy of 81.25%
(474, 2, 600)
class 0 weight: 6.5538461538461545
class 1 weight: 1.18005540166205
Score for fold 1: loss of 0.3204055428504944; accuracy of 83.33333134651184%
(474, 2, 600)
class 0 weight: 6.65625
class 1 weight: 1.1767955801104972
Score for fold 2: loss of 0.308106392621994; accuracy of 83.33333134651184%
(474, 2, 600)
class 0 weight: 6.65625
class 1 weight: 1.1767955801104972
Score for fold 3: loss of 0.2916048765182495; accuracy of 85.41666865348816%
(474, 2, 600)
class 0 weight: 6.56923076923077
class 1 weight: 1.1795580110497237
Score for fold 4: loss of 0.2609560489654541; accuracy of 91.4893627166748%
(474, 2, 600)
class 0 weight: 6.56923076923077
class 1 weight: 1.1795580110497237
Score for fold 5: loss of 0.5040655136108398; accuracy of 74.46808218955994%
(474, 2, 600)
class 0 weight: 6.56923076923077
class 1 weight: 1.1795580110497237
Score for fold 6: loss of 0.34015756845474243; accuracy of 82.97872543334961%
(474, 2, 600)
class 0 weight: 6.56923076923077
class 1 weight: 1.1795580110497237
Score for fold 7: loss of 0.2650609612464905; accuracy of 89.3617033958435%
(474, 2, 600)
class 0 weight: 6.56923076923077
class 1 weight: 1.1795580110497237
Score for fold 8: loss of 0.28683581948280334; accuracy of 85.10638475418091%
(474, 2, 600)
class 0 weight: 6.56923076923077
class 1 weight: 1.1795580110497237
Score for fold 9: loss of 0.17008601129055023; accuracy of 91.4893627166748%
trainig on file: trainData_500.0mssec_P19 p19.anatomy.list.csv.arff
(742, 2, 600)
class 0 weight: 1.938953488372093
class 1 weight: 2.0650154798761613
Score for fold 0: loss of 0.7301536798477173; accuracy of 65.3333306312561%
(742, 2, 600)
class 0 weight: 1.938953488372093
class 1 weight: 2.0650154798761613
Score for fold 1: loss of 0.37006545066833496; accuracy of 80.0000011920929%
(742, 2, 600)
class 0 weight: 1.941860465116279
class 1 weight: 2.0617283950617282
Score for fold 2: loss of 0.49222612380981445; accuracy of 72.972971200943%
(742, 2, 600)
class 0 weight: 1.936231884057971
class 1 weight: 2.0681114551083595
Score for fold 3: loss of 0.43603453040122986; accuracy of 82.43243098258972%
(742, 2, 600)
class 0 weight: 1.936231884057971
class 1 weight: 2.0681114551083595
Score for fold 4: loss of 0.45820096135139465; accuracy of 70.27027010917664%
(742, 2, 600)
class 0 weight: 1.936231884057971
class 1 weight: 2.0681114551083595
Score for fold 5: loss of 0.35882723331451416; accuracy of 85.13513803482056%
(742, 2, 600)
class 0 weight: 1.936231884057971
class 1 weight: 2.0681114551083595
Score for fold 6: loss of 0.43956291675567627; accuracy of 77.027028799057%
(742, 2, 600)
class 0 weight: 1.936231884057971
class 1 weight: 2.0681114551083595
Score for fold 7: loss of 0.42162296175956726; accuracy of 77.027028799057%
(742, 2, 600)
class 0 weight: 1.936231884057971
class 1 weight: 2.0681114551083595
Score for fold 8: loss of 0.3384385406970978; accuracy of 85.13513803482056%
(742, 2, 600)
class 0 weight: 1.936231884057971
class 1 weight: 2.0681114551083595
Score for fold 9: loss of 0.36768198013305664; accuracy of 78.37837934494019%
trainig on file: trainData_500.0mssec_P80 p80.conf.list.csv.arff
(1666, 2, 600)
class 0 weight: 2.0733056708160444
class 1 weight: 1.931701030927835
Score for fold 0: loss of 0.6437235474586487; accuracy of 67.06587076187134%
(1666, 2, 600)
class 0 weight: 2.0733056708160444
class 1 weight: 1.931701030927835
Score for fold 1: loss of 0.51557457447052; accuracy of 73.65269660949707%
(1666, 2, 600)
class 0 weight: 2.0733056708160444
class 1 weight: 1.931701030927835
Score for fold 2: loss of 0.5959357619285583; accuracy of 68.2634711265564%
(1666, 2, 600)
class 0 weight: 2.0761772853185594
class 1 weight: 1.9292149292149292
Score for fold 3: loss of 0.4915829002857208; accuracy of 71.856290102005%
(1666, 2, 600)
class 0 weight: 2.0761772853185594
class 1 weight: 1.9292149292149292
Score for fold 4: loss of 0.3858238160610199; accuracy of 82.6347291469574%
(1666, 2, 600)
class 0 weight: 2.0761772853185594
class 1 weight: 1.9292149292149292
Score for fold 5: loss of 0.41573721170425415; accuracy of 78.44311594963074%
(1666, 2, 600)
class 0 weight: 2.074688796680498
class 1 weight: 1.9305019305019304
Score for fold 6: loss of 0.45610010623931885; accuracy of 75.30120611190796%
(1666, 2, 600)
class 0 weight: 2.074688796680498
class 1 weight: 1.9305019305019304
Score for fold 7: loss of 0.4792088568210602; accuracy of 78.91566157341003%
(1666, 2, 600)
class 0 weight: 2.074688796680498
class 1 weight: 1.9305019305019304
Score for fold 8: loss of 0.6103455424308777; accuracy of 72.89156913757324%
(1666, 2, 600)
class 0 weight: 2.074688796680498
class 1 weight: 1.9305019305019304
Score for fold 9: loss of 0.5227432250976562; accuracy of 76.50602459907532%
trainig on file: trainData_500.0mssec_P36 p36.anatomy.matrix.csv.arff
(502, 2, 600)
class 0 weight: 1.5445205479452053
class 1 weight: 2.8364779874213837
Score for fold 0: loss of 0.5319788455963135; accuracy of 74.50980544090271%
(502, 2, 600)
class 0 weight: 1.5445205479452053
class 1 weight: 2.8364779874213837
Score for fold 1: loss of 0.5975550413131714; accuracy of 66.66666865348816%
(502, 2, 600)
class 0 weight: 1.547945205479452
class 1 weight: 2.825
Score for fold 2: loss of 0.2999056577682495; accuracy of 87.99999952316284%
(502, 2, 600)
class 0 weight: 1.547945205479452
class 1 weight: 2.825
Score for fold 3: loss of 0.4058946669101715; accuracy of 86.00000143051147%
(502, 2, 600)
class 0 weight: 1.547945205479452
class 1 weight: 2.825
Score for fold 4: loss of 0.4799712002277374; accuracy of 81.99999928474426%
(502, 2, 600)
class 0 weight: 1.5426621160409557
class 1 weight: 2.8427672955974845
Score for fold 5: loss of 0.49364525079727173; accuracy of 80.0000011920929%
(502, 2, 600)
class 0 weight: 1.5426621160409557
class 1 weight: 2.8427672955974845
Score for fold 6: loss of 0.2084379643201828; accuracy of 87.99999952316284%
(502, 2, 600)
class 0 weight: 1.5426621160409557
class 1 weight: 2.8427672955974845
Score for fold 7: loss of 0.27263903617858887; accuracy of 89.99999761581421%
(502, 2, 600)
class 0 weight: 1.5426621160409557
class 1 weight: 2.8427672955974845
Score for fold 8: loss of 0.29332807660102844; accuracy of 81.99999928474426%
(502, 2, 600)
class 0 weight: 1.5426621160409557
class 1 weight: 2.8427672955974845
Score for fold 9: loss of 0.32186195254325867; accuracy of 86.00000143051147%
trainig on file: trainData_500.0mssec_P25 p25.anatomy.list.csv.arff
(475, 2, 600)
class 0 weight: 2.259259259259259
class 1 weight: 1.7941176470588234
Score for fold 0: loss of 0.4513779580593109; accuracy of 83.33333134651184%
(475, 2, 600)
class 0 weight: 2.2473684210526317
class 1 weight: 1.80168776371308
Score for fold 1: loss of 0.3696170151233673; accuracy of 85.41666865348816%
(475, 2, 600)
class 0 weight: 2.2473684210526317
class 1 weight: 1.80168776371308
Score for fold 2: loss of 0.3347227871417999; accuracy of 87.5%
(475, 2, 600)
class 0 weight: 2.2473684210526317
class 1 weight: 1.80168776371308
Score for fold 3: loss of 0.2481970638036728; accuracy of 91.66666865348816%
(475, 2, 600)
class 0 weight: 2.2473684210526317
class 1 weight: 1.80168776371308
Score for fold 4: loss of 0.17413485050201416; accuracy of 93.75%
(475, 2, 600)
class 0 weight: 2.2526315789473683
class 1 weight: 1.7983193277310923
Score for fold 5: loss of 0.44099029898643494; accuracy of 78.72340679168701%
(475, 2, 600)
class 0 weight: 2.2526315789473683
class 1 weight: 1.7983193277310923
Score for fold 6: loss of 0.1575312465429306; accuracy of 95.7446813583374%
(475, 2, 600)
class 0 weight: 2.2526315789473683
class 1 weight: 1.7983193277310923
Score for fold 7: loss of 0.26765674352645874; accuracy of 89.3617033958435%
(475, 2, 600)
class 0 weight: 2.2526315789473683
class 1 weight: 1.7983193277310923
Score for fold 8: loss of 0.18976140022277832; accuracy of 93.6170220375061%
(475, 2, 600)
class 0 weight: 2.2526315789473683
class 1 weight: 1.7983193277310923
Score for fold 9: loss of 0.14085793495178223; accuracy of 95.7446813583374%
trainig on file: trainData_500.0mssec_P45 p45.conf.list.csv.arff
(1325, 2, 600)
class 0 weight: 3.435158501440922
class 1 weight: 1.4106508875739645
Score for fold 0: loss of 0.7138953804969788; accuracy of 66.91729426383972%
(1325, 2, 600)
class 0 weight: 3.435158501440922
class 1 weight: 1.4106508875739645
Score for fold 1: loss of 0.5671170949935913; accuracy of 72.93233275413513%
(1325, 2, 600)
class 0 weight: 3.435158501440922
class 1 weight: 1.4106508875739645
Score for fold 2: loss of 0.591768205165863; accuracy of 71.42857313156128%
(1325, 2, 600)
class 0 weight: 3.435158501440922
class 1 weight: 1.4106508875739645
Score for fold 3: loss of 0.604191243648529; accuracy of 63.15789222717285%
(1325, 2, 600)
class 0 weight: 3.435158501440922
class 1 weight: 1.4106508875739645
Score for fold 4: loss of 0.5647656321525574; accuracy of 64.6616518497467%
(1325, 2, 600)
class 0 weight: 3.42816091954023
class 1 weight: 1.4118343195266272
Score for fold 5: loss of 0.5720109939575195; accuracy of 70.45454382896423%
(1325, 2, 600)
class 0 weight: 3.42816091954023
class 1 weight: 1.4118343195266272
Score for fold 6: loss of 0.5049665570259094; accuracy of 67.42424368858337%
(1325, 2, 600)
class 0 weight: 3.42816091954023
class 1 weight: 1.4118343195266272
Score for fold 7: loss of 0.4107677638530731; accuracy of 80.30303120613098%
(1325, 2, 600)
class 0 weight: 3.42816091954023
class 1 weight: 1.4118343195266272
Score for fold 8: loss of 0.485539048910141; accuracy of 70.45454382896423%
(1325, 2, 600)
class 0 weight: 3.4380403458213253
class 1 weight: 1.4101654846335696
Score for fold 9: loss of 0.4338892102241516; accuracy of 75.0%
trainig on file: trainData_500.0mssec_P67 p67.conf.list.csv.arff
(721, 2, 600)
class 0 weight: 2.3225806451612905
class 1 weight: 1.7560975609756098
Score for fold 0: loss of 0.621940016746521; accuracy of 75.34246444702148%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 1: loss of 0.4887694716453552; accuracy of 77.77777910232544%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 2: loss of 0.5218077898025513; accuracy of 76.38888955116272%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 3: loss of 0.42698657512664795; accuracy of 84.72222089767456%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 4: loss of 0.2815861105918884; accuracy of 83.33333134651184%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 5: loss of 0.24200579524040222; accuracy of 87.5%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 6: loss of 0.48106110095977783; accuracy of 80.55555820465088%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 7: loss of 0.32147085666656494; accuracy of 80.55555820465088%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 8: loss of 0.32705721259117126; accuracy of 87.5%
(721, 2, 600)
class 0 weight: 2.3178571428571426
class 1 weight: 1.7588075880758807
Score for fold 9: loss of 0.47614896297454834; accuracy of 73.61111044883728%
trainig on file: trainData_500.0mssec_P53 p53.conf.list.csv.arff
(908, 2, 600)
class 0 weight: 1.8738532110091743
class 1 weight: 2.1443569553805775
Score for fold 0: loss of 0.772545337677002; accuracy of 64.83516693115234%
(908, 2, 600)
class 0 weight: 1.8738532110091743
class 1 weight: 2.1443569553805775
Score for fold 1: loss of 0.6057754755020142; accuracy of 73.62637519836426%
(908, 2, 600)
class 0 weight: 1.8738532110091743
class 1 weight: 2.1443569553805775
Score for fold 2: loss of 0.5260180830955505; accuracy of 68.13187003135681%
(908, 2, 600)
class 0 weight: 1.8738532110091743
class 1 weight: 2.1443569553805775
Score for fold 3: loss of 0.5413264632225037; accuracy of 67.03296899795532%
(908, 2, 600)
class 0 weight: 1.8781609195402298
class 1 weight: 2.1387434554973823
Score for fold 4: loss of 0.5682361125946045; accuracy of 68.13187003135681%
(908, 2, 600)
class 0 weight: 1.8781609195402298
class 1 weight: 2.1387434554973823
Score for fold 5: loss of 0.5321251749992371; accuracy of 71.42857313156128%
(908, 2, 600)
class 0 weight: 1.8781609195402298
class 1 weight: 2.1387434554973823
Score for fold 6: loss of 0.3893359303474426; accuracy of 81.31868243217468%
(908, 2, 600)
class 0 weight: 1.8781609195402298
class 1 weight: 2.1387434554973823
Score for fold 7: loss of 0.401100754737854; accuracy of 82.41758346557617%
(908, 2, 600)
class 0 weight: 1.8761467889908259
class 1 weight: 2.141361256544503
Score for fold 8: loss of 0.6473216414451599; accuracy of 68.88889074325562%
(908, 2, 600)
class 0 weight: 1.8761467889908259
class 1 weight: 2.141361256544503
Score for fold 9: loss of 0.5066438913345337; accuracy of 75.55555701255798%
trainig on file: trainData_500.0mssec_P61 p61.conf.list.csv.arff
(1239, 2, 600)
class 0 weight: 2.8443877551020407
class 1 weight: 1.5421853388658369
Score for fold 0: loss of 0.5629022717475891; accuracy of 75.0%
(1239, 2, 600)
class 0 weight: 2.8443877551020407
class 1 weight: 1.5421853388658369
Score for fold 1: loss of 0.5597196817398071; accuracy of 72.5806474685669%
(1239, 2, 600)
class 0 weight: 2.8443877551020407
class 1 weight: 1.5421853388658369
Score for fold 2: loss of 0.65599524974823; accuracy of 64.51612710952759%
(1239, 2, 600)
class 0 weight: 2.8443877551020407
class 1 weight: 1.5421853388658369
Score for fold 3: loss of 0.618643581867218; accuracy of 68.54838728904724%
(1239, 2, 600)
class 0 weight: 2.851662404092072
class 1 weight: 1.5400552486187844
Score for fold 4: loss of 0.5273367166519165; accuracy of 73.38709831237793%
(1239, 2, 600)
class 0 weight: 2.851662404092072
class 1 weight: 1.5400552486187844
Score for fold 5: loss of 0.40650808811187744; accuracy of 79.83871102333069%
(1239, 2, 600)
class 0 weight: 2.851662404092072
class 1 weight: 1.5400552486187844
Score for fold 6: loss of 0.47827228903770447; accuracy of 70.16128897666931%
(1239, 2, 600)
class 0 weight: 2.851662404092072
class 1 weight: 1.5400552486187844
Score for fold 7: loss of 0.46187394857406616; accuracy of 79.03226017951965%
(1239, 2, 600)
class 0 weight: 2.851662404092072
class 1 weight: 1.5400552486187844
Score for fold 8: loss of 0.40025100111961365; accuracy of 80.64516186714172%
(1239, 2, 600)
class 0 weight: 2.846938775510204
class 1 weight: 1.5414364640883977
Score for fold 9: loss of 0.46152788400650024; accuracy of 77.2357702255249%
trainig on file: trainData_500.0mssec_P51 p51.anatomy.list.csv.arff
(303, 2, 600)
class 0 weight: 1.6585365853658538
class 1 weight: 2.518518518518518
Score for fold 0: loss of 0.63291996717453; accuracy of 64.51612710952759%
(303, 2, 600)
class 0 weight: 1.6585365853658538
class 1 weight: 2.518518518518518
Score for fold 1: loss of 0.3206695020198822; accuracy of 90.32257795333862%
(303, 2, 600)
class 0 weight: 1.6585365853658538
class 1 weight: 2.518518518518518
Score for fold 2: loss of 0.44432657957077026; accuracy of 74.19354915618896%
(303, 2, 600)
class 0 weight: 1.6545454545454545
class 1 weight: 2.5277777777777777
Score for fold 3: loss of 0.17827937006950378; accuracy of 89.99999761581421%
(303, 2, 600)
class 0 weight: 1.6545454545454545
class 1 weight: 2.5277777777777777
Score for fold 4: loss of 0.0678906962275505; accuracy of 100.0%
(303, 2, 600)
class 0 weight: 1.6545454545454545
class 1 weight: 2.5277777777777777
Score for fold 5: loss of 0.13504499197006226; accuracy of 100.0%
(303, 2, 600)
class 0 weight: 1.6545454545454545
class 1 weight: 2.5277777777777777
Score for fold 6: loss of 0.32596728205680847; accuracy of 80.0000011920929%
(303, 2, 600)
class 0 weight: 1.6545454545454545
class 1 weight: 2.5277777777777777
Score for fold 7: loss of 0.0921526700258255; accuracy of 96.66666388511658%
(303, 2, 600)
class 0 weight: 1.6545454545454545
class 1 weight: 2.5277777777777777
Score for fold 8: loss of 0.2328709214925766; accuracy of 89.99999761581421%
(303, 2, 600)
class 0 weight: 1.6545454545454545
class 1 weight: 2.5277777777777777
Score for fold 9: loss of 0.3909848630428314; accuracy of 86.66666746139526%
trainig on file: trainData_500.0mssec_P27 p27.anatomy.list.csv.arff
(1466, 2, 600)
class 0 weight: 10.900826446280991
class 1 weight: 1.1010016694490818
Score for fold 0: loss of 0.7038412094116211; accuracy of 73.46938848495483%
(1466, 2, 600)
class 0 weight: 10.900826446280991
class 1 weight: 1.1010016694490818
Score for fold 1: loss of 0.5143004059791565; accuracy of 80.27210831642151%
(1466, 2, 600)
class 0 weight: 10.991666666666667
class 1 weight: 1.1000834028356963
Score for fold 2: loss of 0.3939984440803528; accuracy of 87.7551019191742%
(1466, 2, 600)
class 0 weight: 10.991666666666667
class 1 weight: 1.1000834028356963
Score for fold 3: loss of 0.44780299067497253; accuracy of 84.35373902320862%
(1466, 2, 600)
class 0 weight: 10.991666666666667
class 1 weight: 1.1000834028356963
Score for fold 4: loss of 0.4540461301803589; accuracy of 84.35373902320862%
(1466, 2, 600)
class 0 weight: 10.991666666666667
class 1 weight: 1.1000834028356963
Score for fold 5: loss of 0.20937351882457733; accuracy of 93.19728016853333%
(1466, 2, 600)
class 0 weight: 10.90909090909091
class 1 weight: 1.1009174311926606
Score for fold 6: loss of 0.6649746894836426; accuracy of 76.02739930152893%
(1466, 2, 600)
class 0 weight: 10.90909090909091
class 1 weight: 1.1009174311926606
Score for fold 7: loss of 0.715797483921051; accuracy of 74.65753555297852%
(1466, 2, 600)
class 0 weight: 10.90909090909091
class 1 weight: 1.1009174311926606
Score for fold 8: loss of 0.18984249234199524; accuracy of 91.09588861465454%
(1466, 2, 600)
class 0 weight: 10.90909090909091
class 1 weight: 1.1009174311926606
Score for fold 9: loss of 0.40853139758110046; accuracy of 81.50684833526611%
trainig on file: trainData_500.0mssec_P23 p23.conf.list.csv.arff
(1071, 2, 600)
class 0 weight: 2.4441624365482233
class 1 weight: 1.6924428822495605
Score for fold 0: loss of 0.47532781958580017; accuracy of 72.22222089767456%
(1071, 2, 600)
class 0 weight: 2.440506329113924
class 1 weight: 1.6942003514938488
Score for fold 1: loss of 0.36445489525794983; accuracy of 77.57009267807007%
(1071, 2, 600)
class 0 weight: 2.440506329113924
class 1 weight: 1.6942003514938488
Score for fold 2: loss of 0.4875989258289337; accuracy of 71.96261882781982%
(1071, 2, 600)
class 0 weight: 2.446700507614213
class 1 weight: 1.6912280701754385
Score for fold 3: loss of 0.4359290301799774; accuracy of 72.89719581604004%
(1071, 2, 600)
class 0 weight: 2.446700507614213
class 1 weight: 1.6912280701754385
Score for fold 4: loss of 0.4059702754020691; accuracy of 73.83177280426025%
(1071, 2, 600)
class 0 weight: 2.446700507614213
class 1 weight: 1.6912280701754385
Score for fold 5: loss of 0.4187798798084259; accuracy of 79.43925261497498%
(1071, 2, 600)
class 0 weight: 2.446700507614213
class 1 weight: 1.6912280701754385
Score for fold 6: loss of 0.44993841648101807; accuracy of 75.70093274116516%
(1071, 2, 600)
class 0 weight: 2.446700507614213
class 1 weight: 1.6912280701754385
Score for fold 7: loss of 0.3317091763019562; accuracy of 83.17757248878479%
(1071, 2, 600)
class 0 weight: 2.446700507614213
class 1 weight: 1.6912280701754385
Score for fold 8: loss of 0.47459861636161804; accuracy of 70.09345889091492%
(1071, 2, 600)
class 0 weight: 2.446700507614213
class 1 weight: 1.6912280701754385
Score for fold 9: loss of 0.33252236247062683; accuracy of 80.37382960319519%
trainig on file: trainData_500.0mssec_P62 p62.anatomy.matrix.csv.arff
(891, 2, 600)
class 0 weight: 14.833333333333332
class 1 weight: 1.072289156626506
Score for fold 0: loss of 0.3828396201133728; accuracy of 77.77777910232544%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 1: loss of 0.46470925211906433; accuracy of 69.66292262077332%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 2: loss of 0.41242748498916626; accuracy of 75.28089880943298%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 3: loss of 0.37245601415634155; accuracy of 82.02247023582458%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 4: loss of 0.4026845395565033; accuracy of 80.89887499809265%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 5: loss of 0.4741959273815155; accuracy of 75.28089880943298%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 6: loss of 0.46479472517967224; accuracy of 77.52808928489685%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 7: loss of 0.36552324891090393; accuracy of 80.89887499809265%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 8: loss of 0.4149697422981262; accuracy of 77.52808928489685%
(891, 2, 600)
class 0 weight: 14.58181818181818
class 1 weight: 1.073627844712182
Score for fold 9: loss of 0.37316393852233887; accuracy of 86.51685118675232%
trainig on file: trainData_500.0mssec_P68 p68.conf.matrix.csv.arff
(1187, 2, 600)
class 0 weight: 2.604878048780488
class 1 weight: 1.6231003039513678
Score for fold 0: loss of 0.4726908206939697; accuracy of 76.47058963775635%
(1187, 2, 600)
class 0 weight: 2.604878048780488
class 1 weight: 1.6231003039513678
Score for fold 1: loss of 0.40561026334762573; accuracy of 76.47058963775635%
(1187, 2, 600)
class 0 weight: 2.604878048780488
class 1 weight: 1.6231003039513678
Score for fold 2: loss of 0.26941806077957153; accuracy of 83.1932783126831%
(1187, 2, 600)
class 0 weight: 2.604878048780488
class 1 weight: 1.6231003039513678
Score for fold 3: loss of 0.30030569434165955; accuracy of 80.67227005958557%
(1187, 2, 600)
class 0 weight: 2.604878048780488
class 1 weight: 1.6231003039513678
Score for fold 4: loss of 0.31617867946624756; accuracy of 78.15126180648804%
(1187, 2, 600)
class 0 weight: 2.604878048780488
class 1 weight: 1.6231003039513678
Score for fold 5: loss of 0.3119722008705139; accuracy of 78.99159789085388%
(1187, 2, 600)
class 0 weight: 2.5985401459854014
class 1 weight: 1.6255707762557077
Score for fold 6: loss of 0.31296488642692566; accuracy of 81.51260614395142%
(1187, 2, 600)
class 0 weight: 2.6009732360097324
class 1 weight: 1.6246200607902737
Score for fold 7: loss of 0.306092768907547; accuracy of 82.20338821411133%
(1187, 2, 600)
class 0 weight: 2.6009732360097324
class 1 weight: 1.6246200607902737
Score for fold 8: loss of 0.3599780201911926; accuracy of 81.35592937469482%
(1187, 2, 600)
class 0 weight: 2.6009732360097324
class 1 weight: 1.6246200607902737
Score for fold 9: loss of 0.2993667721748352; accuracy of 77.96609997749329%
trainig on file: trainData_500.0mssec_P63 p63.anatomy.list.csv.arff
(371, 2, 600)
class 0 weight: 1.8603351955307263
class 1 weight: 2.1623376623376624
Score for fold 0: loss of 0.5497953295707703; accuracy of 81.57894611358643%
(371, 2, 600)
class 0 weight: 1.8555555555555556
class 1 weight: 2.168831168831169
Score for fold 1: loss of 0.47790879011154175; accuracy of 78.37837934494019%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 2: loss of 0.1782625913619995; accuracy of 91.89189076423645%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 3: loss of 0.2265905737876892; accuracy of 91.89189076423645%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 4: loss of 0.32126179337501526; accuracy of 83.7837815284729%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 5: loss of 0.18325884640216827; accuracy of 89.18918967247009%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 6: loss of 0.052131496369838715; accuracy of 100.0%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 7: loss of 0.24686303734779358; accuracy of 89.18918967247009%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 8: loss of 0.6897050142288208; accuracy of 75.67567825317383%
(371, 2, 600)
class 0 weight: 1.8659217877094973
class 1 weight: 2.1548387096774193
Score for fold 9: loss of 0.13466563820838928; accuracy of 97.29729890823364%
trainig on file: trainData_500.0mssec_P50 p50.anatomy.matrix.csv.arff
(527, 2, 600)
class 0 weight: 2.4307692307692306
class 1 weight: 1.6989247311827957
Score for fold 0: loss of 0.5165252089500427; accuracy of 75.47169923782349%
(527, 2, 600)
class 0 weight: 2.4432989690721647
class 1 weight: 1.6928571428571428
Score for fold 1: loss of 0.5508193373680115; accuracy of 66.03773832321167%
(527, 2, 600)
class 0 weight: 2.4432989690721647
class 1 weight: 1.6928571428571428
Score for fold 2: loss of 0.2868749797344208; accuracy of 86.79245114326477%
(527, 2, 600)
class 0 weight: 2.4432989690721647
class 1 weight: 1.6928571428571428
Score for fold 3: loss of 0.5106430649757385; accuracy of 75.47169923782349%
(527, 2, 600)
class 0 weight: 2.4432989690721647
class 1 weight: 1.6928571428571428
Score for fold 4: loss of 0.34015271067619324; accuracy of 81.13207817077637%
(527, 2, 600)
class 0 weight: 2.4432989690721647
class 1 weight: 1.6928571428571428
Score for fold 5: loss of 0.3812812268733978; accuracy of 83.01886916160583%
(527, 2, 600)
class 0 weight: 2.4432989690721647
class 1 weight: 1.6928571428571428
Score for fold 6: loss of 0.4074119031429291; accuracy of 77.35849022865295%
(527, 2, 600)
class 0 weight: 2.4358974358974357
class 1 weight: 1.6964285714285714
Score for fold 7: loss of 0.4983862340450287; accuracy of 75.0%
(527, 2, 600)
class 0 weight: 2.4358974358974357
class 1 weight: 1.6964285714285714
Score for fold 8: loss of 0.3315432667732239; accuracy of 82.69230723381042%
(527, 2, 600)
class 0 weight: 2.4358974358974357
class 1 weight: 1.6964285714285714
Score for fold 9: loss of 0.3420388996601105; accuracy of 80.7692289352417%
trainig on file: trainData_500.0mssec_P12 p12.conf.matrix.csv.arff
(932, 2, 600)
class 0 weight: 2.8996539792387543
class 1 weight: 1.5264116575591986
Score for fold 0: loss of 0.5913517475128174; accuracy of 67.02127456665039%
(932, 2, 600)
class 0 weight: 2.909722222222222
class 1 weight: 1.5236363636363637
Score for fold 1: loss of 0.3893967568874359; accuracy of 78.72340679168701%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 2: loss of 0.5430874228477478; accuracy of 74.19354915618896%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 3: loss of 0.5973415970802307; accuracy of 67.7419364452362%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 4: loss of 0.4191272556781769; accuracy of 74.19354915618896%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 5: loss of 0.5113762617111206; accuracy of 72.04301357269287%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 6: loss of 0.6527047157287598; accuracy of 72.04301357269287%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 7: loss of 0.4909523129463196; accuracy of 70.96773982048035%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 8: loss of 0.5207180976867676; accuracy of 63.44085931777954%
(932, 2, 600)
class 0 weight: 2.9031141868512114
class 1 weight: 1.5254545454545454
Score for fold 9: loss of 0.4150852859020233; accuracy of 76.34408473968506%
trainig on file: trainData_500.0mssec_P8 p8.conf.matrix.csv.arff
(1103, 2, 600)
class 0 weight: 3.859922178988327
class 1 weight: 1.3496598639455781
Score for fold 0: loss of 0.7624118328094482; accuracy of 65.76576828956604%
(1103, 2, 600)
class 0 weight: 3.859922178988327
class 1 weight: 1.3496598639455781
Score for fold 1: loss of 0.5968331098556519; accuracy of 69.36936974525452%
(1103, 2, 600)
class 0 weight: 3.859922178988327
class 1 weight: 1.3496598639455781
Score for fold 2: loss of 0.34923821687698364; accuracy of 80.18018007278442%
(1103, 2, 600)
class 0 weight: 3.8638132295719845
class 1 weight: 1.3491847826086956
Score for fold 3: loss of 0.327092707157135; accuracy of 80.90909123420715%
(1103, 2, 600)
class 0 weight: 3.8638132295719845
class 1 weight: 1.3491847826086956
Score for fold 4: loss of 0.4120303690433502; accuracy of 76.36363506317139%
(1103, 2, 600)
class 0 weight: 3.8638132295719845
class 1 weight: 1.3491847826086956
Score for fold 5: loss of 0.325472891330719; accuracy of 82.72727131843567%
(1103, 2, 600)
class 0 weight: 3.8488372093023253
class 1 weight: 1.3510204081632653
Score for fold 6: loss of 0.45813509821891785; accuracy of 72.72727489471436%
(1103, 2, 600)
class 0 weight: 3.8488372093023253
class 1 weight: 1.3510204081632653
Score for fold 7: loss of 0.2671722173690796; accuracy of 85.45454740524292%
(1103, 2, 600)
class 0 weight: 3.8488372093023253
class 1 weight: 1.3510204081632653
Score for fold 8: loss of 0.39746543765068054; accuracy of 76.36363506317139%
(1103, 2, 600)
class 0 weight: 3.8488372093023253
class 1 weight: 1.3510204081632653
Score for fold 9: loss of 0.46244147419929504; accuracy of 74.54545497894287%
trainig on file: trainData_500.0mssec_P81 p81.conf.matrix.csv.arff
(1385, 2, 600)
class 0 weight: 4.181208053691275
class 1 weight: 1.3143459915611813
Score for fold 0: loss of 0.5294529795646667; accuracy of 75.53957104682922%
(1385, 2, 600)
class 0 weight: 4.181208053691275
class 1 weight: 1.3143459915611813
Score for fold 1: loss of 0.41572725772857666; accuracy of 76.9784152507782%
(1385, 2, 600)
class 0 weight: 4.181208053691275
class 1 weight: 1.3143459915611813
Score for fold 2: loss of 0.5630313754081726; accuracy of 73.3812928199768%
(1385, 2, 600)
class 0 weight: 4.181208053691275
class 1 weight: 1.3143459915611813
Score for fold 3: loss of 0.5251551270484924; accuracy of 72.66187071800232%
(1385, 2, 600)
class 0 weight: 4.1952861952861955
class 1 weight: 1.3129610115911485
Score for fold 4: loss of 0.426931768655777; accuracy of 76.9784152507782%
(1385, 2, 600)
class 0 weight: 4.184563758389261
class 1 weight: 1.3140147523709167
Score for fold 5: loss of 0.534031867980957; accuracy of 72.46376872062683%
(1385, 2, 600)
class 0 weight: 4.184563758389261
class 1 weight: 1.3140147523709167
Score for fold 6: loss of 0.56414395570755; accuracy of 71.7391312122345%
(1385, 2, 600)
class 0 weight: 4.184563758389261
class 1 weight: 1.3140147523709167
Score for fold 7: loss of 0.4249914288520813; accuracy of 74.63768124580383%
(1385, 2, 600)
class 0 weight: 4.184563758389261
class 1 weight: 1.3140147523709167
Score for fold 8: loss of 0.39003339409828186; accuracy of 78.98550629615784%
(1385, 2, 600)
class 0 weight: 4.184563758389261
class 1 weight: 1.3140147523709167
Score for fold 9: loss of 0.4043130874633789; accuracy of 77.53623127937317%
trainig on file: trainData_500.0mssec_P12 p12.anatomy.matrix.csv.arff
(594, 2, 600)
class 0 weight: 9.368421052631579
class 1 weight: 1.119496855345912
Score for fold 0: loss of 0.39701536297798157; accuracy of 78.33333611488342%
(594, 2, 600)
class 0 weight: 9.368421052631579
class 1 weight: 1.119496855345912
Score for fold 1: loss of 0.2976214587688446; accuracy of 81.66666626930237%
(594, 2, 600)
class 0 weight: 9.368421052631579
class 1 weight: 1.119496855345912
Score for fold 2: loss of 0.20393630862236023; accuracy of 93.33333373069763%
(594, 2, 600)
class 0 weight: 9.368421052631579
class 1 weight: 1.119496855345912
Score for fold 3: loss of 0.1638329178094864; accuracy of 91.66666865348816%
(594, 2, 600)
class 0 weight: 9.224137931034482
class 1 weight: 1.1215932914046123
Score for fold 4: loss of 0.3968065679073334; accuracy of 86.44067645072937%
(594, 2, 600)
class 0 weight: 9.224137931034482
class 1 weight: 1.1215932914046123
Score for fold 5: loss of 0.14612101018428802; accuracy of 93.22034120559692%
(594, 2, 600)
class 0 weight: 9.224137931034482
class 1 weight: 1.1215932914046123
Score for fold 6: loss of 0.20626044273376465; accuracy of 91.52542352676392%
(594, 2, 600)
class 0 weight: 9.224137931034482
class 1 weight: 1.1215932914046123
Score for fold 7: loss of 0.12630464136600494; accuracy of 96.61017060279846%
(594, 2, 600)
class 0 weight: 9.224137931034482
class 1 weight: 1.1215932914046123
Score for fold 8: loss of 0.19153474271297455; accuracy of 89.83050584793091%
(594, 2, 600)
class 0 weight: 9.224137931034482
class 1 weight: 1.1215932914046123
Score for fold 9: loss of 0.4668293297290802; accuracy of 81.35592937469482%
trainig on file: trainData_500.0mssec_P5 p5.conf.list.csv.arff
(1366, 2, 600)
class 0 weight: 2.292910447761194
class 1 weight: 1.7734487734487734
Score for fold 0: loss of 0.702946662902832; accuracy of 70.07299065589905%
(1366, 2, 600)
class 0 weight: 2.297196261682243
class 1 weight: 1.770893371757925
Score for fold 1: loss of 0.619799017906189; accuracy of 68.61313581466675%
(1366, 2, 600)
class 0 weight: 2.297196261682243
class 1 weight: 1.770893371757925
Score for fold 2: loss of 0.6224336624145508; accuracy of 67.15328693389893%
(1366, 2, 600)
class 0 weight: 2.297196261682243
class 1 weight: 1.770893371757925
Score for fold 3: loss of 0.5986459255218506; accuracy of 70.07299065589905%
(1366, 2, 600)
class 0 weight: 2.297196261682243
class 1 weight: 1.770893371757925
Score for fold 4: loss of 0.48995086550712585; accuracy of 78.10218930244446%
(1366, 2, 600)
class 0 weight: 2.297196261682243
class 1 weight: 1.770893371757925
Score for fold 5: loss of 0.5393320918083191; accuracy of 75.91241002082825%
(1366, 2, 600)
class 0 weight: 2.294776119402985
class 1 weight: 1.7723342939481266
Score for fold 6: loss of 0.40349990129470825; accuracy of 84.5588207244873%
(1366, 2, 600)
class 0 weight: 2.294776119402985
class 1 weight: 1.7723342939481266
Score for fold 7: loss of 0.42373165488243103; accuracy of 80.14705777168274%
(1366, 2, 600)
class 0 weight: 2.294776119402985
class 1 weight: 1.7723342939481266
Score for fold 8: loss of 0.44248101115226746; accuracy of 76.47058963775635%
(1366, 2, 600)
class 0 weight: 2.294776119402985
class 1 weight: 1.7723342939481266
Score for fold 9: loss of 0.5197293758392334; accuracy of 77.9411792755127%
trainig on file: trainData_500.0mssec_P38 p38.anatomy.matrix.csv.arff
(449, 2, 600)
class 0 weight: 5.050000000000001
class 1 weight: 1.2469135802469136
Score for fold 0: loss of 0.4388367235660553; accuracy of 82.22222328186035%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 1: loss of 0.15079164505004883; accuracy of 93.33333373069763%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 2: loss of 0.13468657433986664; accuracy of 95.55555582046509%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 3: loss of 0.0534302294254303; accuracy of 97.77777791023254%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 4: loss of 0.1360473781824112; accuracy of 95.55555582046509%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 5: loss of 0.04934856668114662; accuracy of 97.77777791023254%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 6: loss of 0.1166691854596138; accuracy of 97.77777791023254%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 7: loss of 0.07547535747289658; accuracy of 97.77777791023254%
(449, 2, 600)
class 0 weight: 5.113924050632911
class 1 weight: 1.243076923076923
Score for fold 8: loss of 0.03297014907002449; accuracy of 97.77777791023254%
(449, 2, 600)
class 0 weight: 5.0625
class 1 weight: 1.2461538461538462
Score for fold 9: loss of 0.070214182138443; accuracy of 95.45454382896423%
trainig on file: trainData_500.0mssec_P64 p64.conf.matrix.csv.arff
(1290, 2, 600)
class 0 weight: 4.5
class 1 weight: 1.2857142857142856
Score for fold 0: loss of 0.7212380170822144; accuracy of 65.89147448539734%
(1290, 2, 600)
class 0 weight: 4.5
class 1 weight: 1.2857142857142856
Score for fold 1: loss of 0.5657562613487244; accuracy of 68.99224519729614%
(1290, 2, 600)
class 0 weight: 4.5
class 1 weight: 1.2857142857142856
Score for fold 2: loss of 0.4223393201828003; accuracy of 75.1937985420227%
(1290, 2, 600)
class 0 weight: 4.5
class 1 weight: 1.2857142857142856
Score for fold 3: loss of 0.37739789485931396; accuracy of 75.1937985420227%
(1290, 2, 600)
class 0 weight: 4.517509727626459
class 1 weight: 1.2842920353982301
Score for fold 4: loss of 0.41934382915496826; accuracy of 74.41860437393188%
(1290, 2, 600)
class 0 weight: 4.517509727626459
class 1 weight: 1.2842920353982301
Score for fold 5: loss of 0.4766254425048828; accuracy of 72.86821603775024%
(1290, 2, 600)
class 0 weight: 4.517509727626459
class 1 weight: 1.2842920353982301
Score for fold 6: loss of 0.41980108618736267; accuracy of 75.96899271011353%
(1290, 2, 600)
class 0 weight: 4.517509727626459
class 1 weight: 1.2842920353982301
Score for fold 7: loss of 0.3618999123573303; accuracy of 75.96899271011353%
(1290, 2, 600)
class 0 weight: 4.517509727626459
class 1 weight: 1.2842920353982301
Score for fold 8: loss of 0.4853886365890503; accuracy of 72.09302186965942%
(1290, 2, 600)
class 0 weight: 4.517509727626459
class 1 weight: 1.2842920353982301
Score for fold 9: loss of 0.4106687307357788; accuracy of 76.74418687820435%
trainig on file: trainData_500.0mssec_P70 p70.conf.matrix.csv.arff
(2191, 2, 600)
class 0 weight: 2.807692307692308
class 1 weight: 1.5531914893617023
Score for fold 0: loss of 0.5546427369117737; accuracy of 70.90908885002136%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 1: loss of 0.4757230579853058; accuracy of 75.34246444702148%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 2: loss of 0.37676766514778137; accuracy of 80.82191944122314%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 3: loss of 0.31872421503067017; accuracy of 86.30136847496033%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 4: loss of 0.3781487047672272; accuracy of 82.19178318977356%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 5: loss of 0.3361496329307556; accuracy of 87.67123222351074%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 6: loss of 0.3384571373462677; accuracy of 87.21461296081543%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 7: loss of 0.32904303073883057; accuracy of 87.67123222351074%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 8: loss of 0.34276485443115234; accuracy of 84.93150472640991%
(2191, 2, 600)
class 0 weight: 2.8051209103840686
class 1 weight: 1.55397951142632
Score for fold 9: loss of 0.36876118183135986; accuracy of 85.3881299495697%
trainig on file: trainData_500.0mssec_P2 p2.anatomy.matrix.csv.arff
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 0: loss of 0.5612828731536865; accuracy of 80.7692289352417%
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 1: loss of 0.3650123178958893; accuracy of 86.53846383094788%
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 2: loss of 0.3922717869281769; accuracy of 86.53846383094788%
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 3: loss of 0.36168205738067627; accuracy of 80.7692289352417%
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 4: loss of 0.34810611605644226; accuracy of 80.7692289352417%
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 5: loss of 0.18260549008846283; accuracy of 94.2307710647583%
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 6: loss of 0.28647568821907043; accuracy of 84.61538553237915%
(518, 2, 600)
class 0 weight: 7.639344262295082
class 1 weight: 1.1506172839506172
Score for fold 7: loss of 0.21710887551307678; accuracy of 88.46153616905212%
(518, 2, 600)
class 0 weight: 7.532258064516129
class 1 weight: 1.1530864197530863
Score for fold 8: loss of 0.4309004247188568; accuracy of 82.35294222831726%
(518, 2, 600)
class 0 weight: 7.532258064516129
class 1 weight: 1.1530864197530863
Score for fold 9: loss of 0.29437685012817383; accuracy of 80.39215803146362%
trainig on file: trainData_500.0mssec_P71 p71.conf.list.csv.arff
(916, 2, 600)
class 0 weight: 1.79520697167756
class 1 weight: 2.2575342465753425
Score for fold 0: loss of 0.5141244530677795; accuracy of 73.9130437374115%
(916, 2, 600)
class 0 weight: 1.79520697167756
class 1 weight: 2.2575342465753425
Score for fold 1: loss of 0.47521016001701355; accuracy of 79.347825050354%
(916, 2, 600)
class 0 weight: 1.79520697167756
class 1 weight: 2.2575342465753425
Score for fold 2: loss of 0.4417950510978699; accuracy of 75.0%
(916, 2, 600)
class 0 weight: 1.79520697167756
class 1 weight: 2.2575342465753425
Score for fold 3: loss of 0.4396888017654419; accuracy of 76.0869562625885%
(916, 2, 600)
class 0 weight: 1.79520697167756
class 1 weight: 2.2575342465753425
Score for fold 4: loss of 0.4957870841026306; accuracy of 73.9130437374115%
(916, 2, 600)
class 0 weight: 1.79520697167756
class 1 weight: 2.2575342465753425
Score for fold 5: loss of 0.3873092830181122; accuracy of 78.2608687877655%
(916, 2, 600)
class 0 weight: 1.797385620915033
class 1 weight: 2.2540983606557377
Score for fold 6: loss of 0.46973901987075806; accuracy of 79.1208803653717%
(916, 2, 600)
class 0 weight: 1.797385620915033
class 1 weight: 2.2540983606557377
Score for fold 7: loss of 0.40487250685691833; accuracy of 79.1208803653717%
(916, 2, 600)
class 0 weight: 1.797385620915033
class 1 weight: 2.2540983606557377
Score for fold 8: loss of 0.31141239404678345; accuracy of 84.61538553237915%
(916, 2, 600)
class 0 weight: 1.797385620915033
class 1 weight: 2.2540983606557377
Score for fold 9: loss of 0.4275154769420624; accuracy of 76.92307829856873%
trainig on file: trainData_500.0mssec_P54 p54.anatomy.matrix.csv.arff
(456, 2, 600)
class 0 weight: 1.6334661354581672
class 1 weight: 2.578616352201258
Score for fold 0: loss of 0.0504932627081871; accuracy of 97.826087474823%
(456, 2, 600)
class 0 weight: 1.6334661354581672
class 1 weight: 2.578616352201258
Score for fold 1: loss of 0.0966518223285675; accuracy of 97.826087474823%
(456, 2, 600)
class 0 weight: 1.6334661354581672
class 1 weight: 2.578616352201258
Score for fold 2: loss of 0.15994247794151306; accuracy of 91.30434989929199%
(456, 2, 600)
class 0 weight: 1.6334661354581672
class 1 weight: 2.578616352201258
Score for fold 3: loss of 0.026592912152409554; accuracy of 100.0%
(456, 2, 600)
class 0 weight: 1.6334661354581672
class 1 weight: 2.578616352201258
Score for fold 4: loss of 0.1865517944097519; accuracy of 93.478262424469%
(456, 2, 600)
class 0 weight: 1.6334661354581672
class 1 weight: 2.578616352201258
Score for fold 5: loss of 0.27101296186447144; accuracy of 86.95651888847351%
(456, 2, 600)
class 0 weight: 1.6374501992031871
class 1 weight: 2.56875
Score for fold 6: loss of 0.11507049202919006; accuracy of 95.55555582046509%
(456, 2, 600)
class 0 weight: 1.6374501992031871
class 1 weight: 2.56875
Score for fold 7: loss of 0.17148494720458984; accuracy of 95.55555582046509%
(456, 2, 600)
class 0 weight: 1.6374501992031871
class 1 weight: 2.56875
Score for fold 8: loss of 0.05186474695801735; accuracy of 97.77777791023254%
(456, 2, 600)
class 0 weight: 1.630952380952381
class 1 weight: 2.5849056603773586
Score for fold 9: loss of 0.06804320216178894; accuracy of 97.77777791023254%
[<keras.callbacks.History object at 0x7fbbd0169180>, <keras.callbacks.History object at 0x7fbb403d7e20>, <keras.callbacks.History object at 0x7fbb83968ac0>, <keras.callbacks.History object at 0x7fbaa431ff70>, <keras.callbacks.History object at 0x7fb80c56e3e0>, <keras.callbacks.History object at 0x7fb7f6665c60>, <keras.callbacks.History object at 0x7fb7f5a4b4c0>, <keras.callbacks.History object at 0x7fb7f475d1e0>, <keras.callbacks.History object at 0x7fb7f231aaa0>, <keras.callbacks.History object at 0x7fb7b4cfacb0>, <keras.callbacks.History object at 0x7fbaa5e09a50>, <keras.callbacks.History object at 0x7fb7ac4d7310>, <keras.callbacks.History object at 0x7fb79c6c6fb0>, <keras.callbacks.History object at 0x7fb794ac6260>, <keras.callbacks.History object at 0x7fb788db7ac0>, <keras.callbacks.History object at 0x7fbaa5a9d3c0>, <keras.callbacks.History object at 0x7fb780472c20>, <keras.callbacks.History object at 0x7fb794d18040>, <keras.callbacks.History object at 0x7fb7704f2950>, <keras.callbacks.History object at 0x7fb7687d3fa0>, <keras.callbacks.History object at 0x7fb7602c9ab0>, <keras.callbacks.History object at 0x7fb7546af340>, <keras.callbacks.History object at 0x7fb73928f5b0>, <keras.callbacks.History object at 0x7fb731fb6650>, <keras.callbacks.History object at 0x7fb73037fbb0>, <keras.callbacks.History object at 0x7fb728679780>, <keras.callbacks.History object at 0x7fb6e0166fe0>, <keras.callbacks.History object at 0x7fb664c22bf0>, <keras.callbacks.History object at 0x7fb6639f6170>, <keras.callbacks.History object at 0x7fb65f607520>, <keras.callbacks.History object at 0x7fb65d3cf760>, <keras.callbacks.History object at 0x7fb65c7d6680>, <keras.callbacks.History object at 0x7fb65c3b3be0>, <keras.callbacks.History object at 0x7fb6546bd5a0>, <keras.callbacks.History object at 0x7fb80daab0d0>, <keras.callbacks.History object at 0x7fb650666cb0>, <keras.callbacks.History object at 0x7fb64ea5e260>, <keras.callbacks.History object at 0x7fb64d9c1f00>, <keras.callbacks.History object at 0x7fb7f7c9f790>, <keras.callbacks.History object at 0x7fb64a169060>, <keras.callbacks.History object at 0x7fb647d428f0>, <keras.callbacks.History object at 0x7fb646117e20>, <keras.callbacks.History object at 0x7fb6445059f0>, <keras.callbacks.History object at 0x7fb7f6e0b250>, <keras.callbacks.History object at 0x7fb640ea6e00>, <keras.callbacks.History object at 0x7fb640aa63b0>, <keras.callbacks.History object at 0x7fb645205f00>, <keras.callbacks.History object at 0x7fb63bd77ee0>, <keras.callbacks.History object at 0x7fb63a952020>, <keras.callbacks.History object at 0x7fb63850f8b0>, <keras.callbacks.History object at 0x7fb63710d180>, <keras.callbacks.History object at 0x7fb795f129b0>, <keras.callbacks.History object at 0x7fb634a8aaa0>, <keras.callbacks.History object at 0x7fb631689ab0>, <keras.callbacks.History object at 0x7fb7694b3340>, <keras.callbacks.History object at 0x7fb62de26f80>, <keras.callbacks.History object at 0x7fb62c20a470>, <keras.callbacks.History object at 0x7fb62a5f3cd0>, <keras.callbacks.History object at 0x7fb631688a30>, <keras.callbacks.History object at 0x7fb627a02410>, <keras.callbacks.History object at 0x7fb6255cfca0>, <keras.callbacks.History object at 0x7fb6241d1510>, <keras.callbacks.History object at 0x7fb621daeda0>, <keras.callbacks.History object at 0x7fb62035ee30>, <keras.callbacks.History object at 0x7fb61e76de70>, <keras.callbacks.History object at 0x7fb61cb4f7c0>, <keras.callbacks.History object at 0x7fb61af19060>, <keras.callbacks.History object at 0x7fb6193028f0>, <keras.callbacks.History object at 0x7fb6176dfe20>, <keras.callbacks.History object at 0x7fb615ac5a50>, <keras.callbacks.History object at 0x7fb613ea65c0>, <keras.callbacks.History object at 0x7fb661ef7b50>, <keras.callbacks.History object at 0x7fb6138416c0>, <keras.callbacks.History object at 0x7fb60ffd6c50>, <keras.callbacks.History object at 0x7fb60db7ed10>, <keras.callbacks.History object at 0x7fb60c955d50>, <keras.callbacks.History object at 0x7fb609d1b5b0>, <keras.callbacks.History object at 0x7fb6080df7f0>, <keras.callbacks.History object at 0x7fb6064da6b0>, <keras.callbacks.History object at 0x7fb6048c3c10>, <keras.callbacks.History object at 0x7fb602c95570>, <keras.callbacks.History object at 0x7fb60107f010>, <keras.callbacks.History object at 0x7fb656f02bc0>, <keras.callbacks.History object at 0x7fb651ef6140>, <keras.callbacks.History object at 0x7fb5fbe0f970>, <keras.callbacks.History object at 0x7fb5fa1e9300>, <keras.callbacks.History object at 0x7fb5f85ceb90>, <keras.callbacks.History object at 0x7fb5f5ba2cb0>, <keras.callbacks.History object at 0x7fb5f0949c90>, <keras.callbacks.History object at 0x7fb5e9b7b4f0>, <keras.callbacks.History object at 0x7fb5e9b78d00>, <keras.callbacks.History object at 0x7fb5e0fdbbe0>, <keras.callbacks.History object at 0x7fb5da00d480>, <keras.callbacks.History object at 0x7fb5d4836d10>, <keras.callbacks.History object at 0x7fb5cd9bedd0>, <keras.callbacks.History object at 0x7fb5c6b45e10>, <keras.callbacks.History object at 0x7fb5d4283670>, <keras.callbacks.History object at 0x7fb5b901f8b0>, <keras.callbacks.History object at 0x7fb5b218e9e0>, <keras.callbacks.History object at 0x7fb5ab1bbf40>, <keras.callbacks.History object at 0x7fb5a445d780>, <keras.callbacks.History object at 0x7fb59972f730>, <keras.callbacks.History object at 0x7fb5934b5b10>, <keras.callbacks.History object at 0x7fb5a3e933a0>, <keras.callbacks.History object at 0x7fb58574b5b0>, <keras.callbacks.History object at 0x7fb57dfbe4d0>, <keras.callbacks.History object at 0x7fb577137d00>, <keras.callbacks.History object at 0x7fb584ea55a0>, <keras.callbacks.History object at 0x7fb569456e00>, <keras.callbacks.History object at 0x7fb5625faef0>, <keras.callbacks.History object at 0x7fb55b7b9f00>, <keras.callbacks.History object at 0x7fb55491b760>, <keras.callbacks.History object at 0x7fb598b7d450>, <keras.callbacks.History object at 0x7fb56885b730>, <keras.callbacks.History object at 0x7fb5477bf970>, <keras.callbacks.History object at 0x7fb53d24a830>, <keras.callbacks.History object at 0x7fb5368e7dc0>, <keras.callbacks.History object at 0x7fb52fdad930>, <keras.callbacks.History object at 0x7fb55ac07190>, <keras.callbacks.History object at 0x7fb5226c2d40>, <keras.callbacks.History object at 0x7fb517ba2290>, <keras.callbacks.History object at 0x7fb511043b20>, <keras.callbacks.History object at 0x7fb50a229390>, <keras.callbacks.History object at 0x7fb52d8c6c20>, <keras.callbacks.History object at 0x7fb4fca72ce0>, <keras.callbacks.History object at 0x7fb515b75d20>, <keras.callbacks.History object at 0x7fb4ef113580>, <keras.callbacks.History object at 0x7fb4e847b7c0>, <keras.callbacks.History object at 0x7fb4dd7de6e0>, <keras.callbacks.History object at 0x7fb4d6b0fc10>, <keras.callbacks.History object at 0x7fb4f5c655a0>, <keras.callbacks.History object at 0x7fb4c93b7040>, <keras.callbacks.History object at 0x7fb4be5f6c20>, <keras.callbacks.History object at 0x7fb4b38b2170>, <keras.callbacks.History object at 0x7fb4acb2b9a0>, <keras.callbacks.History object at 0x7fb4a1dd5270>, <keras.callbacks.History object at 0x7fb49b066ad0>, <keras.callbacks.History object at 0x7fb494347fd0>, <keras.callbacks.History object at 0x7fb48d5edbd0>, <keras.callbacks.History object at 0x7fb4ac465e40>, <keras.callbacks.History object at 0x7fb485a6f6d0>, <keras.callbacks.History object at 0x7fb47ec7b8e0>, <keras.callbacks.History object at 0x7fb477f2e7a0>, <keras.callbacks.History object at 0x7fb471043d60>, <keras.callbacks.History object at 0x7fb46a2ad8a0>, <keras.callbacks.History object at 0x7fb4636f3100>, <keras.callbacks.History object at 0x7fb45c952cb0>, <keras.callbacks.History object at 0x7fb451b96200>, <keras.callbacks.History object at 0x7fb46f057a60>, <keras.callbacks.History object at 0x7fb444021300>, <keras.callbacks.History object at 0x7fb43d276b60>, <keras.callbacks.History object at 0x7fb4364c6c50>, <keras.callbacks.History object at 0x7fb42f549c60>, <keras.callbacks.History object at 0x7fb4248334c0>, <keras.callbacks.History object at 0x7fb41d987700>, <keras.callbacks.History object at 0x7fb416b225c0>, <keras.callbacks.History object at 0x7fb423f53b50>, <keras.callbacks.History object at 0x7fb404d896c0>, <keras.callbacks.History object at 0x7fb3fdee6f20>, <keras.callbacks.History object at 0x7fb3f7062ad0>, <keras.callbacks.History object at 0x7fb3f01da020>, <keras.callbacks.History object at 0x7fb3e531f8b0>, <keras.callbacks.History object at 0x7fb3fdd8d150>, <keras.callbacks.History object at 0x7fb3d7f7e980>, <keras.callbacks.History object at 0x7fb3d13bbfa0>, <keras.callbacks.History object at 0x7fb3ca80da80>, <keras.callbacks.History object at 0x7fb3f66632e0>, <keras.callbacks.History object at 0x7fb3bd05bf70>, <keras.callbacks.History object at 0x7fb3b649e3e0>, <keras.callbacks.History object at 0x7fb3af8cfc40>, <keras.callbacks.History object at 0x7fb3a8cfd510>, <keras.callbacks.History object at 0x7fb4327d0130>, <keras.callbacks.History object at 0x7fb3a11956c0>, <keras.callbacks.History object at 0x7fb39d0bef20>, <keras.callbacks.History object at 0x7fb396c7aad0>, <keras.callbacks.History object at 0x7fb38fef2020>, <keras.callbacks.History object at 0x7fb3891af880>, <keras.callbacks.History object at 0x7fb38248d120>, <keras.callbacks.History object at 0x7fb37b736980>, <keras.callbacks.History object at 0x7fb3709fffa0>, <keras.callbacks.History object at 0x7fb369eb5a80>, <keras.callbacks.History object at 0x7fb3631932e0>, <keras.callbacks.History object at 0x7fb35c0bff70>, <keras.callbacks.History object at 0x7fb3551963e0>, <keras.callbacks.History object at 0x7fb34e2afc40>, <keras.callbacks.History object at 0x7fb3473894e0>, <keras.callbacks.History object at 0x7fb34048ad70>, <keras.callbacks.History object at 0x7fb33955ee30>, <keras.callbacks.History object at 0x7fb346875e70>, <keras.callbacks.History object at 0x7fb32b947700>, <keras.callbacks.History object at 0x7fb324a33910>, <keras.callbacks.History object at 0x7fb31db127d0>, <keras.callbacks.History object at 0x7fb31682fe80>, <keras.callbacks.History object at 0x7fb30f73d8d0>, <keras.callbacks.History object at 0x7fb308657130>, <keras.callbacks.History object at 0x7fb301562ce0>, <keras.callbacks.History object at 0x7fb30ef1d540>, <keras.callbacks.History object at 0x7fb2f35820b0>, <keras.callbacks.History object at 0x7fb2ec4cec20>, <keras.callbacks.History object at 0x7fb2f9b6f790>, <keras.callbacks.History object at 0x7fb2de3e7fa0>, <keras.callbacks.History object at 0x7fb2eb8b1ba0>, <keras.callbacks.History object at 0x7fb2d0b1f400>, <keras.callbacks.History object at 0x7fb2c9e97640>, <keras.callbacks.History object at 0x7fb2c32465c0>, <keras.callbacks.History object at 0x7fb2bc5fbb50>, <keras.callbacks.History object at 0x7fb2e3541720>, <keras.callbacks.History object at 0x7fb2aef3efb0>, <keras.callbacks.History object at 0x7fb2a82d6b30>, <keras.callbacks.History object at 0x7fb588222080>, <keras.callbacks.History object at 0x7fb370b1a050>, <keras.callbacks.History object at 0x7fb3a1194ac0>, <keras.callbacks.History object at 0x7fb29a5ff5e0>, <keras.callbacks.History object at 0x7fb290bfbf70>, <keras.callbacks.History object at 0x7fb28a3076a0>, <keras.callbacks.History object at 0x7fb2833c9600>, <keras.callbacks.History object at 0x7fb27c4b23b0>, <keras.callbacks.History object at 0x7fb27557fc10>, <keras.callbacks.History object at 0x7fb26e6514b0>, <keras.callbacks.History object at 0x7fb267656d10>, <keras.callbacks.History object at 0x7fb2608b6dd0>, <keras.callbacks.History object at 0x7fb259909e10>, <keras.callbacks.History object at 0x7fb252e7f670>, <keras.callbacks.History object at 0x7fb24c1938b0>, <keras.callbacks.History object at 0x7fb2454e6770>, <keras.callbacks.History object at 0x7fb23e7ebd00>, <keras.callbacks.History object at 0x7fb237b15870>, <keras.callbacks.History object at 0x7fb230e470d0>, <keras.callbacks.History object at 0x7fb22a392c80>, <keras.callbacks.History object at 0x7fb2236d21d0>, <keras.callbacks.History object at 0x7fb21ca13a30>, <keras.callbacks.History object at 0x7fb215c552d0>, <keras.callbacks.History object at 0x7fb20f0a2b30>, <keras.callbacks.History object at 0x7fb2083cdbd0>, <keras.callbacks.History object at 0x7fb2217b1c30>, <keras.callbacks.History object at 0x7fb1fac9f4c0>, <keras.callbacks.History object at 0x7fb1f4003700>, <keras.callbacks.History object at 0x7fb1ed382590>, <keras.callbacks.History object at 0x7fb2063cbdf0>, <keras.callbacks.History object at 0x7fb1df9f9690>, <keras.callbacks.History object at 0x7fb1d4d92ef0>, <keras.callbacks.History object at 0x7fb1ce0d2e30>, <keras.callbacks.History object at 0x7fb1c6b25c90>, <keras.callbacks.History object at 0x7fb1bf9b7c40>, <keras.callbacks.History object at 0x7fb1c69edc30>, <keras.callbacks.History object at 0x7fb1b27e7c10>, <keras.callbacks.History object at 0x7fb1a8bb1c00>, <keras.callbacks.History object at 0x7fb1a2373be0>, <keras.callbacks.History object at 0x7fb19be39ba0>, <keras.callbacks.History object at 0x7fb195a03b50>, <keras.callbacks.History object at 0x7fb18e9c9ed0>, <keras.callbacks.History object at 0x7fb18cfdb700>, <keras.callbacks.History object at 0x7fb1822b3940>, <keras.callbacks.History object at 0x7fb17b64e800>, <keras.callbacks.History object at 0x7fb174a13f10>, <keras.callbacks.History object at 0x7fb16ddf9930>, <keras.callbacks.History object at 0x7fb18e61f250>, <keras.callbacks.History object at 0x7fb160536e00>, <keras.callbacks.History object at 0x7fb1598d6350>, <keras.callbacks.History object at 0x7fb14ec53bb0>, <keras.callbacks.History object at 0x7fb2083ce680>, <keras.callbacks.History object at 0x7fb143f895a0>, <keras.callbacks.History object at 0x7fb14331f010>, <keras.callbacks.History object at 0x7fb140f5ebc0>, <keras.callbacks.History object at 0x7fb139826140>, <keras.callbacks.History object at 0x7fb1745e3970>, <keras.callbacks.History object at 0x7fb12bf21210>, <keras.callbacks.History object at 0x7fb1252d6a70>, <keras.callbacks.History object at 0x7fb11e69b6d0>, <keras.callbacks.History object at 0x7fb15e769b70>, <keras.callbacks.History object at 0x7fb11bdeb3d0>, <keras.callbacks.History object at 0x7fb11b2c7610>, <keras.callbacks.History object at 0x7fb0ff7ca4d0>, <keras.callbacks.History object at 0x7fb0f8bf7d30>, <keras.callbacks.History object at 0x7fb0ee0155d0>, <keras.callbacks.History object at 0x7fb0e740ae30>, <keras.callbacks.History object at 0x7fb0e082b820>, <keras.callbacks.History object at 0x7fb11c90df30>, <keras.callbacks.History object at 0x7fb0d325f790>, <keras.callbacks.History object at 0x7fb0cc6879d0>, <keras.callbacks.History object at 0x7fb0c58e2890>, <keras.callbacks.History object at 0x7fb0bed07f40>, <keras.callbacks.History object at 0x7fb0b8121990>, <keras.callbacks.History object at 0x7fb0b15231f0>, <keras.callbacks.History object at 0x7fb0ec23eda0>, <keras.callbacks.History object at 0x7fb09fdc22f0>, <keras.callbacks.History object at 0x7fb0951e7b50>, <keras.callbacks.History object at 0x7fb08a71d420>, <keras.callbacks.History object at 0x7fb083a5ac50>, <keras.callbacks.History object at 0x7fb07cdf6d40>, <keras.callbacks.History object at 0x7fb076239d50>, <keras.callbacks.History object at 0x7fb06f6835b0>, <keras.callbacks.History object at 0x7fb068a6b7f0>, <keras.callbacks.History object at 0x7fb0af79a6b0>, <keras.callbacks.History object at 0x7fb05b503c10>, <keras.callbacks.History object at 0x7fb05495d5a0>, <keras.callbacks.History object at 0x7fb08ecb3010>, <keras.callbacks.History object at 0x7fb04317abc0>, <keras.callbacks.History object at 0x7fb0385d2110>, <keras.callbacks.History object at 0x7fb0319ef970>, <keras.callbacks.History object at 0x7fb02ae1d210>, <keras.callbacks.History object at 0x7fb0595baa70>, <keras.callbacks.History object at 0x7fb01ce9f6d0>, <keras.callbacks.History object at 0x7fb015d55b70>, <keras.callbacks.History object at 0x7fb023923400>, <keras.callbacks.History object at 0x7fb007ad7640>, <keras.callbacks.History object at 0x7fb0009ae4d0>, <keras.callbacks.History object at 0x7faff9877d30>, <keras.callbacks.History object at 0x7faff2755600>, <keras.callbacks.History object at 0x7fb000172e30>, <keras.callbacks.History object at 0x7fafe46ef820>, <keras.callbacks.History object at 0x7fafdd5d9f60>, <keras.callbacks.History object at 0x7fafd6bbf7c0>, <keras.callbacks.History object at 0x7fafcfd9d060>, <keras.callbacks.History object at 0x7fafc918a8c0>, <keras.callbacks.History object at 0x7fafc2473f40>, <keras.callbacks.History object at 0x7fafbb775990>, <keras.callbacks.History object at 0x7fafb4a1b1f0>, <keras.callbacks.History object at 0x7fafadceeda0>, <keras.callbacks.History object at 0x7fb0274c5fc0>, <keras.callbacks.History object at 0x7fb144214af0>, <keras.callbacks.History object at 0x7fafa61f7820>, <keras.callbacks.History object at 0x7fafc23266e0>, <keras.callbacks.History object at 0x7faf9b7b7c10>, <keras.callbacks.History object at 0x7faf94b5d5a0>, <keras.callbacks.History object at 0x7faf8e0d3040>, <keras.callbacks.History object at 0x7faf8756abf0>, <keras.callbacks.History object at 0x7faf80bf6140>, <keras.callbacks.History object at 0x7faf7a08f9a0>, <keras.callbacks.History object at 0x7faf73541240>, <keras.callbacks.History object at 0x7faf6c8e6aa0>, <keras.callbacks.History object at 0x7faf65e8bfa0>, <keras.callbacks.History object at 0x7faf5ecbdba0>, <keras.callbacks.History object at 0x7faf57db7400>, <keras.callbacks.History object at 0x7faf50ea7640>, <keras.callbacks.History object at 0x7faf49fb2500>, <keras.callbacks.History object at 0x7faf5778bd60>, <keras.callbacks.History object at 0x7faf3c381600>, <keras.callbacks.History object at 0x7faf35472e60>, <keras.callbacks.History object at 0x7faf42943850>, <keras.callbacks.History object at 0x7faf2766df60>, <keras.callbacks.History object at 0x7faf2077b7c0>, <keras.callbacks.History object at 0x7faf19f01060>, <keras.callbacks.History object at 0x7faf133c68f0>, <keras.callbacks.History object at 0x7faf0ca6bf70>, <keras.callbacks.History object at 0x7faf05f059c0>, <keras.callbacks.History object at 0x7faeff3bb220>, <keras.callbacks.History object at 0x7faef886edd0>, <keras.callbacks.History object at 0x7faeedd32320>, <keras.callbacks.History object at 0x7faee7203b80>, <keras.callbacks.History object at 0x7faee069d420>, <keras.callbacks.History object at 0x7faed9b5ecb0>, <keras.callbacks.History object at 0x7faf2588ad40>, <keras.callbacks.History object at 0x7faf1e9c1d80>, <keras.callbacks.History object at 0x7faec38fb5e0>, <keras.callbacks.History object at 0x7faebc4af820>, <keras.callbacks.History object at 0x7faeb4f6e6e0>, <keras.callbacks.History object at 0x7faeadc2fc10>, <keras.callbacks.History object at 0x7faea680d5a0>, <keras.callbacks.History object at 0x7fae9f3af040>, <keras.callbacks.History object at 0x7fae97f3abf0>, <keras.callbacks.History object at 0x7fae90b0e170>, <keras.callbacks.History object at 0x7fae8a2339d0>, <keras.callbacks.History object at 0x7fae83349240>, <keras.callbacks.History object at 0x7fae7c356aa0>, <keras.callbacks.History object at 0x7fae7556ffa0>, <keras.callbacks.History object at 0x7fae6e691ba0>, <keras.callbacks.History object at 0x7fae67447430>, <keras.callbacks.History object at 0x7fae5c81f640>, <keras.callbacks.History object at 0x7fae6e056500>, <keras.callbacks.History object at 0x7fae4ec5fd90>, <keras.callbacks.History object at 0x7fae47d79630>, <keras.callbacks.History object at 0x7fae40e7ee90>, <keras.callbacks.History object at 0x7fae39f3fe80>, <keras.callbacks.History object at 0x7fae33089f90>, <keras.callbacks.History object at 0x7fae2c14f7f0>, <keras.callbacks.History object at 0x7fae25221090>, <keras.callbacks.History object at 0x7fae1e3268c0>, <keras.callbacks.History object at 0x7fae17453f70>, <keras.callbacks.History object at 0x7fae107959f0>, <keras.callbacks.History object at 0x7fae098b7250>, <keras.callbacks.History object at 0x7fae029bedd0>, <keras.callbacks.History object at 0x7fadfbb2e350>, <keras.callbacks.History object at 0x7fadf4dfbb80>, <keras.callbacks.History object at 0x7fadedfdd420>, <keras.callbacks.History object at 0x7fade719ec80>, <keras.callbacks.History object at 0x7fade057ad70>, <keras.callbacks.History object at 0x7fadd9771d80>, <keras.callbacks.History object at 0x7fadd2947610>, <keras.callbacks.History object at 0x7faddfcef820>, <keras.callbacks.History object at 0x7fadc4cd66e0>, <keras.callbacks.History object at 0x7faddfcee740>, <keras.callbacks.History object at 0x7faee069c9a0>, <keras.callbacks.History object at 0x7fadbc16f5b0>, <keras.callbacks.History object at 0x7fadbaddf7c0>, <keras.callbacks.History object at 0x7fadba186680>, <keras.callbacks.History object at 0x7fadb852bbe0>, <keras.callbacks.History object at 0x7fada75a1570>, <keras.callbacks.History object at 0x7fadd07bf010>, <keras.callbacks.History object at 0x7fad99dcebc0>, <keras.callbacks.History object at 0x7fad9321a0e0>, <keras.callbacks.History object at 0x7fad8c823940>, <keras.callbacks.History object at 0x7fad80fe51e0>, <keras.callbacks.History object at 0x7fadc99cea40>, <keras.callbacks.History object at 0x7fad72a536a0>, <keras.callbacks.History object at 0x7fad6b7a5b40>, <keras.callbacks.History object at 0x7fad645533a0>, <keras.callbacks.History object at 0x7fad5d2cb5b0>, <keras.callbacks.History object at 0x7fad560624a0>, <keras.callbacks.History object at 0x7fad4ede7d00>, <keras.callbacks.History object at 0x7fadc2f155a0>, <keras.callbacks.History object at 0x7fad3cad6e00>, <keras.callbacks.History object at 0x7fad3645aef0>, <keras.callbacks.History object at 0x7fad2f8adf00>, <keras.callbacks.History object at 0x7fad28ce7760>, <keras.callbacks.History object at 0x7fad221139a0>, <keras.callbacks.History object at 0x7fad1b57a860>, <keras.callbacks.History object at 0x7fad149b7e20>, <keras.callbacks.History object at 0x7fad0ddf9960>, <keras.callbacks.History object at 0x7fad79e671c0>, <keras.callbacks.History object at 0x7facf8832d70>, <keras.callbacks.History object at 0x7facf1c662c0>, <keras.callbacks.History object at 0x7faceb00bb20>, <keras.callbacks.History object at 0x7face45a53c0>, <keras.callbacks.History object at 0x7facd99fec20>, <keras.callbacks.History object at 0x7facd0a6ece0>, <keras.callbacks.History object at 0x7facc9f0dd20>, <keras.callbacks.History object at 0x7facc339b5b0>, <keras.callbacks.History object at 0x7facb5aef7c0>, <keras.callbacks.History object at 0x7facaef6a680>, <keras.callbacks.History object at 0x7faca83cbbe0>, <keras.callbacks.History object at 0x7face8fc1570>, <keras.callbacks.History object at 0x7fac9aaa6fe0>, <keras.callbacks.History object at 0x7fac93dd6b90>, <keras.callbacks.History object at 0x7fac8d14a0e0>, <keras.callbacks.History object at 0x7fac8648b940>, <keras.callbacks.History object at 0x7faca17211e0>, <keras.callbacks.History object at 0x7fac78d25d50>, <keras.callbacks.History object at 0x7fac7201e8c0>, <keras.callbacks.History object at 0x7fac6b29f430>, <keras.callbacks.History object at 0x7fac6476bcd0>, <keras.callbacks.History object at 0x7fac5db45870>, <keras.callbacks.History object at 0x7fac56d570a0>, <keras.callbacks.History object at 0x7fac501a6c50>, <keras.callbacks.History object at 0x7fac494361a0>, <keras.callbacks.History object at 0x7fac42673a00>, <keras.callbacks.History object at 0x7fac3b8d12a0>, <keras.callbacks.History object at 0x7fac34b56b00>, <keras.callbacks.History object at 0x7fac2ddcbfd0>, <keras.callbacks.History object at 0x7fac23051c30>, <keras.callbacks.History object at 0x7fac1c2bf460>, <keras.callbacks.History object at 0x7fac1553b6a0>, <keras.callbacks.History object at 0x7fac27f0e560>, <keras.callbacks.History object at 0x7fac07853dc0>, <keras.callbacks.History object at 0x7fac00921660>, <keras.callbacks.History object at 0x7fac0de06ec0>, <keras.callbacks.History object at 0x7fabf2b1fee0>, <keras.callbacks.History object at 0x7fabe7c65fc0>, <keras.callbacks.History object at 0x7fabe0d9b820>, <keras.callbacks.History object at 0x7fabf25110c0>, <keras.callbacks.History object at 0x7fabd31e6920>, <keras.callbacks.History object at 0x7fabcc2d7eb0>, <keras.callbacks.History object at 0x7fabc499da20>, <keras.callbacks.History object at 0x7fabbe17b280>, <keras.callbacks.History object at 0x7fabb7192e30>, <keras.callbacks.History object at 0x7fabb019a380>, <keras.callbacks.History object at 0x7faba9183be0>, <keras.callbacks.History object at 0x7faba218d480>, <keras.callbacks.History object at 0x7fab9b352d10>, <keras.callbacks.History object at 0x7fab9431ada0>, <keras.callbacks.History object at 0x7fab8d331de0>, <keras.callbacks.History object at 0x7fab8621f640>, <keras.callbacks.History object at 0x7fab7f5cb880>, <keras.callbacks.History object at 0x7fab78762740>, <keras.callbacks.History object at 0x7fab6d8dbcd0>, <keras.callbacks.History object at 0x7fab7effd600>, <keras.callbacks.History object at 0x7fab5fddf0d0>, <keras.callbacks.History object at 0x7fab58f56c50>, <keras.callbacks.History object at 0x7fab6638a1a0>, <keras.callbacks.History object at 0x7fab47257a00>, <keras.callbacks.History object at 0x7fabbe17b610>, <keras.callbacks.History object at 0x7facd99fdf60>, <keras.callbacks.History object at 0x7fab460fef50>, <keras.callbacks.History object at 0x7fab3eff2b30>, <keras.callbacks.History object at 0x7fab3d52e080>, <keras.callbacks.History object at 0x7fab3c8d38b0>, <keras.callbacks.History object at 0x7fab31f6d150>, <keras.callbacks.History object at 0x7fab2afd69b0>, <keras.callbacks.History object at 0x7fab23e4eaa0>, <keras.callbacks.History object at 0x7fab1cd15ae0>, <keras.callbacks.History object at 0x7fab15baf310>, <keras.callbacks.History object at 0x7fab0ec26fb0>, <keras.callbacks.History object at 0x7fab08466410>, <keras.callbacks.History object at 0x7fab233f3c70>, <keras.callbacks.History object at 0x7faafac8d540>, <keras.callbacks.History object at 0x7faaf4086d70>, <keras.callbacks.History object at 0x7faaed486e30>, <keras.callbacks.History object at 0x7faae68a5e70>, <keras.callbacks.History object at 0x7faadfcb36d0>, <keras.callbacks.History object at 0x7fab13e0f910>, <keras.callbacks.History object at 0x7faad26ee800>, <keras.callbacks.History object at 0x7faacbb0be20>, <keras.callbacks.History object at 0x7faac4ee59c0>, <keras.callbacks.History object at 0x7faaff717220>, <keras.callbacks.History object at 0x7faab7706e00>, <keras.callbacks.History object at 0x7faab0b36380>, <keras.callbacks.History object at 0x7faaa9f17c40>, <keras.callbacks.History object at 0x7faaa33546d0>, <keras.callbacks.History object at 0x7faaa7efb130>, <keras.callbacks.History object at 0x7faa9c940670>, <keras.callbacks.History object at 0x7faa96a43460>, <keras.callbacks.History object at 0x7faa8e8ef730>, <keras.callbacks.History object at 0x7faa839f2860>, <keras.callbacks.History object at 0x7faa7cf9be20>, <keras.callbacks.History object at 0x7faa7635d960>, <keras.callbacks.History object at 0x7faaa13771c0>, <keras.callbacks.History object at 0x7faa68aaad70>, <keras.callbacks.History object at 0x7faa61e522c0>, <keras.callbacks.History object at 0x7faa5b1c3b20>, <keras.callbacks.History object at 0x7faa5454d3c0>, <keras.callbacks.History object at 0x7faa4da9ac20>, <keras.callbacks.History object at 0x7faa46e36ce0>, <keras.callbacks.History object at 0x7faa75f2dd20>, <keras.callbacks.History object at 0x7faa3977f580>, <keras.callbacks.History object at 0x7faa32bef7f0>, <keras.callbacks.History object at 0x7faa2c03a680>, <keras.callbacks.History object at 0x7faa2549bb80>, <keras.callbacks.History object at 0x7faa2a021ba0>, <keras.callbacks.History object at 0x7faa1e8f7b50>, <keras.callbacks.History object at 0x7faa18c59b40>, <keras.callbacks.History object at 0x7faa12d0bb50>, <keras.callbacks.History object at 0x7faa02c996f0>, <keras.callbacks.History object at 0x7fa9fb55ef80>, <keras.callbacks.History object at 0x7fa9f44aeb00>, <keras.callbacks.History object at 0x7fa9ed452050>, <keras.callbacks.History object at 0x7fa9e63bb8b0>, <keras.callbacks.History object at 0x7fa9df541150>, <keras.callbacks.History object at 0x7fa9d84ba9e0>, <keras.callbacks.History object at 0x7fa9d1456aa0>, <keras.callbacks.History object at 0x7fa9ca2cdae0>, <keras.callbacks.History object at 0x7fa9c3353310>, <keras.callbacks.History object at 0x7fa9bc2befb0>, <keras.callbacks.History object at 0x7fa9b5c1e410>, <keras.callbacks.History object at 0x7fa9af0f7c70>, <keras.callbacks.History object at 0x7fa9cf2b1510>, <keras.callbacks.History object at 0x7fa9a1d02d70>, <keras.callbacks.History object at 0x7fa997226e30>, <keras.callbacks.History object at 0x7fa98c72de70>, <keras.callbacks.History object at 0x7fa985c2b6d0>, <keras.callbacks.History object at 0x7fa97f14b910>, <keras.callbacks.History object at 0x7fa9786467d0>, <keras.callbacks.History object at 0x7fa96db43e80>, <keras.callbacks.History object at 0x7fa9a61d18d0>, <keras.callbacks.History object at 0x7fa960093130>, <keras.callbacks.History object at 0x7fa983b02d10>, <keras.callbacks.History object at 0x7fa97664e230>, <keras.callbacks.History object at 0x7fa94bc67ac0>, <keras.callbacks.History object at 0x7fa944f35360>, <keras.callbacks.History object at 0x7fa93a1b2b90>, <keras.callbacks.History object at 0x7fa933446c50>, <keras.callbacks.History object at 0x7fa95281dcc0>, <keras.callbacks.History object at 0x7fa921b634f0>, <keras.callbacks.History object at 0x7fa91aa2f730>, <keras.callbacks.History object at 0x7fa92845e5f0>, <keras.callbacks.History object at 0x7fa90cb17b50>, <keras.callbacks.History object at 0x7fa9058816f0>, <keras.callbacks.History object at 0x7fa8fe912f50>, <keras.callbacks.History object at 0x7fa8f79b6b00>, <keras.callbacks.History object at 0x7fa8f0a0e050>, <keras.callbacks.History object at 0x7fa8e9c8b8b0>, <keras.callbacks.History object at 0x7fa8f6ed4340>, <keras.callbacks.History object at 0x7fa8d7c12da0>, <keras.callbacks.History object at 0x7fa8d1ec10f0>, <keras.callbacks.History object at 0x7fa8cbb32950>, <keras.callbacks.History object at 0x7fa8c49a7f10>, <keras.callbacks.History object at 0x7fa8bdb3da50>, <keras.callbacks.History object at 0x7fa8b6cf32b0>, <keras.callbacks.History object at 0x7fa8afd239a0>, <keras.callbacks.History object at 0x7fa8a8eaa3e0>, <keras.callbacks.History object at 0x7fa8a2227c10>, <keras.callbacks.History object at 0x7fa89b3954b0>, <keras.callbacks.History object at 0x7fa8944fad40>, <keras.callbacks.History object at 0x7fa88d716dd0>, <keras.callbacks.History object at 0x7fa8828f5e10>, <keras.callbacks.History object at 0x7fa89b1eb670>, <keras.callbacks.History object at 0x7fa874edf8b0>, <keras.callbacks.History object at 0x7fa86e0e2770>, <keras.callbacks.History object at 0x7fa8672efd00>, <keras.callbacks.History object at 0x7fa8605258a0>, <keras.callbacks.History object at 0x7fab5fddc250>, <keras.callbacks.History object at 0x7fa95c0324a0>, <keras.callbacks.History object at 0x7faa7cf99ba0>, <keras.callbacks.History object at 0x7fa858543670>, <keras.callbacks.History object at 0x7fa8569de530>, <keras.callbacks.History object at 0x7fa855f4bd90>, <keras.callbacks.History object at 0x7fa853ee9630>, <keras.callbacks.History object at 0x7fa841e2ae90>, <keras.callbacks.History object at 0x7fa83b20be80>, <keras.callbacks.History object at 0x7fa86015df90>, <keras.callbacks.History object at 0x7fa82d62b7f0>, <keras.callbacks.History object at 0x7fa826869090>, <keras.callbacks.History object at 0x7fa83b03e920>, <keras.callbacks.History object at 0x7fa818c47eb0>, <keras.callbacks.History object at 0x7fa8264c9a20>, <keras.callbacks.History object at 0x7fa80ae1f250>, <keras.callbacks.History object at 0x7fa803f4ee00>, <keras.callbacks.History object at 0x7fa7fd086350>, <keras.callbacks.History object at 0x7fa7f61bfbb0>, <keras.callbacks.History object at 0x7fa7ef2f9480>, <keras.callbacks.History object at 0x7fa7e85f2cb0>, <keras.callbacks.History object at 0x7fa7e16fad70>, <keras.callbacks.History object at 0x7fa7ee989de0>, <keras.callbacks.History object at 0x7fa7d36db610>, <keras.callbacks.History object at 0x7fa7cc657880>, <keras.callbacks.History object at 0x7fa7c565e740>, <keras.callbacks.History object at 0x7fa7be623ca0>, <keras.callbacks.History object at 0x7fa7cbf115d0>, <keras.callbacks.History object at 0x7fa7b07d3070>, <keras.callbacks.History object at 0x7fa7a9762c50>, <keras.callbacks.History object at 0x7fa7a25fe170>, <keras.callbacks.History object at 0x7fa79b6f39d0>, <keras.callbacks.History object at 0x7fa7946d5270>, <keras.callbacks.History object at 0x7fa78deb2b00>, <keras.callbacks.History object at 0x7fa7872ebfd0>, <keras.callbacks.History object at 0x7fa78096dbd0>, <keras.callbacks.History object at 0x7fa779daf430>, <keras.callbacks.History object at 0x7fa7a1c53670>, <keras.callbacks.History object at 0x7fa76c656530>, <keras.callbacks.History object at 0x7fa765aafd90>, <keras.callbacks.History object at 0x7fa75ef19660>, <keras.callbacks.History object at 0x7fa75833ae90>, <keras.callbacks.History object at 0x7fa75198bee0>, <keras.callbacks.History object at 0x7fa74acddf90>, <keras.callbacks.History object at 0x7fa7400a37f0>, <keras.callbacks.History object at 0x7fa739335090>, <keras.callbacks.History object at 0x7fa7327e28f0>, <keras.callbacks.History object at 0x7fa727b4fe20>, <keras.callbacks.History object at 0x7fa720f019f0>, <keras.callbacks.History object at 0x7fa71a307250>, <keras.callbacks.History object at 0x7fa7136b2e30>, <keras.callbacks.History object at 0x7fa70cc82350>, <keras.callbacks.History object at 0x7fa706053bb0>, <keras.callbacks.History object at 0x7fa6fec59450>, <keras.callbacks.History object at 0x7fa7116f6ce0>, <keras.callbacks.History object at 0x7fa6f0b62da0>, <keras.callbacks.History object at 0x7fa6e9b29de0>, <keras.callbacks.History object at 0x7fa6e2abb640>, <keras.callbacks.History object at 0x7fa6dba6f8b0>, <keras.callbacks.History object at 0x7fa6d4a02770>, <keras.callbacks.History object at 0x7fa6cdb8fcd0>, <keras.callbacks.History object at 0x7fa6c6b11600>, <keras.callbacks.History object at 0x7fa6bfa830a0>, <keras.callbacks.History object at 0x7fa6b8a4ec50>, <keras.callbacks.History object at 0x7fa6b1ba21d0>, <keras.callbacks.History object at 0x7fa6aabfba00>, <keras.callbacks.History object at 0x7fa6a3c752d0>, <keras.callbacks.History object at 0x7fa69ccd2b30>, <keras.callbacks.History object at 0x7fa695f4b670>, <keras.callbacks.History object at 0x7fa68ef6ffa0>, <keras.callbacks.History object at 0x7fa68802b730>, <keras.callbacks.History object at 0x7fa680ff95a0>, <keras.callbacks.History object at 0x7fa67a35f580>, <keras.callbacks.History object at 0x7fa6741ad540>, <keras.callbacks.History object at 0x7fa66a0274f0>, <keras.callbacks.History object at 0x7fa663a8d4e0>, <keras.callbacks.History object at 0x7fa65d7df4c0>, <keras.callbacks.History object at 0x7fa65765d480>, <keras.callbacks.History object at 0x7fa6510bb460>, <keras.callbacks.History object at 0x7fa64ae35780>, <keras.callbacks.History object at 0x7fa644b0afe0>, <keras.callbacks.History object at 0x7fa63990ab90>, <keras.callbacks.History object at 0x7fa632a2e0e0>, <keras.callbacks.History object at 0x7fa62c1b7940>, <keras.callbacks.History object at 0x7fa62568d1e0>, <keras.callbacks.History object at 0x7fa61ed56a40>, <keras.callbacks.History object at 0x7fa61823f6a0>, <keras.callbacks.History object at 0x7fa611735b40>, <keras.callbacks.History object at 0x7fa60abdf3d0>, <keras.callbacks.History object at 0x7fa603fbb5b0>, <keras.callbacks.History object at 0x7fa5f95824a0>, <keras.callbacks.History object at 0x7fa5f2a7fd00>, <keras.callbacks.History object at 0x7fa5e7f3d5a0>, <keras.callbacks.History object at 0x7fa62a422e00>, <keras.callbacks.History object at 0x7fa5da83aef0>, <keras.callbacks.History object at 0x7fa5d3c35f00>, <keras.callbacks.History object at 0x7fa616097760>, <keras.callbacks.History object at 0x7fa5c63df9a0>, <keras.callbacks.History object at 0x7fa5bf7fe860>, <keras.callbacks.History object at 0x7fa5b8bc7e20>, <keras.callbacks.History object at 0x7fa5b1fbd960>, <keras.callbacks.History object at 0x7fa5ab5771c0>, <keras.callbacks.History object at 0x7fa5a4976da0>, <keras.callbacks.History object at 0x7fa59dce22c0>, <keras.callbacks.History object at 0x7fa5cae9bb20>, <keras.callbacks.History object at 0x7fa59044d3c0>, <keras.callbacks.History object at 0x7fa5897fec50>, <keras.callbacks.History object at 0x7fa57eb72d10>, <keras.callbacks.History object at 0x7fa577f2dd20>, <keras.callbacks.History object at 0x7fa56d2d3580>, <keras.callbacks.History object at 0x7fa5668277c0>, <keras.callbacks.History object at 0x7fa55fc1e680>, <keras.callbacks.History object at 0x7fa558febc10>, <keras.callbacks.History object at 0x7fa551f55570>, <keras.callbacks.History object at 0x7fa54afcefe0>, <keras.callbacks.History object at 0x7fa544236b90>, <keras.callbacks.History object at 0x7fa5393a6110>, <keras.callbacks.History object at 0x7fa53250f970>, <keras.callbacks.History object at 0x7fa52b6891e0>, <keras.callbacks.History object at 0x7fa538db6a40>, <keras.callbacks.History object at 0x7fa51db1f6a0>, <keras.callbacks.History object at 0x7fa516c8db40>, <keras.callbacks.History object at 0x7fa50bdeb3a0>, <keras.callbacks.History object at 0x7fa51d0c6fb0>, <keras.callbacks.History object at 0x7fa4fdd7a890>, <keras.callbacks.History object at 0x7fa4f7952f50>, <keras.callbacks.History object at 0x7fa4f16ba830>, <keras.callbacks.History object at 0x7fa4eb2eaef0>, <keras.callbacks.History object at 0x7fa4e4c2a7d0>, <keras.callbacks.History object at 0x7fa4de976e90>, <keras.callbacks.History object at 0x7fa4d82be770>, <keras.callbacks.History object at 0x7fa4d1eeadd0>, <keras.callbacks.History object at 0x7fa4cfe76350>, <keras.callbacks.History object at 0x7fa4c4ed7b80>, <keras.callbacks.History object at 0x7fa574566110>, <keras.callbacks.History object at 0x7fa50eb181c0>, <keras.callbacks.History object at 0x7fa63990beb0>, <keras.callbacks.History object at 0x7fa74da76b60>, <keras.callbacks.History object at 0x7fab31f6e200>, <keras.callbacks.History object at 0x7fa4b7167640>, <keras.callbacks.History object at 0x7fa4b66f7850>, <keras.callbacks.History object at 0x7fa4b523a740>, <keras.callbacks.History object at 0x7fa4afdd3cd0>, <keras.callbacks.History object at 0x7fa4adf755d0>, <keras.callbacks.History object at 0x7fa4a597b070>, <keras.callbacks.History object at 0x7fa49ef3ac20>, <keras.callbacks.History object at 0x7fa49823a1a0>, <keras.callbacks.History object at 0x7fa4914179d0>, <keras.callbacks.History object at 0x7fa4867f5270>, <keras.callbacks.History object at 0x7fa47fb02ad0>, <keras.callbacks.History object at 0x7fa478fbffd0>, <keras.callbacks.History object at 0x7fa4722b5bd0>, <keras.callbacks.History object at 0x7fa46b5b3430>, <keras.callbacks.History object at 0x7fa4646ab670>, <keras.callbacks.History object at 0x7fa478dee530>, <keras.callbacks.History object at 0x7fa456c73d90>, <keras.callbacks.History object at 0x7fa44fe71630>, <keras.callbacks.History object at 0x7fa469496ec0>, <keras.callbacks.History object at 0x7fa442233e80>, <keras.callbacks.History object at 0x7fa43b461f90>, <keras.callbacks.History object at 0x7fa434677820>, <keras.callbacks.History object at 0x7fa42d89d090>, <keras.callbacks.History object at 0x7fa4269868f0>, <keras.callbacks.History object at 0x7fa41f7bbe20>, <keras.callbacks.History object at 0x7fa42d1859f0>, <keras.callbacks.History object at 0x7fa40d983250>, <keras.callbacks.History object at 0x7fa40699ae00>, <keras.callbacks.History object at 0x7fa417faa350>, <keras.callbacks.History object at 0x7fa3f89d7bb0>, <keras.callbacks.History object at 0x7fa3f1a15450>, <keras.callbacks.History object at 0x7fa3eaa22cb0>, <keras.callbacks.History object at 0x7fa3f82a2d70>, <keras.callbacks.History object at 0x7fa3dcc25db0>, <keras.callbacks.History object at 0x7fa3d6227610>, <keras.callbacks.History object at 0x7fa3e9e9b880>, <keras.callbacks.History object at 0x7fa3c8942710>, <keras.callbacks.History object at 0x7fa3c1cf3ca0>, <keras.callbacks.History object at 0x7fa3bb0a55d0>, <keras.callbacks.History object at 0x7fa3b441f070>, <keras.callbacks.History object at 0x7fa3ad79ec20>, <keras.callbacks.History object at 0x7fa3d3d96170>, <keras.callbacks.History object at 0x7fa3a00639d0>, <keras.callbacks.History object at 0x7fa3993fd2a0>, <keras.callbacks.History object at 0x7fa39277aad0>, <keras.callbacks.History object at 0x7fa38b9ebfd0>, <keras.callbacks.History object at 0x7fa384e95bd0>, <keras.callbacks.History object at 0x7fa37e207430>, <keras.callbacks.History object at 0x7fa37759b670>, <keras.callbacks.History object at 0x7fa37095a530>, <keras.callbacks.History object at 0x7fa369d2bdc0>, <keras.callbacks.History object at 0x7fa39fe95660>, <keras.callbacks.History object at 0x7fa35c662e90>, <keras.callbacks.History object at 0x7fa3559ffe80>, <keras.callbacks.History object at 0x7fa34ee21f90>, <keras.callbacks.History object at 0x7fa3483ab820>, <keras.callbacks.History object at 0x7fa33d83d090>, <keras.callbacks.History object at 0x7fa336cd68f0>, <keras.callbacks.History object at 0x7fa33016fe20>, <keras.callbacks.History object at 0x7fa367e0d9f0>, <keras.callbacks.History object at 0x7fa322c8b250>, <keras.callbacks.History object at 0x7fa31c102e00>, <keras.callbacks.History object at 0x7fa3154be350>, <keras.callbacks.History object at 0x7fa30ea5fbb0>, <keras.callbacks.History object at 0x7fa303c21480>, <keras.callbacks.History object at 0x7fa2f8f1ece0>, <keras.callbacks.History object at 0x7fa2f2226d70>, <keras.callbacks.History object at 0x7fa2eb571db0>, <keras.callbacks.History object at 0x7fa2e487f640>, <keras.callbacks.History object at 0x7fa301e43850>, <keras.callbacks.History object at 0x7fa2d70ca740>, <keras.callbacks.History object at 0x7fa2d03fbca0>, <keras.callbacks.History object at 0x7fa2c97194e0>, <keras.callbacks.History object at 0x7fa2c2093460>, <keras.callbacks.History object at 0x7fa2bc799450>, <keras.callbacks.History object at 0x7fa2b6633400>, <keras.callbacks.History object at 0x7fa2ac0e13f0>, <keras.callbacks.History object at 0x7fa2bbe033a0>, <keras.callbacks.History object at 0x7fa29f23c8e0>, <keras.callbacks.History object at 0x7fa2998576a0>, <keras.callbacks.History object at 0x7fa29358f8e0>, <keras.callbacks.History object at 0x7fa28c4067a0>, <keras.callbacks.History object at 0x7fa28550bca0>, <keras.callbacks.History object at 0x7fa284981c90>, <keras.callbacks.History object at 0x7fa27e83fc40>, <keras.callbacks.History object at 0x7fa278a89c30>, <keras.callbacks.History object at 0x7fa27297bbe0>, <keras.callbacks.History object at 0x7fa26ca79f30>, <keras.callbacks.History object at 0x7fa264997790>, <keras.callbacks.History object at 0x7fa25991b9d0>, <keras.callbacks.History object at 0x7fa252ca2890>, <keras.callbacks.History object at 0x7fa24c037f40>, <keras.callbacks.History object at 0x7fa2453f5990>, <keras.callbacks.History object at 0x7fa23e7bf220>, <keras.callbacks.History object at 0x7fa237c02da0>, <keras.callbacks.History object at 0x7fa230ed62f0>, <keras.callbacks.History object at 0x7fa250b7bb50>, <keras.callbacks.History object at 0x7fa2234893f0>, <keras.callbacks.History object at 0x7fa21c776c80>, <keras.callbacks.History object at 0x7fa215a52d10>, <keras.callbacks.History object at 0x7fa22efb5d80>, <keras.callbacks.History object at 0x7fa2081ff5b0>, <keras.callbacks.History object at 0x7fa2014af7f0>, <keras.callbacks.History object at 0x7fa21a7699f0>, <keras.callbacks.History object at 0x7fa1f3b32530>, <keras.callbacks.History object at 0x7fa1ecf33d90>, <keras.callbacks.History object at 0x7fa1e62b9630>, <keras.callbacks.History object at 0x7fa1db602e90>, <keras.callbacks.History object at 0x7fa1d496be80>, <keras.callbacks.History object at 0x7fa1f1d61f90>, <keras.callbacks.History object at 0x7fa1c726f7f0>, <keras.callbacks.History object at 0x7fa1b7dd1090>, <keras.callbacks.History object at 0x7fa1d961a920>, <keras.callbacks.History object at 0x7fa1aac57e20>, <keras.callbacks.History object at 0x7fa1a41719f0>, <keras.callbacks.History object at 0x7fa1995df250>, <keras.callbacks.History object at 0x7fa192a4ee00>, <keras.callbacks.History object at 0x7fa18f9da350>, <keras.callbacks.History object at 0x7fa18533fbb0>, <keras.callbacks.History object at 0x7fa17a995450>, <keras.callbacks.History object at 0x7fa16f5e6cb0>, <keras.callbacks.History object at 0x7fa1b146ed70>, <keras.callbacks.History object at 0x7fa15deb9db0>, <keras.callbacks.History object at 0x7fa157307640>, <keras.callbacks.History object at 0x7fa1500e3880>, <keras.callbacks.History object at 0x7fa14918a710>, <keras.callbacks.History object at 0x7fa14226bca0>, <keras.callbacks.History object at 0x7fa14faa15d0>, <keras.callbacks.History object at 0x7fa134643070>, <keras.callbacks.History object at 0x7fa12d6dac20>, <keras.callbacks.History object at 0x7fa126696170>, <keras.callbacks.History object at 0x7fa11f823a00>, <keras.callbacks.History object at 0x7fa1188c9270>, <keras.callbacks.History object at 0x7fa1119a2ad0>, <keras.callbacks.History object at 0x7fa10abb7fd0>, <keras.callbacks.History object at 0x7fa118311bd0>, <keras.callbacks.History object at 0x7fa0fd0df430>, <keras.callbacks.History object at 0x7fa0f6243670>, <keras.callbacks.History object at 0x7fa0ef3be530>, <keras.callbacks.History object at 0x7fa0e851bd90>, <keras.callbacks.History object at 0x7fa0e16b5630>, <keras.callbacks.History object at 0x7facbbf46e90>, <keras.callbacks.History object at 0x7fa0e0a57ee0>, <keras.callbacks.History object at 0x7facbaea1fc0>, <keras.callbacks.History object at 0x7fa0c035b7f0>, <keras.callbacks.History object at 0x7fa0b9735090>, <keras.callbacks.History object at 0x7fa0b2aea8f0>, <keras.callbacks.History object at 0x7fa0abdabe20>, <keras.callbacks.History object at 0x7fa0a526d9f0>, <keras.callbacks.History object at 0x7fa09e57f250>, <keras.callbacks.History object at 0x7fa097942e00>, <keras.callbacks.History object at 0x7fa0ed4be350>, <keras.callbacks.History object at 0x7fa086273bb0>, <keras.callbacks.History object at 0x7fa07f615450>, <keras.callbacks.History object at 0x7fa077fdecb0>, <keras.callbacks.History object at 0x7fa070f96da0>, <keras.callbacks.History object at 0x7fa069e91db0>, <keras.callbacks.History object at 0x7fa062d83610>, <keras.callbacks.History object at 0x7fa05bc63850>, <keras.callbacks.History object at 0x7fa3c51e8ca0>, <keras.callbacks.History object at 0x7fa11bd1da80>, <keras.callbacks.History object at 0x7fa2453f52a0>, <keras.callbacks.History object at 0x7fa35c6607c0>, <keras.callbacks.History object at 0x7fa46b5b1f60>, <keras.callbacks.History object at 0x7fa05086af20>, <keras.callbacks.History object at 0x7fa0622faad0>, <keras.callbacks.History object at 0x7fa04c136020>, <keras.callbacks.History object at 0x7fa0494b7880>, <keras.callbacks.History object at 0x7fa0436f5120>, <keras.callbacks.History object at 0x7fa054422980>, <keras.callbacks.History object at 0x7fa035afaaa0>, <keras.callbacks.History object at 0x7fa02eaada80>, <keras.callbacks.History object at 0x7fa027c4b2e0>, <keras.callbacks.History object at 0x7fa01cebbf70>, <keras.callbacks.History object at 0x7fa0164a23e0>, <keras.callbacks.History object at 0x7fa00f9cfc70>, <keras.callbacks.History object at 0x7fa033c154e0>, <keras.callbacks.History object at 0x7fa0023ded40>, <keras.callbacks.History object at 0x7f9ffb6fee00>, <keras.callbacks.History object at 0x7f9ff4c35e70>, <keras.callbacks.History object at 0x7f9fee0576a0>, <keras.callbacks.History object at 0x7f9fe3653910>, <keras.callbacks.History object at 0x7f9fdca927d0>, <keras.callbacks.History object at 0x7f9fd5ebfe80>, <keras.callbacks.History object at 0x7fa0144598d0>, <keras.callbacks.History object at 0x7f9fc86e7130>, <keras.callbacks.History object at 0x7f9fc1b22d10>, <keras.callbacks.History object at 0x7f9fbaf4e260>, <keras.callbacks.History object at 0x7f9ff2e7ba90>, <keras.callbacks.History object at 0x7f9fad951330>, <keras.callbacks.History object at 0x7f9fa6d66b90>, <keras.callbacks.History object at 0x7f9fda8d6c50>, <keras.callbacks.History object at 0x7f9f995f5c90>, <keras.callbacks.History object at 0x7f9f92a374f0>, <keras.callbacks.History object at 0x7f9f8bbf7760>, <keras.callbacks.History object at 0x7f9f84ea25f0>, <keras.callbacks.History object at 0x7f9f7e123b50>, <keras.callbacks.History object at 0x7f9f971a56f0>, <keras.callbacks.History object at 0x7f9f707fef50>, <keras.callbacks.History object at 0x7f9f69afab30>, <keras.callbacks.History object at 0x7f9f62dc6080>, <keras.callbacks.History object at 0x7f9f5808f8b0>, <keras.callbacks.History object at 0x7f9f513651e0>, <keras.callbacks.History object at 0x7f9f49e42a70>, <keras.callbacks.History object at 0x7f9f4316f6d0>, <keras.callbacks.History object at 0x7f9f4f58db40>, <keras.callbacks.History object at 0x7f9f350e33a0>, <keras.callbacks.History object at 0x7f9f2dfb3610>, <keras.callbacks.History object at 0x7f9f3b6c64a0>, <keras.callbacks.History object at 0x7f9f17d47d00>, <keras.callbacks.History object at 0x7f9f10c095a0>, <keras.callbacks.History object at 0x7f9f09aaee00>, <keras.callbacks.History object at 0x7f9f02986ef0>, <keras.callbacks.History object at 0x7f9efb861f30>, <keras.callbacks.History object at 0x7f9ef5093760>, <keras.callbacks.History object at 0x7f9eee6439a0>, <keras.callbacks.History object at 0x7f9ee7a1a860>, <keras.callbacks.History object at 0x7f9ee0e2be20>, <keras.callbacks.History object at 0x7f9f09295960>, <keras.callbacks.History object at 0x7f9ed36731f0>, <keras.callbacks.History object at 0x7f9ecca76d70>, <keras.callbacks.History object at 0x7f9ec5e822c0>, <keras.callbacks.History object at 0x7f9ebf287b20>, <keras.callbacks.History object at 0x7f9eb86b93c0>, <keras.callbacks.History object at 0x7f9eb1b16c20>, <keras.callbacks.History object at 0x7f9eab15ece0>, <keras.callbacks.History object at 0x7f9ea45add20>, <keras.callbacks.History object at 0x7f9e9d9d3580>, <keras.callbacks.History object at 0x7f9e96def7c0>, <keras.callbacks.History object at 0x7f9e8c1ea680>, <keras.callbacks.History object at 0x7f9e8561fbe0>, <keras.callbacks.History object at 0x7f9e7ea69780>, <keras.callbacks.History object at 0x7f9e77e86fe0>, <keras.callbacks.History object at 0x7f9e714b6bc0>, <keras.callbacks.History object at 0x7f9e6a1ca0e0>, <keras.callbacks.History object at 0x7f9e631d7940>, <keras.callbacks.History object at 0x7f9e5c1fd1e0>, <keras.callbacks.History object at 0x7f9e5523ea40>, <keras.callbacks.History object at 0x7f9e4e2876a0>, <keras.callbacks.History object at 0x7f9e47299b40>, <keras.callbacks.History object at 0x7f9e3c27f3d0>, <keras.callbacks.History object at 0x7f9e354a75b0>, <keras.callbacks.History object at 0x7f9e2e4da4a0>, <keras.callbacks.History object at 0x7f9e274dfd00>, <keras.callbacks.History object at 0x7f9e209f15a0>, <keras.callbacks.History object at 0x7f9e19ccae00>, <keras.callbacks.History object at 0x7f9e12fcaef0>, <keras.callbacks.History object at 0x7f9e0c2d1f00>, <keras.callbacks.History object at 0x7f9e017c3760>, <keras.callbacks.History object at 0x7f9dfaacb9a0>, <keras.callbacks.History object at 0x7f9df3dca860>, <keras.callbacks.History object at 0x7f9ded0a7e20>, <keras.callbacks.History object at 0x7f9de63d9960>, <keras.callbacks.History object at 0x7f9ddf6c31f0>, <keras.callbacks.History object at 0x7f9dd4836da0>, <keras.callbacks.History object at 0x7f9dcda6e2c0>, <keras.callbacks.History object at 0x7f9deb1d7b20>, <keras.callbacks.History object at 0x7f9dc00fd3f0>, <keras.callbacks.History object at 0x7f9db932ec20>, <keras.callbacks.History object at 0x7f9dd282ece0>, <keras.callbacks.History object at 0x7f9dab799d50>, <keras.callbacks.History object at 0x7f9da09fb5b0>, <keras.callbacks.History object at 0x7f9d99bf37c0>, <keras.callbacks.History object at 0x7f9d92e326b0>, <keras.callbacks.History object at 0x7f9db0673be0>, <keras.callbacks.History object at 0x7f9d84fdd780>, <keras.callbacks.History object at 0x7f9d7e09afe0>, <keras.callbacks.History object at 0x7f9d77022b90>, <keras.callbacks.History object at 0x7f9d701fe0e0>, <keras.callbacks.History object at 0x7f9d69283940>, <keras.callbacks.History object at 0x7f9d623411e0>, <keras.callbacks.History object at 0x7f9d5b3dea40>, <keras.callbacks.History object at 0x7f9d5463f6a0>, <keras.callbacks.History object at 0x7f9d4d6d5b40>, <keras.callbacks.History object at 0x7f9d46eff3a0>, <keras.callbacks.History object at 0x7f9d61793610>, <keras.callbacks.History object at 0x7f9d398da4a0>, <keras.callbacks.History object at 0x7f9d32da7d00>, <keras.callbacks.History object at 0x7f9d2c2895a0>, <keras.callbacks.History object at 0x7f9d25782e30>, <keras.callbacks.History object at 0x7f9d1ec8f820>, <keras.callbacks.History object at 0x7f9d14145f30>, <keras.callbacks.History object at 0x7f9d0d827790>, <keras.callbacks.History object at 0x7f9d06d2f9d0>, <keras.callbacks.History object at 0x7f9cff84a8c0>, <keras.callbacks.History object at 0x7f9cf86bff40>, <keras.callbacks.History object at 0x7f9cf1751990>, <keras.callbacks.History object at 0x7f9cea6ef1f0>, <keras.callbacks.History object at 0x7f9ce3686da0>, <keras.callbacks.History object at 0x7f9cdc62a2f0>, <keras.callbacks.History object at 0x7f9cd5797b80>, <keras.callbacks.History object at 0x7f9cce7353f0>, <keras.callbacks.History object at 0x7f9cc76cac80>, <keras.callbacks.History object at 0x7f9cc0662d40>, <keras.callbacks.History object at 0x7f9cb9905d80>, <keras.callbacks.History object at 0x7f9caea5b8b0>, <keras.callbacks.History object at 0x7f9ca7bd12a0>, <keras.callbacks.History object at 0x7f9ca0d0ead0>, <keras.callbacks.History object at 0x7f9cae463fd0>, <keras.callbacks.History object at 0x7f9c931cdbd0>, <keras.callbacks.History object at 0x7f9c8c32f430>, <keras.callbacks.History object at 0x7f9c85467670>, <keras.callbacks.History object at 0x7f9c7ddd6530>, <keras.callbacks.History object at 0x7f9c7771bd90>, <keras.callbacks.History object at 0x7f9c709d5630>, <keras.callbacks.History object at 0x7f9c65bc6e90>, <keras.callbacks.History object at 0x7f9c5ee0be80>, <keras.callbacks.History object at 0x7f9c7b6edfc0>, <keras.callbacks.History object at 0x7f9c514837f0>, <keras.callbacks.History object at 0x7f9c4a6c9090>, <keras.callbacks.History object at 0x7f9c439068f0>, <keras.callbacks.History object at 0x7f9c3cb47e20>, <keras.callbacks.History object at 0x7f9c35d759f0>, <keras.callbacks.History object at 0x7f9c2efaf280>, <keras.callbacks.History object at 0x7f9c27f2ee00>, <keras.callbacks.History object at 0x7f9c2103e350>, <keras.callbacks.History object at 0x7f9c2e95bbb0>, <keras.callbacks.History object at 0x7f9c133e9450>, <keras.callbacks.History object at 0x7f9c0c4aacb0>, <keras.callbacks.History object at 0x7f9c05596d70>, <keras.callbacks.History object at 0x7f9bfe675db0>, <keras.callbacks.History object at 0x7f9bf76ff610>, <keras.callbacks.History object at 0x7f9bf079f850>, <keras.callbacks.History object at 0x7f9be98ae740>, <keras.callbacks.History object at 0x7f9be3207ca0>, <keras.callbacks.History object at 0x7f9bdc625600>, <keras.callbacks.History object at 0x7f9bfc503070>, <keras.callbacks.History object at 0x7f9bcef5ec50>, <keras.callbacks.History object at 0x7f9bc83fa170>, <keras.callbacks.History object at 0x7f9bc18a79d0>, <keras.callbacks.History object at 0x7f9bb6d51270>, <keras.callbacks.History object at 0x7f9bb01daad0>, <keras.callbacks.History object at 0x7f9ba986bfd0>, <keras.callbacks.History object at 0x7f9ba2d01c00>, <keras.callbacks.History object at 0x7f9b9c06b430>, <keras.callbacks.History object at 0x7f9b9142f670>, <keras.callbacks.History object at 0x7f9b86822560>, <keras.callbacks.History object at 0x7f9b7fc1fdc0>, <keras.callbacks.History object at 0x7f9b7901d630>, <keras.callbacks.History object at 0x7f9b7241ee90>, <keras.callbacks.History object at 0x7f9bb4fafee0>, <keras.callbacks.History object at 0x7f9b64dfdf90>, <keras.callbacks.History object at 0x7f9b5e1eb7f0>, <keras.callbacks.History object at 0x7f9b575f50c0>, <keras.callbacks.History object at 0x7f9b5052e920>, <keras.callbacks.History object at 0x7f9b49853eb0>, <keras.callbacks.History object at 0x7f9b42a459f0>, <keras.callbacks.History object at 0x7f9b3bc37250>, <keras.callbacks.History object at 0x7f9b34e6ee00>, <keras.callbacks.History object at 0x7f9b2e086380>, <keras.callbacks.History object at 0x7f9b23283bb0>, <keras.callbacks.History object at 0x7f9b3b7b1480>, <keras.callbacks.History object at 0x7f9b15892950>, <keras.callbacks.History object at 0x7f9b0e8ff010>, <keras.callbacks.History object at 0x7f9b088b2920>, <keras.callbacks.History object at 0x7f9b0271b610>, <keras.callbacks.History object at 0x7f9afc3564a0>, <keras.callbacks.History object at 0x7f9b5051dc30>, <keras.callbacks.History object at 0x7fa01451bfd0>, <keras.callbacks.History object at 0x7f9be3204910>, <keras.callbacks.History object at 0x7f9cf86be560>, <keras.callbacks.History object at 0x7f9e0f5230d0>, <keras.callbacks.History object at 0x7f9f26413550>, <keras.callbacks.History object at 0x7fa03fd42440>, <keras.callbacks.History object at 0x7f9aed78ab00>, <keras.callbacks.History object at 0x7f9aec45e050>, <keras.callbacks.History object at 0x7f9ae3747880>, <keras.callbacks.History object at 0x7f9ae14ed120>, <keras.callbacks.History object at 0x7f9ae107e980>, <keras.callbacks.History object at 0x7f9ad4c0ffa0>, <keras.callbacks.History object at 0x7f9acbc71ab0>, <keras.callbacks.History object at 0x7f9ac4b2f2e0>, <keras.callbacks.History object at 0x7f9abde07f70>, <keras.callbacks.History object at 0x7f9ab6ce63e0>, <keras.callbacks.History object at 0x7f9ab044bc40>, <keras.callbacks.History object at 0x7f9ac9d614e0>, <keras.callbacks.History object at 0x7f9aa2d2ed40>, <keras.callbacks.History object at 0x7f9a9bfb2e00>, <keras.callbacks.History object at 0x7f9a953e9e40>, <keras.callbacks.History object at 0x7f9a8e76f6d0>, <keras.callbacks.History object at 0x7f9a87ccb8e0>, <keras.callbacks.History object at 0x7f9a80f3e7a0>, <keras.callbacks.History object at 0x7f9a7a3cbd60>, <keras.callbacks.History object at 0x7f9a737518a0>, <keras.callbacks.History object at 0x7f9a6c63f130>, <keras.callbacks.History object at 0x7f9a6160ece0>, <keras.callbacks.History object at 0x7f9a72e26230>, <keras.callbacks.History object at 0x7f9a5362fa60>, <keras.callbacks.History object at 0x7f9a4c669300>, <keras.callbacks.History object at 0x7f9a456aab60>, <keras.callbacks.History object at 0x7f9a3e682cb0>, <keras.callbacks.History object at 0x7f9a3768dc60>, <keras.callbacks.History object at 0x7f9a3065b4c0>, <keras.callbacks.History object at 0x7f9a29873700>, <keras.callbacks.History object at 0x7f9a22fee5f0>, <keras.callbacks.History object at 0x7f9a1c407be0>, <keras.callbacks.History object at 0x7f9a158556c0>, <keras.callbacks.History object at 0x7f9a0ec92f20>, <keras.callbacks.History object at 0x7f9a080daad0>, <keras.callbacks.History object at 0x7f9a0152e020>, <keras.callbacks.History object at 0x7f99fa973880>, <keras.callbacks.History object at 0x7f99f3de5120>, <keras.callbacks.History object at 0x7f99ed23a980>, <keras.callbacks.History object at 0x7f99e685bfa0>, <keras.callbacks.History object at 0x7f99df565ab0>, <keras.callbacks.History object at 0x7f99f1de72e0>, <keras.callbacks.History object at 0x7f99d0dfff70>, <keras.callbacks.History object at 0x7f99ca692410>, <keras.callbacks.History object at 0x7f99c16bbc40>, <keras.callbacks.History object at 0x7f99bc1754e0>, <keras.callbacks.History object at 0x7f99b51cad40>, <keras.callbacks.History object at 0x7f99ae41ee30>, <keras.callbacks.History object at 0x7f99a7485e40>, <keras.callbacks.History object at 0x7f999fcf76a0>, <keras.callbacks.History object at 0x7f9998dd38e0>, <keras.callbacks.History object at 0x7f99919f67a0>, <keras.callbacks.History object at 0x7f998a61fd60>, <keras.callbacks.History object at 0x7f99832698a0>, <keras.callbacks.History object at 0x7f997c0cb130>, <keras.callbacks.History object at 0x7f9974d1ecb0>, <keras.callbacks.History object at 0x7f996d9aa200>, <keras.callbacks.History object at 0x7f996636fa60>, <keras.callbacks.History object at 0x7f995efd9300>, <keras.callbacks.History object at 0x7f9957c16b60>, <keras.callbacks.History object at 0x7f9951602cb0>, <keras.callbacks.History object at 0x7f994a9f9c60>, <keras.callbacks.History object at 0x7f9943fc34c0>, <keras.callbacks.History object at 0x7f993d3b7700>, <keras.callbacks.History object at 0x7f9996daa5f0>, <keras.callbacks.History object at 0x7f9979effbe0>, <keras.callbacks.History object at 0x7f9928fb56c0>, <keras.callbacks.History object at 0x7f99223a2f20>, <keras.callbacks.History object at 0x7f991b762ad0>, <keras.callbacks.History object at 0x7f9910c22050>, <keras.callbacks.History object at 0x7f9909ca3880>, <keras.callbacks.History object at 0x7f99030c9150>, <keras.callbacks.History object at 0x7f98fc2d2980>, <keras.callbacks.History object at 0x7f98f54c3fa0>, <keras.callbacks.History object at 0x7f98ee6dda80>, <keras.callbacks.History object at 0x7f98e79172e0>, <keras.callbacks.History object at 0x7f98e0adaf80>, <keras.callbacks.History object at 0x7f98d9cfe410>, <keras.callbacks.History object at 0x7f98d2f07c40>, <keras.callbacks.History object at 0x7f98ee1e9510>, <keras.callbacks.History object at 0x7f98c58d2d40>, <keras.callbacks.History object at 0x7f98bed3ee00>, <keras.callbacks.History object at 0x7f98b8195e40>, <keras.callbacks.History object at 0x7f98b15fb6a0>, <keras.callbacks.History object at 0x7f98aaa4b8e0>, <keras.callbacks.History object at 0x7f98a3eb67a0>, <keras.callbacks.History object at 0x7f989cf4fd60>, <keras.callbacks.History object at 0x7f98d2a958a0>]
loss : 0.6763359184462677
accuracy : 0.8118101580476623
tp : 399.43437428409317
tn : 240.92333237993722
precision : 0.8772393202254839

putting in conf matrix
[[3273 4717]
 [2625 3143]]
